diff --git a/lib/mkmf.rb b/lib/mkmf.rb
index 8f4e69e..dcb7590 100644
--- a/lib/mkmf.rb
+++ b/lib/mkmf.rb
@@ -2193,7 +2193,7 @@ def init_mkmf(config = CONFIG, rbconfig = RbConfig::CONFIG)
   $LOCAL_LIBS = ""
 
   $cleanfiles = config_string('CLEANFILES') {|s| Shellwords.shellwords(s)} || []
-  $cleanfiles << "mkmf.log"
+  $cleanfiles << "mkmf.log .*.time"
   $distcleanfiles = config_string('DISTCLEANFILES') {|s| Shellwords.shellwords(s)} || []
   $distcleandirs = config_string('DISTCLEANDIRS') {|s| Shellwords.shellwords(s)} || []
 
diff --git a/configure.in b/configure.in
index 7453c91..4aefd2d 100644
--- a/configure.in
+++ b/configure.in
@@ -2435,6 +2435,10 @@ if test "$EXEEXT" = .exe; then
     AC_SUBST(EXECUTABLE_EXTS)
 fi
 
+dnl enable gc debugging
+AC_ARG_ENABLE(gcdebug,
+       AS_HELP_STRING([--enable-gcdebug], [build garbage collector with debugging enabled]),
+       [AC_DEFINE(GC_DEBUG,1)])
 dnl }
 dnl build section {
 
diff --git a/gc.c b/gc.c
index 0f84e22..feb54f1 100644
--- a/gc.c
+++ b/gc.c
@@ -98,6 +98,15 @@ ruby_gc_params_t initial_params = {
 #endif
 };
 
+#ifndef HAVE_LONG_LONG
+#define LONG_LONG long
+#endif
+
+static int heap_min_slots = 10000;
+static int heap_slots_increment = 10000;
+static int initial_heap_slots_increment = 10000;
+static double heap_slots_growth_factor = 1.8;
+
 #define nomem_error GET_VM()->special_exceptions[ruby_error_nomemory]
 
 #if SIZEOF_LONG == SIZEOF_VOIDP
@@ -302,7 +311,7 @@ typedef struct RVALUE {
 	struct RComplex complex;
     } as;
 #ifdef GC_DEBUG
-    const char *file;
+    VALUE file;
     int   line;
 #endif
 } RVALUE;
@@ -372,11 +381,25 @@ typedef struct rb_objspace {
 	size_t free_min;
 	size_t final_num;
 	size_t do_heap_free;
+        unsigned long max_blocks_to_free;
+        unsigned long freed_blocks;
     } heap;
     struct {
+        unsigned long processed;
+        unsigned long freed_objects;
+        unsigned long freelist_size;
+        unsigned long zombies;
+        unsigned long free_counts[T_MASK+1];
+        unsigned long live_counts[T_MASK+1];
+        unsigned long gc_time_accumulator_before_gc;
+        unsigned long live_after_last_mark_phase;
+    } stats;
+    struct {
 	int dont_gc;
 	int dont_lazy_sweep;
 	int during_gc;
+        int gc_statistics;
+        int verbose_gc_stats;
     } flags;
     struct {
 	st_table *table;
@@ -393,6 +416,14 @@ typedef struct rb_objspace {
     struct gc_list *global_list;
     size_t count;
     int gc_stress;
+    long heap_size;
+    unsigned LONG_LONG gc_time_accumulator;
+    FILE* gc_data_file;
+    long gc_collections;
+    unsigned LONG_LONG gc_allocated_size;
+    unsigned LONG_LONG gc_num_allocations;
+    unsigned long live_objects;
+    unsigned LONG_LONG allocated_objects;
 } rb_objspace_t;
 
 #if defined(ENABLE_VM_OBJSPACE) && ENABLE_VM_OBJSPACE
@@ -415,6 +446,16 @@ int *ruby_initial_gc_stress_ptr = &rb_objspace.gc_stress;
 #define heaps_freed		objspace->heap.freed
 #define dont_gc 		objspace->flags.dont_gc
 #define during_gc		objspace->flags.during_gc
+#define gc_statistics           objspace->flags.gc_statistics
+#define verbose_gc_stats        objspace->flags.verbose_gc_stats
+#define heap_size               objspace->heap_size
+#define gc_time_accumulator     objspace->gc_time_accumulator
+#define gc_data_file            objspace->gc_data_file
+#define gc_collections          objspace->gc_collections
+#define gc_allocated_size       objspace->gc_allocated_size
+#define gc_num_allocations      objspace->gc_num_allocations
+#define live_objects            objspace->live_objects
+#define allocated_objects       objspace->allocated_objects
 #define finalizer_table 	objspace->final.table
 #define deferred_final_list	objspace->final.deferred
 #define global_List		objspace->global_list
@@ -422,6 +463,14 @@ int *ruby_initial_gc_stress_ptr = &rb_objspace.gc_stress;
 #define initial_malloc_limit	initial_params.initial_malloc_limit
 #define initial_heap_min_slots	initial_params.initial_heap_min_slots
 #define initial_free_min	initial_params.initial_free_min
+#define free_counts             objspace->stats.free_counts
+#define live_counts             objspace->stats.live_counts
+#define processed               objspace->stats.processed
+#define zombies                 objspace->stats.zombies
+#define freelist_size           objspace->stats.freelist_size
+#define freed_objects           objspace->stats.freed_objects
+#define gc_time_accumulator_before_gc objspace->stats.gc_time_accumulator_before_gc
+#define live_after_last_mark_phase objspace->stats.live_after_last_mark_phase
 
 static void rb_objspace_call_finalizer(rb_objspace_t *objspace);
 
@@ -444,24 +493,59 @@ static void init_mark_stack(mark_stack_t *stack);
 void
 rb_gc_set_params(void)
 {
-    char *malloc_limit_ptr, *heap_min_slots_ptr, *free_min_ptr;
+    char *envp;
+
+    rb_objspace_t *objspace = &rb_objspace;
+
+    gc_data_file = stderr;
 
     if (rb_safe_level() > 0) return;
 
-    malloc_limit_ptr = getenv("RUBY_GC_MALLOC_LIMIT");
-    if (malloc_limit_ptr != NULL) {
-	int malloc_limit_i = atoi(malloc_limit_ptr);
+    envp = getenv("RUBY_GC_STATS");
+    if (envp != NULL) {
+        int i = atoi(envp);
+        if (i > 0) {
+            verbose_gc_stats = 1;
+            fprintf(stderr, "RUBY_GC_STATS=%d\n", verbose_gc_stats);
+        }
+        /* child processes should not inherit RUBY_GC_STATS */
+        ruby_unsetenv("RUBY_GC_STATS");
+    }
+
+    envp = getenv("RUBY_GC_DATA_FILE");
+    if (envp != NULL) {
+        FILE* data_file = fopen(envp, "w");
+        if (data_file != NULL) {
+            gc_data_file = data_file;
+        }
+        else {
+            fprintf(stderr, "can't open gc log file %s for writing, using default\n", envp);
+        }
+        /* child processes should not inherit RUBY_GC_DATA_FILE to avoid clobbering */
+        ruby_unsetenv("RUBY_GC_DATA_FILE");
+    }
+
+    envp = getenv("RUBY_GC_MALLOC_LIMIT");
+    if (envp != NULL) {
+	int malloc_limit_i = atoi(envp);
+        if (verbose_gc_stats) {
+            fprintf(gc_data_file, "RUBY_GC_MALLOC_LIMIT=%s\n", envp);
+        }
 	if (RTEST(ruby_verbose))
 	    fprintf(stderr, "malloc_limit=%d (%d)\n",
 		    malloc_limit_i, initial_malloc_limit);
 	if (malloc_limit_i > 0) {
 	    initial_malloc_limit = malloc_limit_i;
+            // malloc_limit = initial_malloc_limit;
 	}
     }
 
-    heap_min_slots_ptr = getenv("RUBY_HEAP_MIN_SLOTS");
-    if (heap_min_slots_ptr != NULL) {
-	int heap_min_slots_i = atoi(heap_min_slots_ptr);
+    envp = getenv("RUBY_HEAP_MIN_SLOTS");
+    if (envp != NULL) {
+	int heap_min_slots_i = atoi(envp);
+        if (verbose_gc_stats) {
+            fprintf(gc_data_file, "RUBY_HEAP_MIN_SLOTS=%s\n", envp);
+        }
 	if (RTEST(ruby_verbose))
 	    fprintf(stderr, "heap_min_slots=%d (%d)\n",
 		    heap_min_slots_i, initial_heap_min_slots);
@@ -471,15 +555,42 @@ rb_gc_set_params(void)
 	}
     }
 
-    free_min_ptr = getenv("RUBY_FREE_MIN");
-    if (free_min_ptr != NULL) {
-	int free_min_i = atoi(free_min_ptr);
+    if (!(envp = getenv("RUBY_FREE_MIN")))
+        envp = getenv("RUBY_HEAP_FREE_MIN");
+    if (envp != NULL) {
+	int free_min_i = atoi(envp);
+        if (verbose_gc_stats) {
+            fprintf(gc_data_file, "RUBY_HEAP_FREE_MIN=%s\n", envp);
+        }
 	if (RTEST(ruby_verbose))
 	    fprintf(stderr, "free_min=%d (%d)\n", free_min_i, initial_free_min);
 	if (free_min_i > 0) {
 	    initial_free_min = free_min_i;
 	}
     }
+
+    envp = getenv("RUBY_HEAP_SLOTS_INCREMENT");
+    if (envp != NULL) {
+        int i = atoi(envp);
+        if (verbose_gc_stats) {
+            fprintf(gc_data_file, "RUBY_HEAP_SLOTS_INCREMENT=%s\n", envp);
+        }
+        heap_slots_increment = i;
+        initial_heap_slots_increment = heap_slots_increment;
+    }
+
+    envp = getenv("RUBY_HEAP_SLOTS_GROWTH_FACTOR");
+    if (envp != NULL) {
+        double d = atof(envp);
+        if (verbose_gc_stats) {
+            fprintf(gc_data_file, "RUBY_HEAP_SLOTS_GROWTH_FACTOR=%s\n", envp);
+        }
+        if (d > 0) {
+            heap_slots_growth_factor = d;
+        }
+    }
+
+    fflush(gc_data_file);
 }
 
 #if defined(ENABLE_VM_OBJSPACE) && ENABLE_VM_OBJSPACE
@@ -776,6 +887,11 @@ vm_malloc_fixup(rb_objspace_t *objspace, void *mem, size_t size)
     mem = (size_t *)mem + 1;
 #endif
 
+    if (gc_statistics) {
+        gc_allocated_size += size;
+	gc_num_allocations += 1;
+    }
+
     return mem;
 }
 
@@ -836,6 +952,13 @@ vm_xrealloc(rb_objspace_t *objspace, void *ptr, size_t size)
     mem = (size_t *)mem + 1;
 #endif
 
+    /* TODO: we can't count correctly unless we store old size on heap
+    if (gc_statistics) {
+        gc_allocated_size += size;
+	gc_num_allocations += 1;
+    }
+    */
+
     return mem;
 }
 
@@ -917,7 +1040,6 @@ ruby_xfree(void *x)
 	vm_xfree(&rb_objspace, x);
 }
 
-
 /*
  *  call-seq:
  *     GC.enable    -> true or false
@@ -963,6 +1085,455 @@ rb_gc_disable(void)
     return old ? Qtrue : Qfalse;
 }
 
+/*
+ *  call-seq:
+ *     GC.enable_stats    => true or false
+ *
+ *  Enables garbage collection statistics, returning <code>true</code> if garbage
+ *  collection statistics was already enabled.
+ *
+ *     GC.enable_stats   #=> false or true
+ *     GC.enable_stats   #=> true
+ *
+ */
+
+VALUE
+rb_gc_enable_stats()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    int old = gc_statistics;
+    gc_statistics = 1;
+    return old ? Qtrue : Qfalse;
+}
+
+/*
+ *  call-seq:
+ *     GC.disable_stats    => true or false
+ *
+ *  Disables garbage collection statistics, returning <code>true</code> if garbage
+ *  collection statistics was already disabled.
+ *
+ *     GC.disable_stats   #=> false or true
+ *     GC.disable_stats   #=> true
+ *
+ */
+
+VALUE
+rb_gc_disable_stats()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    int old = gc_statistics;
+    gc_statistics = 0;
+    return old ? Qtrue : Qfalse;
+}
+
+/*
+ *  call-seq:
+ *     GC.stats_enabled?    => true or false
+ *
+ *  Check whether GC stats have been enabled.
+ *
+ *     GC.stats_enabled?   #=> false or true
+ *
+ */
+
+VALUE
+rb_gc_stats_enabled()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    return gc_statistics ? Qtrue : Qfalse;
+}
+
+
+/*
+ *  call-seq:
+ *     GC.clear_stats    => nil
+ *
+ *  Clears garbage collection statistics, returning nil. This resets the number
+ *  of collections (GC.collections) and the time used (GC.time) to 0.
+ *
+ *     GC.clear_stats    #=> nil
+ *
+ */
+
+VALUE
+rb_gc_clear_stats()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    gc_collections = 0;
+    gc_time_accumulator = 0;
+    gc_time_accumulator_before_gc = 0;
+    gc_allocated_size = 0;
+    gc_num_allocations = 0;
+    return Qnil;
+}
+
+/*
+ *  call-seq:
+ *     GC.allocated_size    => Integer
+ *
+ *  Returns the size of memory (in bytes) allocated since GC statistics collection
+ *  was enabled.
+ *
+ *     GC.allocated_size    #=> 35
+ *
+ */
+
+VALUE
+rb_gc_allocated_size()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+#if HAVE_LONG_LONG
+    return ULL2NUM(gc_allocated_size);
+#else
+    return ULONG2NUM(gc_allocated_size);
+#endif
+}
+
+/*
+ *  call-seq:
+ *     GC.num_allocations    => Integer
+ *
+ *  Returns the number of memory allocations since GC statistics collection
+ *  was enabled.
+ *
+ *     GC.num_allocations    #=> 150
+ *
+ */
+VALUE
+rb_gc_num_allocations()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+#if HAVE_LONG_LONG
+    return ULL2NUM(gc_num_allocations);
+#else
+    return ULONG2NUM(gc_num_allocations);
+#endif
+}
+
+/*
+ *  call-seq:
+ *     GC.enable_trace    => true or false
+ *
+ *  Enables garbage collection tracing, returning <code>true</code> if garbage
+ *  collection tracing was already enabled.
+ *
+ *     GC.enable_trace   #=> false or true
+ *     GC.enable_trace   #=> true
+ *
+ */
+
+VALUE
+rb_gc_enable_trace()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    int old = verbose_gc_stats;
+    verbose_gc_stats = 1;
+    return old ? Qtrue : Qfalse;
+}
+
+/*
+ *  call-seq:
+ *     GC.disable_trace    => true or false
+ *
+ *  Disables garbage collection tracing, returning <code>true</code> if garbage
+ *  collection tracing was already disabled.
+ *
+ *     GC.disable_trace   #=> false or true
+ *     GC.disable_trace   #=> true
+ *
+ */
+
+VALUE
+rb_gc_disable_trace()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    int old = verbose_gc_stats;
+    verbose_gc_stats = 0;
+    return old ? Qtrue : Qfalse;
+}
+
+/*
+ *  call-seq:
+ *     GC.trace_enabled?    => true or false
+ *
+ *  Check whether GC tracing has been enabled.
+ *
+ *     GC.trace_enabled?   #=> false or true
+ *
+ */
+
+VALUE
+rb_gc_trace_enabled()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    return verbose_gc_stats ? Qtrue : Qfalse;
+}
+
+
+const char* GC_LOGFILE_IVAR = "@gc_logfile_name";
+
+/*
+ *  call-seq:
+ *     GC.log_file(filename=nil, mode="w")    => boolean
+ *
+ *  Changes the GC data log file. Closes the currently open logfile.
+ *  Returns true if the file was successfully opened for
+ *  writing. Returns false if the file could not be opened for
+ *  writing. Returns the name of the current logfile (or nil) if no
+ *  parameter is given. Restores logging to stderr when given nil as
+ *  an argument.
+ *
+ *     GC.log_file                  #=> nil
+ *     GC.log_file "/tmp/gc.log"    #=> true
+ *     GC.log_file                  #=> "/tmp/gc.log"
+ *     GC.log_file nil              #=> true
+ *
+ */
+
+VALUE
+rb_gc_log_file(int argc, VALUE *argv, VALUE self)
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    VALUE filename = Qnil;
+    VALUE mode_str = Qnil;
+    FILE* f = NULL;
+    const char* mode = "w";
+
+    VALUE current_logfile_name = rb_iv_get(rb_mGC, GC_LOGFILE_IVAR);
+
+    if (argc==0)
+        return current_logfile_name;
+
+    rb_scan_args(argc, argv, "02", &filename, &mode_str);
+
+    if (filename == Qnil) {
+        /* close current logfile and reset logfile to stderr */
+        if (gc_data_file != stderr) {
+            fclose(gc_data_file);
+            gc_data_file = stderr;
+            rb_iv_set(rb_mGC, GC_LOGFILE_IVAR, Qnil);
+        }
+        return Qtrue;
+    }
+
+    /* we have a real logfile name */
+    filename = StringValue(filename);
+
+    if (rb_equal(current_logfile_name, filename) == Qtrue) {
+        /* do nothing if we get the file name we're already logging to */
+        return Qtrue;
+    }
+
+    /* get mode for file opening */
+    if (mode_str != Qnil)
+    {
+      mode = RSTRING_PTR(StringValue(mode_str));
+    }
+
+    /* try to open file in given mode */
+    if (f = fopen(RSTRING_PTR(filename), mode)) {
+        if (gc_data_file != stderr) {
+            fclose(gc_data_file);
+        }
+        gc_data_file = f;
+        rb_iv_set(rb_mGC, GC_LOGFILE_IVAR, filename);
+    } else {
+        return Qfalse;
+    }
+    return Qtrue;
+}
+
+/*
+ *  call-seq:
+ *     GC.log String  => String
+ *
+ *  Logs string to the GC data file and returns it.
+ *
+ *     GC.log "manual GC call"    #=> "manual GC call"
+ *
+ */
+
+VALUE
+rb_gc_log(self, original_str)
+     VALUE self, original_str;
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    if (original_str == Qnil) {
+        fprintf(gc_data_file, "\n");
+    }
+    else {
+        VALUE str = StringValue(original_str);
+        char *p = RSTRING_PTR(str);
+        fprintf(gc_data_file, "%s\n", p);
+    }
+    return original_str;
+}
+
+/*
+ *  call-seq:
+ *     GC.dump    => nil
+ *
+ *  dumps information about the current GC data structures to the GC log file
+ *
+ *     GC.dump    #=> nil
+ *
+ */
+
+VALUE
+rb_gc_dump()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    size_t i;
+
+    for (i = 0; i < heaps_used; i++) {
+        size_t limit = objspace->heap.sorted[i].slot->limit;
+        fprintf(gc_data_file, "HEAP[%2lu]: size=%7lu\n", (unsigned long)i, (unsigned long)limit);
+    }
+
+    return Qnil;
+}
+
+static const char* obj_type(VALUE tp);
+
+#ifdef GC_DEBUG
+/*
+ *  call-seq:
+ *     GC.dump_file_and_line_info(String, boolean)    => nil
+ *
+ *  dumps information on which currently allocated object was created by which file and on which line
+ *
+ *     GC.dump_file_and_line_info(String, boolean)    #=> nil
+ *
+ *  The second parameter specifies whether class names should be included in the dump.
+ *  Note that including class names will allocate additional string objects on the heap.
+ *
+ */
+
+VALUE
+rb_gc_dump_file_and_line_info(int argc, VALUE *argv)
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    VALUE filename, str, include_classnames = Qnil;
+    char *fname = NULL;
+    char *klass = NULL;
+    FILE* f = NULL;
+    size_t i = 0;
+
+    rb_scan_args(argc, argv, "11", &filename, &include_classnames);
+
+    str = StringValue(filename);
+    fname = RSTRING_PTR(str);
+    f = fopen(fname, "w");
+
+    for (i = 0; i < heaps_used; i++) {
+        RVALUE *p, *pend;
+
+        p = objspace->heap.sorted[i].start; pend = objspace->heap.sorted[i].end;
+        for (;p < pend; p++) {
+            if (p->as.basic.flags) {
+                const char *src_filename = (p->file && p->file != Qnil )? RSTRING_PTR(p->file) : "";
+                fprintf(f, "%s:%s:%d", obj_type(p->as.basic.flags & T_MASK), src_filename, (int)p->line);
+                // rb_obj_classname will create objects on the heap, we need a better solution
+                if (include_classnames == Qtrue) {
+                    /* write the class */
+                    fprintf(f, ":");
+                    switch (BUILTIN_TYPE(p)) {
+                    case T_NONE:
+                        fprintf(f, "__none__");
+                        break;
+                    case T_UNDEF:
+                        fprintf(f, "__undef__");
+                        break;
+                    case T_NODE:
+                        fprintf(f, "__node__");
+                        break;
+                    default:
+                        if (!p->as.basic.klass) {
+                            fprintf(f, "__unknown__");
+                        } else {
+                            fprintf(f, "%s", rb_obj_classname((VALUE)p));
+                        }
+                    }
+                    /* print object size for some known object types */
+                    switch (BUILTIN_TYPE(p)) {
+                    case T_STRING:
+                        fprintf(f, ":%lu", RSTRING_LEN(p));
+                        break;
+                    case T_ARRAY:
+                        fprintf(f, ":%lu", RARRAY_LEN(p));
+                        break;
+                    case T_HASH:
+                        fprintf(f, ":%lu", (long unsigned int)RHASH_SIZE(p));
+                        break;
+                    }
+                }
+                fprintf(f, "\n");
+            }
+        }
+    }
+    fclose(f);
+    return Qnil;
+}
+#endif
+
+/*
+ *  call-seq:
+ *     GC.heap_slots    => Integer
+ *
+ *  Returns the number of heap slots available for object allocations.
+ *
+ *     GC.heap_slots    #=> 10000
+ *
+ */
+VALUE
+rb_gc_heap_slots()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    return LONG2NUM(heap_size);
+}
+
+
+/*
+ *  call-seq:
+ *     GC.collections    => Integer
+ *
+ *  Returns the number of garbage collections performed while GC statistics collection
+ *  was enabled.
+ *
+ *     GC.collections    #=> 35
+ *
+ */
+
+VALUE
+rb_gc_collections()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    return LONG2NUM(gc_collections);
+}
+
+/*
+ *  call-seq:
+ *     GC.time    => Integer
+ *
+ *  Returns the time spent during garbage collection while GC statistics collection
+ *  was enabled (in micro seconds).
+ *
+ *     GC.time    #=> 20000
+ *
+ */
+
+VALUE
+rb_gc_time()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+#if HAVE_LONG_LONG
+    return LL2NUM(gc_time_accumulator);
+#else
+    return LONG2NUM(gc_time_accumulator);
+#endif
+}
+
 VALUE rb_mGC;
 
 void
@@ -1034,6 +1605,12 @@ allocate_sorted_heaps(rb_objspace_t *objspace, size_t next_heaps_length)
 static void
 assign_heap_slot(rb_objspace_t *objspace)
 {
+    /*
+    if (gc_statistics & verbose_gc_stats) {
+	fprintf(gc_data_file, "assigning heap slot\n");
+    }
+    */
+
     RVALUE *p, *pend, *membase;
     struct heaps_slot *slot;
     size_t hi, lo, mid;
@@ -1095,6 +1672,7 @@ assign_heap_slot(rb_objspace_t *objspace)
     if (lomem == 0 || lomem > p) lomem = p;
     if (himem < pend) himem = pend;
     heaps_used++;
+    heap_size += objs;
 
     while (p < pend) {
 	p->as.free.flags = 0;
@@ -1151,7 +1729,7 @@ initial_expand_heap(rb_objspace_t *objspace)
 static void
 set_heaps_increment(rb_objspace_t *objspace)
 {
-    size_t next_heaps_length = (size_t)(heaps_used * 1.8);
+    size_t next_heaps_length = (size_t)(heaps_used * heap_slots_growth_factor);
 
     if (next_heaps_length == heaps_used) {
         next_heaps_length++;
@@ -1184,6 +1762,22 @@ rb_during_gc(void)
 
 #define RANY(o) ((RVALUE*)(o))
 
+#ifdef GC_DEBUG
+static VALUE
+_rb_sourcefile(void)
+{
+    rb_thread_t *th = GET_THREAD();
+    rb_control_frame_t *cfp = rb_vm_get_ruby_level_next_cfp(th, th->cfp);
+
+    if (cfp) {
+	return cfp->iseq->filename;
+    }
+    else {
+	return Qnil;
+    }
+}
+#endif
+
 VALUE
 rb_newobj(void)
 {
@@ -1215,9 +1809,11 @@ rb_newobj(void)
 
     MEMZERO((void*)obj, RVALUE, 1);
 #ifdef GC_DEBUG
-    RANY(obj)->file = rb_sourcefile();
+    RANY(obj)->file = _rb_sourcefile();
     RANY(obj)->line = rb_sourceline();
 #endif
+    live_objects++;
+    allocated_objects++;
     GC_PROF_INC_LIVE_NUM;
 
     return obj;
@@ -1769,6 +2365,12 @@ gc_mark_children(rb_objspace_t *objspace, VALUE ptr)
 {
     register RVALUE *obj = RANY(ptr);
 
+#ifdef GC_DEBUG
+    if (obj->file && obj->file != Qnil && is_pointer_to_heap(objspace, (void*)obj->file)) {
+	gc_mark(objspace, obj->file, lev);
+    }
+#endif
+
     goto marking;		/* skip */
 
   again:
@@ -1779,6 +2381,12 @@ gc_mark_children(rb_objspace_t *objspace, VALUE ptr)
     obj->as.basic.flags |= FL_MARK;
     objspace->heap.live_num++;
 
+#ifdef GC_DEBUG
+    if (obj->file && obj->file != Qnil && is_pointer_to_heap(objspace, (void*)obj->file)) {
+	gc_mark(objspace, obj->file, lev);
+    }
+#endif
+
   marking:
     if (FL_TEST(obj, FL_EXIVAR)) {
 	rb_mark_generic_ivar(ptr);
@@ -2121,6 +2729,25 @@ free_unused_heaps(rb_objspace_t *objspace)
     }
 }
 
+static inline unsigned long
+elapsed_musecs(struct timeval since)
+{
+    struct timeval now;
+    struct timeval temp;
+
+    gettimeofday(&now, NULL);
+
+    if ((now.tv_usec-since.tv_usec)<0) {
+        temp.tv_sec = now.tv_sec-since.tv_sec-1;
+        temp.tv_usec = 1000000+now.tv_usec-since.tv_usec;
+    } else {
+        temp.tv_sec = now.tv_sec-since.tv_sec;
+        temp.tv_usec = now.tv_usec-since.tv_usec;
+    }
+
+    return temp.tv_sec*1000000 + temp.tv_usec;
+}
+
 static void
 slot_sweep(rb_objspace_t *objspace, struct heaps_slot *sweep_slot)
 {
@@ -2128,14 +2755,23 @@ slot_sweep(rb_objspace_t *objspace, struct heaps_slot *sweep_slot)
     RVALUE *p, *pend;
     RVALUE *free = freelist, *final = deferred_final_list;
     int deferred;
+    int do_gc_stats = gc_statistics & verbose_gc_stats;
+
+    struct timeval tv1;
+    if (gc_statistics) gettimeofday(&tv1, NULL);
 
     p = sweep_slot->slot; pend = p + sweep_slot->limit;
     while (p < pend) {
         if (!(p->as.basic.flags & FL_MARK)) {
+            if (do_gc_stats && !p->as.basic.flags) {
+                /* slot was free before GC */
+                freelist_size++;
+            }
             if (p->as.basic.flags &&
                 ((deferred = obj_free(objspace, (VALUE)p)) ||
 		 (FL_TEST(p, FL_FINALIZE)))) {
                 if (!deferred) {
+                    if (do_gc_stats) zombies++;
                     p->as.free.flags = T_ZOMBIE;
                     RDATA(p)->dfree = 0;
                 }
@@ -2145,6 +2781,10 @@ slot_sweep(rb_objspace_t *objspace, struct heaps_slot *sweep_slot)
                 final_num++;
             }
             else {
+                if (do_gc_stats) {
+                    VALUE obt = p->as.basic.flags & T_MASK;
+                    if (obt) free_counts[obt]++;
+                }
                 add_freelist(objspace, p);
                 free_num++;
             }
@@ -2152,13 +2792,22 @@ slot_sweep(rb_objspace_t *objspace, struct heaps_slot *sweep_slot)
         else if (BUILTIN_TYPE(p) == T_ZOMBIE) {
             /* objects to be finalized */
             /* do nothing remain marked */
+            if (do_gc_stats) zombies++;
         }
         else {
             RBASIC(p)->flags &= ~FL_MARK;
+	    if (do_gc_stats) {
+	       live_counts[p->as.basic.flags & T_MASK]++;
+	    }
         }
         p++;
+        processed++;
     }
-    if (final_num + free_num == sweep_slot->limit &&
+
+    freed_objects += free_num;
+
+    if (objspace->heap.freed_blocks < objspace->heap.max_blocks_to_free &&
+        final_num + free_num == sweep_slot->limit &&
         objspace->heap.free_num > objspace->heap.do_heap_free) {
         RVALUE *pp;
 
@@ -2169,6 +2818,8 @@ slot_sweep(rb_objspace_t *objspace, struct heaps_slot *sweep_slot)
         sweep_slot->limit = final_num;
         freelist = free;	/* cancel this page from freelist */
         unlink_heap_slot(objspace, sweep_slot);
+        objspace->heap.freed_blocks += 1;
+        heap_size -= final_num + free_num;
     }
     else {
         objspace->heap.free_num += free_num;
@@ -2181,6 +2832,10 @@ slot_sweep(rb_objspace_t *objspace, struct heaps_slot *sweep_slot)
             RUBY_VM_SET_FINALIZER_INTERRUPT(th);
         }
     }
+
+    if (gc_statistics) {
+	gc_time_accumulator += elapsed_musecs(tv1);
+    }
 }
 
 static int
@@ -2201,6 +2856,21 @@ ready_to_gc(rb_objspace_t *objspace)
 static void
 before_gc_sweep(rb_objspace_t *objspace)
 {
+    if (gc_statistics & verbose_gc_stats) {
+        /*
+	fprintf(gc_data_file, "Sweep started\n");
+        */
+        freed_objects = 0;
+        processed = 0;
+        zombies = 0;
+        freelist_size = 0;
+        MEMZERO((void*)free_counts, unsigned long, T_MASK+1);
+        MEMZERO((void*)live_counts, unsigned long, T_MASK+1);
+    }
+
+    objspace->heap.max_blocks_to_free = heaps_used - (heap_min_slots / HEAP_OBJ_LIMIT);
+    objspace->heap.freed_blocks = 0;
+
     freelist = 0;
     objspace->heap.do_heap_free = (size_t)((heaps_used * HEAP_OBJ_LIMIT) * 0.65);
     objspace->heap.free_min = (size_t)((heaps_used * HEAP_OBJ_LIMIT)  * 0.2);
@@ -2220,8 +2890,13 @@ before_gc_sweep(rb_objspace_t *objspace)
 static void
 after_gc_sweep(rb_objspace_t *objspace)
 {
+    int i;
+    struct timeval tv1;
+
     GC_PROF_SET_MALLOC_INFO;
 
+    if (gc_statistics) gettimeofday(&tv1, NULL);
+
     if (objspace->heap.free_num < objspace->heap.free_min) {
         set_heaps_increment(objspace);
         heaps_increment(objspace);
@@ -2234,6 +2909,29 @@ after_gc_sweep(rb_objspace_t *objspace)
     malloc_increase = 0;
 
     free_unused_heaps(objspace);
+
+    if (gc_statistics) {
+	gc_time_accumulator += elapsed_musecs(tv1);
+
+        if (verbose_gc_stats) {
+            /* log gc stats if requested */
+            fprintf(gc_data_file, "GC time: %lu musec\n", (unsigned long)(gc_time_accumulator-gc_time_accumulator_before_gc));
+            fprintf(gc_data_file, "objects processed: %7lu\n", (unsigned long)processed);
+            fprintf(gc_data_file, "live objects     : %7lu\n", (unsigned long)live_after_last_mark_phase);
+            fprintf(gc_data_file, "freelist objects : %7lu\n", (unsigned long)freelist_size);
+            fprintf(gc_data_file, "freed objects    : %7lu\n", (unsigned long)freed_objects);
+            fprintf(gc_data_file, "zombies          : %7lu\n", (unsigned long)zombies);
+            for(i=0; i<T_MASK; i++) {
+                if (free_counts[i]>0 || live_counts[i]>0) {
+                    fprintf(gc_data_file,
+                            "kept %7lu / freed %7lu objects of type %s\n",
+                            (unsigned long)live_counts[i], (unsigned long)free_counts[i], obj_type((int)i));
+                }
+            }
+            rb_gc_dump();
+            fflush(gc_data_file);
+        }
+    }
 }
 
 static int
@@ -2267,9 +2965,11 @@ rest_sweep(rb_objspace_t *objspace)
 
 static void gc_marks(rb_objspace_t *objspace);
 
+/* only called from rb_new_obj */
 static int
 gc_lazy_sweep(rb_objspace_t *objspace)
 {
+    struct timeval gctv1;
     int res;
     INIT_GC_PROF_PARAMS;
 
@@ -2291,7 +2991,6 @@ gc_lazy_sweep(rb_objspace_t *objspace)
             GC_PROF_TIMER_STOP(Qfalse);
             return res;
         }
-        after_gc_sweep(objspace);
     }
     else {
         if (heaps_increment(objspace)) {
@@ -2299,6 +2998,18 @@ gc_lazy_sweep(rb_objspace_t *objspace)
             return TRUE;
         }
     }
+    after_gc_sweep(objspace);
+
+    if (gc_statistics) {
+        gc_time_accumulator_before_gc = gc_time_accumulator;
+	gc_collections++;
+	gettimeofday(&gctv1, NULL);
+        /*
+	if (verbose_gc_stats) {
+	    fprintf(gc_data_file, "Garbage collection started (gc_lazy_sweep)\n");
+	}
+        */
+    }
 
     gc_marks(objspace);
 
@@ -2307,6 +3018,10 @@ gc_lazy_sweep(rb_objspace_t *objspace)
 	set_heaps_increment(objspace);
     }
 
+    if (gc_statistics) {
+	gc_time_accumulator += elapsed_musecs(gctv1);
+    }
+
     GC_PROF_SWEEP_TIMER_START;
     if(!(res = lazy_sweep(objspace))) {
         after_gc_sweep(objspace);
@@ -2318,6 +3033,7 @@ gc_lazy_sweep(rb_objspace_t *objspace)
     GC_PROF_SWEEP_TIMER_STOP;
 
     GC_PROF_TIMER_STOP(Qtrue);
+
     return res;
 }
 
@@ -2544,9 +3260,15 @@ gc_marks(rb_objspace_t *objspace)
     rb_thread_t *th = GET_THREAD();
     GC_PROF_MARK_TIMER_START;
 
+    /*
+    if (gc_statistics & verbose_gc_stats) {
+        fprintf(gc_data_file, "Marking objects\n");
+    }
+    */
+
     objspace->heap.live_num = 0;
     objspace->count++;
-
+    live_objects = 0;
 
     SET_STACK_END;
 
@@ -2578,11 +3300,15 @@ gc_marks(rb_objspace_t *objspace)
     gc_mark_stacked_objects(objspace);
 
     GC_PROF_MARK_TIMER_STOP;
+
+    live_after_last_mark_phase = objspace->heap.live_num;
 }
 
 static int
 garbage_collect(rb_objspace_t *objspace)
 {
+    struct timeval gctv1;
+
     INIT_GC_PROF_PARAMS;
 
     if (GC_NOTIFY) printf("start garbage_collect()\n");
@@ -2598,15 +3324,31 @@ garbage_collect(rb_objspace_t *objspace)
 
     rest_sweep(objspace);
 
+    if (gc_statistics) {
+        gc_time_accumulator_before_gc = gc_time_accumulator;
+	gc_collections++;
+	gettimeofday(&gctv1, NULL);
+        /*
+	if (verbose_gc_stats) {
+	    fprintf(gc_data_file, "Garbage collection started (garbage_collect)\n");
+	}
+        */
+    }
+
     during_gc++;
     gc_marks(objspace);
 
+    if (gc_statistics) {
+	gc_time_accumulator += elapsed_musecs(gctv1);
+    }
+
     GC_PROF_SWEEP_TIMER_START;
     gc_sweep(objspace);
     GC_PROF_SWEEP_TIMER_STOP;
 
     GC_PROF_TIMER_STOP(Qtrue);
     if (GC_NOTIFY) printf("end garbage_collect()\n");
+
     return TRUE;
 }
 
@@ -3080,6 +3822,39 @@ rb_gc_call_finalizer_at_exit(void)
     rb_objspace_call_finalizer(&rb_objspace);
 }
 
+static const char* obj_type(VALUE type)
+{
+    switch (type) {
+        case T_NIL    : return "NIL";
+        case T_OBJECT : return "OBJECT";
+        case T_CLASS  : return "CLASS";
+        case T_ICLASS : return "ICLASS";
+        case T_MODULE : return "MODULE";
+        case T_FLOAT  : return "FLOAT";
+        case T_COMPLEX: return "COMPLEX";
+        case T_RATIONAL: return "RATIONAL";
+        case T_STRING : return "STRING";
+        case T_REGEXP : return "REGEXP";
+        case T_ARRAY  : return "ARRAY";
+        case T_FIXNUM : return "FIXNUM";
+        case T_HASH   : return "HASH";
+        case T_STRUCT : return "STRUCT";
+        case T_BIGNUM : return "BIGNUM";
+        case T_FILE   : return "FILE";
+
+        case T_TRUE   : return "TRUE";
+        case T_FALSE  : return "FALSE";
+        case T_DATA   : return "DATA";
+        case T_MATCH  : return "MATCH";
+        case T_SYMBOL : return "SYMBOL";
+        case T_ZOMBIE : return "ZOMBIE";
+
+        case T_UNDEF  : return "UNDEF";
+        case T_NODE   : return "NODE";
+        default: return "____";
+    }
+}
+
 static void
 rb_objspace_call_finalizer(rb_objspace_t *objspace)
 {
@@ -3388,6 +4163,49 @@ count_objects(int argc, VALUE *argv, VALUE os)
     return hash;
 }
 
+/* call-seq:
+ *  ObjectSpace.live_objects => number
+ *
+ * Returns the count of objects currently allocated in the system. This goes
+ * down after the garbage collector runs.
+ */
+static
+VALUE os_live_objects(VALUE self)
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    return ULONG2NUM(live_objects);
+}
+
+unsigned long rb_os_live_objects()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    return live_objects;
+}
+
+/* call-seq:
+ *  ObjectSpace.allocated_objects => number
+ *
+ * Returns the count of objects allocated since the Ruby interpreter has
+ * started.  This number can only increase. To know how many objects are
+ * currently allocated, use ObjectSpace::live_objects
+ */
+static
+VALUE os_allocated_objects(VALUE self)
+{
+    rb_objspace_t *objspace = &rb_objspace;
+#if defined(HAVE_LONG_LONG)
+    return ULL2NUM(allocated_objects);
+#else
+    return ULONG2NUM(allocated_objects);
+#endif
+}
+
+unsigned LONG_LONG rb_os_allocated_objects()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    return allocated_objects;
+}
+
 /*
  *  call-seq:
  *     GC.count -> Integer
@@ -3680,6 +4498,28 @@ Init_GC(void)
     rb_define_singleton_method(rb_mGC, "stat", gc_stat, -1);
     rb_define_method(rb_mGC, "garbage_collect", rb_gc_start, 0);
 
+    rb_define_singleton_method(rb_mGC, "enable_stats", rb_gc_enable_stats, 0);
+    rb_define_singleton_method(rb_mGC, "disable_stats", rb_gc_disable_stats, 0);
+    rb_define_singleton_method(rb_mGC, "stats_enabled?", rb_gc_stats_enabled, 0);
+    rb_define_singleton_method(rb_mGC, "clear_stats", rb_gc_clear_stats, 0);
+    rb_define_singleton_method(rb_mGC, "allocated_size", rb_gc_allocated_size, 0);
+    rb_define_singleton_method(rb_mGC, "num_allocations", rb_gc_num_allocations, 0);
+    rb_define_singleton_method(rb_mGC, "heap_slots", rb_gc_heap_slots, 0);
+    rb_define_const(rb_mGC, "HEAP_SLOT_SIZE", INT2FIX(sizeof(RVALUE)));
+
+    rb_define_singleton_method(rb_mGC, "log", rb_gc_log, 1);
+    rb_define_singleton_method(rb_mGC, "log_file", rb_gc_log_file, -1);
+    rb_define_singleton_method(rb_mGC, "enable_trace", rb_gc_enable_trace, 0);
+    rb_define_singleton_method(rb_mGC, "disable_trace", rb_gc_disable_trace, 0);
+    rb_define_singleton_method(rb_mGC, "trace_enabled?", rb_gc_trace_enabled, 0);
+
+    rb_define_singleton_method(rb_mGC, "collections", rb_gc_collections, 0);
+    rb_define_singleton_method(rb_mGC, "time", rb_gc_time, 0);
+    rb_define_singleton_method(rb_mGC, "dump", rb_gc_dump, 0);
+#ifdef GC_DEBUG
+    rb_define_singleton_method(rb_mGC, "dump_file_and_line_info", rb_gc_dump_file_and_line_info, -1);
+#endif
+
     rb_mProfiler = rb_define_module_under(rb_mGC, "Profiler");
     rb_define_singleton_method(rb_mProfiler, "enabled?", gc_profile_enable_get, 0);
     rb_define_singleton_method(rb_mProfiler, "enable", gc_profile_enable, 0);
@@ -3693,6 +4533,9 @@ Init_GC(void)
     rb_define_module_function(rb_mObSpace, "each_object", os_each_obj, -1);
     rb_define_module_function(rb_mObSpace, "garbage_collect", rb_gc_start, 0);
 
+    rb_define_module_function(rb_mObSpace, "live_objects", os_live_objects, 0);
+    rb_define_module_function(rb_mObSpace, "allocated_objects", os_allocated_objects, 0);
+
     rb_define_module_function(rb_mObSpace, "define_finalizer", define_final, -1);
     rb_define_module_function(rb_mObSpace, "undefine_finalizer", undefine_final, 1);
 
diff --git a/eval_error.c b/eval_error.c
index fd06adf..69c3b48 100644
--- a/eval_error.c
+++ b/eval_error.c
@@ -164,8 +164,8 @@ error_print(void)
         int skip = eclass == rb_eSysStackError;
 
 #define TRACE_MAX (TRACE_HEAD+TRACE_TAIL+5)
-#define TRACE_HEAD 8
-#define TRACE_TAIL 5
+#define TRACE_HEAD 100
+#define TRACE_TAIL 100
 
 	for (i = 1; i < len; i++) {
 	    if (TYPE(ptr[i]) == T_STRING) {
diff --git a/gc.c b/gc.c
index feb54f1..7f65b98 100644
--- a/gc.c
+++ b/gc.c
@@ -1345,6 +1345,34 @@ rb_gc_log_file(int argc, VALUE *argv, VALUE self)
 }
 
 /*
+ * Called from process.c before a fork. Flushes the gc log file to
+ * avoid writing the buffered output twice (once in the parent, and
+ * once in the child).
+ */
+void
+rb_gc_before_fork()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    fflush(gc_data_file);
+}
+
+/*
+ * Called from process.c after a fork in the child process. Turns off
+ * logging, disables GC stats and resets all gc counters and timing
+ * information.
+ */
+void
+rb_gc_after_fork()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    rb_gc_disable_stats();
+    rb_gc_clear_stats();
+    rb_gc_disable_trace();
+    gc_data_file = stderr;
+    rb_iv_set(rb_mGC, GC_LOGFILE_IVAR, Qnil);
+}
+
+/*
  *  call-seq:
  *     GC.log String  => String
  *
diff --git a/include/ruby/intern.h b/include/ruby/intern.h
index 5851f20..82350d1 100644
--- a/include/ruby/intern.h
+++ b/include/ruby/intern.h
@@ -435,6 +435,8 @@ void rb_gc_call_finalizer_at_exit(void);
 VALUE rb_gc_enable(void);
 VALUE rb_gc_disable(void);
 VALUE rb_gc_start(void);
+void rb_gc_before_fork _((void));
+void rb_gc_after_fork _((void));
 #define Init_stack(addr) ruby_init_stack(addr)
 void rb_gc_set_params(void);
 /* hash.c */
diff --git a/process.c b/process.c
index 99cfc69..8bee602 100644
--- a/process.c
+++ b/process.c
@@ -2804,9 +2804,11 @@ rb_f_fork(VALUE obj)
     rb_pid_t pid;
 
     rb_secure(2);
+    rb_gc_before_fork();
 
     switch (pid = rb_fork(0, 0, 0, Qnil)) {
       case 0:
+        rb_gc_after_fork();
 	rb_thread_atfork();
 	if (rb_block_given_p()) {
 	    int status;
diff --git a/gc.c b/gc.c
index 7f65b98..8498fe4 100644
--- a/gc.c
+++ b/gc.c
@@ -281,7 +281,6 @@ getrusage_time(void)
 #define GC_PROF_DEC_LIVE_NUM
 #endif
 
-
 #if defined(_MSC_VER) || defined(__BORLANDC__) || defined(__CYGWIN__)
 #pragma pack(push, 1) /* magic for reducing sizeof(RVALUE): 24 -> 20 */
 #endif
@@ -1562,6 +1561,24 @@ rb_gc_time()
 #endif
 }
 
+/*
+ *  call-seq:
+ *     GC.heap_slots_live_after_last_gc    => Integer
+ *
+ *  Returns the number of heap slots which were live after the last garbage collection.
+ *
+ *     GC.heap_slots_live_after_last_gc    #=> 231223
+ *
+ */
+VALUE
+rb_gc_heap_slots_live_after_last_gc()
+{
+    rb_objspace_t *objspace = &rb_objspace;
+    return ULONG2NUM(live_after_last_mark_phase);
+}
+
+
+
 VALUE rb_mGC;
 
 void
@@ -4533,6 +4550,7 @@ Init_GC(void)
     rb_define_singleton_method(rb_mGC, "allocated_size", rb_gc_allocated_size, 0);
     rb_define_singleton_method(rb_mGC, "num_allocations", rb_gc_num_allocations, 0);
     rb_define_singleton_method(rb_mGC, "heap_slots", rb_gc_heap_slots, 0);
+    rb_define_singleton_method(rb_mGC, "heap_slots_live_after_last_gc", rb_gc_heap_slots_live_after_last_gc, 0);
     rb_define_const(rb_mGC, "HEAP_SLOT_SIZE", INT2FIX(sizeof(RVALUE)));
 
     rb_define_singleton_method(rb_mGC, "log", rb_gc_log, 1);
diff --git a/lib/webrick/httpresponse.rb b/lib/webrick/httpresponse.rb
index 0d36c07..ce72b3f 100644
--- a/lib/webrick/httpresponse.rb
+++ b/lib/webrick/httpresponse.rb
@@ -202,7 +202,7 @@ module WEBrick
       if @header['connection'] == "close"
          @keep_alive = false
       elsif keep_alive?
-        if chunked? || @header['content-length']
+        if chunked? || @header['content-length'] || @status == 304 || @status == 204
           @header['connection'] = "Keep-Alive"
         else
           msg = "Could not determine content-length of response body. Set content-length of the response or set Response#chunked = true"
diff --git a/gc.c b/gc.c
index 8498fe4..b5fc83c 100644
--- a/gc.c
+++ b/gc.c
@@ -1052,6 +1052,7 @@ ruby_xfree(void *x)
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_enable(void)
 {
@@ -1074,6 +1075,7 @@ rb_gc_enable(void)
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_disable(void)
 {
@@ -1096,6 +1098,7 @@ rb_gc_disable(void)
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_enable_stats()
 {
@@ -1117,6 +1120,7 @@ rb_gc_enable_stats()
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_disable_stats()
 {
@@ -1136,6 +1140,7 @@ rb_gc_disable_stats()
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_stats_enabled()
 {
@@ -1155,6 +1160,7 @@ rb_gc_stats_enabled()
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_clear_stats()
 {
@@ -1178,6 +1184,7 @@ rb_gc_clear_stats()
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_allocated_size()
 {
@@ -1222,6 +1229,7 @@ rb_gc_num_allocations()
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_enable_trace()
 {
@@ -1243,6 +1251,7 @@ rb_gc_enable_trace()
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_disable_trace()
 {
@@ -1262,6 +1271,7 @@ rb_gc_disable_trace()
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_trace_enabled()
 {
@@ -1290,6 +1300,7 @@ const char* GC_LOGFILE_IVAR = "@gc_logfile_name";
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_log_file(int argc, VALUE *argv, VALUE self)
 {
@@ -1381,6 +1392,7 @@ rb_gc_after_fork()
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_log(self, original_str)
      VALUE self, original_str;
@@ -1407,6 +1419,7 @@ rb_gc_log(self, original_str)
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_dump()
 {
@@ -1437,6 +1450,7 @@ static const char* obj_type(VALUE tp);
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_dump_file_and_line_info(int argc, VALUE *argv)
 {
@@ -1532,6 +1546,7 @@ rb_gc_heap_slots()
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_collections()
 {
@@ -1550,6 +1565,7 @@ rb_gc_collections()
  *
  */
 
+RUBY_FUNC_EXPORTED
 VALUE
 rb_gc_time()
 {
@@ -4245,6 +4261,7 @@ VALUE os_allocated_objects(VALUE self)
 #endif
 }
 
+RUBY_FUNC_EXPORTED
 unsigned LONG_LONG rb_os_allocated_objects()
 {
     rb_objspace_t *objspace = &rb_objspace;
diff --git a/NEWS b/NEWS
index 30fec33..9f6c172 100644
--- a/NEWS
+++ b/NEWS
@@ -103,6 +103,16 @@ with all sufficient information, see the ChangeLog file.
       * String#prepend
       * String#byteslice
 
+  * Thread
+    * added method:
+      * added Thread#thread_variable_get for getting thread local variables
+        (these are different than Fiber local variables).
+      * added Thread#thread_variable_set for setting thread local variables.
+      * added Thread#thread_variables for getting a list of the thread local
+        variable keys.
+      * added Thread#thread_variable? for testing to see if a particular thread
+        variable has been set.
+
   * Time
     * extended method:
       * Time#strftime supports %:z and %::z.
diff --git a/test/ruby/test_thread.rb b/test/ruby/test_thread.rb
index e8cb3b1..9ff491d 100644
--- a/test/ruby/test_thread.rb
+++ b/test/ruby/test_thread.rb
@@ -27,6 +27,79 @@ class TestThread < Test::Unit::TestCase
     end
   end
 
+  def test_main_thread_variable_in_enumerator
+    assert_equal Thread.main, Thread.current
+
+    Thread.current.thread_variable_set :foo, "bar"
+
+    thread, value = Fiber.new {
+      Fiber.yield [Thread.current, Thread.current.thread_variable_get(:foo)]
+    }.resume
+
+    assert_equal Thread.current, thread
+    assert_equal Thread.current.thread_variable_get(:foo), value
+  end
+
+  def test_thread_variable_in_enumerator
+    Thread.new {
+      Thread.current.thread_variable_set :foo, "bar"
+
+      thread, value = Fiber.new {
+        Fiber.yield [Thread.current, Thread.current.thread_variable_get(:foo)]
+      }.resume
+
+      assert_equal Thread.current, thread
+      assert_equal Thread.current.thread_variable_get(:foo), value
+    }.join
+  end
+
+  def test_thread_variables
+    assert_equal [], Thread.new { Thread.current.thread_variables }.join.value
+
+    t = Thread.new {
+      Thread.current.thread_variable_set(:foo, "bar")
+      Thread.current.thread_variables
+    }
+    assert_equal [:foo], t.join.value
+  end
+
+  def test_thread_variable?
+    refute Thread.new { Thread.current.thread_variable?("foo") }.join.value
+    t = Thread.new {
+      Thread.current.thread_variable_set("foo", "bar")
+    }.join
+
+    assert t.thread_variable?("foo")
+    assert t.thread_variable?(:foo)
+    refute t.thread_variable?(:bar)
+  end
+
+  def test_thread_variable_strings_and_symbols_are_the_same_key
+    t = Thread.new {}.join
+    t.thread_variable_set("foo", "bar")
+    assert_equal "bar", t.thread_variable_get(:foo)
+  end
+
+  def test_thread_variable_frozen
+    t = Thread.new { }.join
+    t.freeze
+    assert_raises(RuntimeError) do
+      t.thread_variable_set(:foo, "bar")
+    end
+  end
+
+  def test_thread_variable_security
+    t = Thread.new { sleep }
+
+    assert_raises(SecurityError) do
+      Thread.new { $SAFE = 4; t.thread_variable_get(:foo) }.join
+    end
+
+    assert_raises(SecurityError) do
+      Thread.new { $SAFE = 4; t.thread_variable_set(:foo, :baz) }.join
+    end
+  end
+
   def test_mutex_synchronize
     m = Mutex.new
     r = 0
diff --git a/thread.c b/thread.c
index 387bf1f..1cf340c 100644
--- a/thread.c
+++ b/thread.c
@@ -2125,7 +2125,9 @@ rb_thread_local_aset(VALUE thread, ID id, VALUE val)
  *      thr[sym] = obj   -> obj
  *
  *  Attribute Assignment---Sets or creates the value of a thread-local variable,
- *  using either a symbol or a string. See also <code>Thread#[]</code>.
+ *  using either a symbol or a string. See also <code>Thread#[]</code>.	 For
+ *  thread-local variables, please see <code>Thread#thread_variable_set</code>
+ *  and <code>Thread#thread_variable_get</code>.
  */
 
 static VALUE
@@ -2136,6 +2138,80 @@ rb_thread_aset(VALUE self, VALUE id, VALUE val)
 
 /*
  *  call-seq:
+ *	thr.thread_variable_get(key)  -> obj or nil
+ *
+ *  Returns the value of a thread local variable that has been set.  Note that
+ *  these are different than fiber local values.  For fiber local values,
+ *  please see Thread#[] and Thread#[]=.
+ *
+ *  Thread local values are carried along with threads, and do not respect
+ *  fibers.  For example:
+ *
+ *    Thread.new {
+ *	Thread.current.thread_variable_set("foo", "bar") # set a thread local
+ *	Thread.current["foo"] = "bar"			 # set a fiber local
+ *
+ *	Fiber.new {
+ *	  Fiber.yield [
+ *	    Thread.current.thread_variable_get("foo"), # get the thread local
+ *	    Thread.current["foo"],		       # get the fiber local
+ *	  ]
+ *	}.resume
+ *    }.join.value # => ['bar', nil]
+ *
+ *  The value "bar" is returned for the thread local, where nil is returned
+ *  for the fiber local.  The fiber is executed in the same thread, so the
+ *  thread local values are available.
+ *
+ *  See also Thread#[]
+ */
+
+static VALUE
+rb_thread_variable_get(VALUE thread, VALUE id)
+{
+    VALUE locals;
+    rb_thread_t *th;
+
+    GetThreadPtr(thread, th);
+
+    if (rb_safe_level() >= 4 && th != GET_THREAD()) {
+	rb_raise(rb_eSecurityError, "Insecure: can't access thread locals");
+    }
+
+    locals = rb_iv_get(thread, "locals");
+    return rb_hash_aref(locals, ID2SYM(rb_to_id(id)));
+}
+
+/*
+ *  call-seq:
+ *	thr.thread_variable_set(key, value)
+ *
+ *  Sets a thread local with +key+ to +value+.	Note that these are local to
+ *  threads, and not to fibers.	 Please see Thread#thread_variable_get and
+ *  Thread#[] for more information.
+ */
+
+static VALUE
+rb_thread_variable_set(VALUE thread, VALUE id, VALUE val)
+{
+    VALUE locals;
+    rb_thread_t *th;
+
+    GetThreadPtr(thread, th);
+
+    if (rb_safe_level() >= 4 && th != GET_THREAD()) {
+	rb_raise(rb_eSecurityError, "Insecure: can't modify thread locals");
+    }
+    if (OBJ_FROZEN(thread)) {
+	rb_error_frozen("thread locals");
+    }
+
+    locals = rb_iv_get(thread, "locals");
+    return rb_hash_aset(locals, ID2SYM(rb_to_id(id)), val);
+}
+
+/*
+ *  call-seq:
  *     thr.key?(sym)   -> true or false
  *
  *  Returns <code>true</code> if the given string (or symbol) exists as a
@@ -3006,6 +3082,9 @@ rb_gc_save_machine_context(rb_thread_t *th)
 
 /*
  *
+ *  For thread-local variables, please see <code>Thread#thread_local_get</code>
+ *  and <code>Thread#thread_local_set</code>.
+ *
  */
 
 void
@@ -3208,6 +3287,76 @@ thgroup_list_i(st_data_t key, st_data_t val, st_data_t data)
     return ST_CONTINUE;
 }
 
+static int
+keys_i(VALUE key, VALUE value, VALUE ary)
+{
+    rb_ary_push(ary, key);
+    return ST_CONTINUE;
+}
+
+/*
+ *  call-seq:
+ *     thr.thread_variables   -> array
+ *
+ *  Returns an an array of the names of the thread-local variables (as Symbols).
+ *
+ *     thr = Thread.new do
+ *       Thread.current.thread_variable_set(:cat, 'meow')
+ *       Thread.current.thread_variable_set("dog", 'woof')
+ *     end
+ *     thr.join               #=> #<Thread:0x401b3f10 dead>
+ *     thr.thread_variables   #=> [:dog, :cat]
+ *
+ *  Note that these are not fiber local variables.  Please see Thread#[] and
+ *  Thread#thread_variable_get for more details.
+ */
+
+static VALUE
+rb_thread_variables(VALUE thread)
+{
+    VALUE locals;
+    VALUE ary;
+
+    locals = rb_iv_get(thread, "locals");
+    ary = rb_ary_new();
+    rb_hash_foreach(locals, keys_i, ary);
+
+    return ary;
+}
+
+/*
+ *  call-seq:
+ *     thr.thread_variable?(key)   -> true or false
+ *
+ *  Returns <code>true</code> if the given string (or symbol) exists as a
+ *  thread-local variable.
+ *
+ *     me = Thread.current
+ *     me.thread_variable_set(:oliver, "a")
+ *     me.thread_variable?(:oliver)    #=> true
+ *     me.thread_variable?(:stanley)   #=> false
+ *
+ *  Note that these are not fiber local variables.  Please see Thread#[] and
+ *  Thread#thread_variable_get for more details.
+ */
+
+static VALUE
+rb_thread_variable_p(VALUE thread, VALUE key)
+{
+    VALUE locals;
+
+    locals = rb_iv_get(thread, "locals");
+
+    if (!RHASH(locals)->ntbl)
+        return Qfalse;
+
+    if (st_lookup(RHASH(locals)->ntbl, ID2SYM(rb_to_id(key)), 0)) {
+	return Qtrue;
+    }
+
+    return Qfalse;
+}
+
 /*
  *  call-seq:
  *     thgrp.list   -> array
@@ -4693,6 +4842,10 @@ Init_Thread(void)
     rb_define_method(rb_cThread, "priority", rb_thread_priority, 0);
     rb_define_method(rb_cThread, "priority=", rb_thread_priority_set, 1);
     rb_define_method(rb_cThread, "status", rb_thread_status, 0);
+    rb_define_method(rb_cThread, "thread_variable_get", rb_thread_variable_get, 1);
+    rb_define_method(rb_cThread, "thread_variable_set", rb_thread_variable_set, 2);
+    rb_define_method(rb_cThread, "thread_variables", rb_thread_variables, 0);
+    rb_define_method(rb_cThread, "thread_variable?", rb_thread_variable_p, 1);
     rb_define_method(rb_cThread, "alive?", rb_thread_alive_p, 0);
     rb_define_method(rb_cThread, "stop?", rb_thread_stop_p, 0);
     rb_define_method(rb_cThread, "abort_on_exception", rb_thread_abort_exc, 0);
diff --git a/vm.c b/vm.c
index 4af44b0..80123c8 100644
--- a/vm.c
+++ b/vm.c
@@ -1910,6 +1910,7 @@ ruby_thread_init(VALUE self)
     GetThreadPtr(self, th);
 
     th_init(th, self);
+    rb_iv_set(self, "locals", rb_hash_new());
     th->vm = vm;
 
     th->top_wrapper = 0;
@@ -2180,6 +2181,7 @@ Init_VM(void)
 
 	/* create main thread */
 	th_self = th->self = TypedData_Wrap_Struct(rb_cThread, &thread_data_type, th);
+	rb_iv_set(th_self, "locals", rb_hash_new());
 	vm->main_thread = th;
 	vm->running_thread = th;
 	th->vm = vm;
diff --git a/array.c b/array.c
index e427cb3..c33d106 100644
--- a/array.c
+++ b/array.c
@@ -295,6 +295,22 @@ rb_ary_frozen_p(VALUE ary)
     return Qfalse;
 }
 
+/* This can be used to take a snapshot of an array (with
+   e.g. rb_ary_replace) and check later whether the array has been
+   modified from the snapshot.  The snapshot is cheap, though if
+   something does modify the array it will pay the cost of copying
+   it. */
+VALUE
+rb_ary_shared_with_p(VALUE ary1, VALUE ary2)
+{
+    if (!ARY_EMBED_P(ary1) && ARY_SHARED_P(ary1)
+     && !ARY_EMBED_P(ary2) && ARY_SHARED_P(ary2)
+     && RARRAY(ary1)->as.heap.aux.shared == RARRAY(ary2)->as.heap.aux.shared) {
+	return Qtrue;
+    }
+    return Qfalse;
+}
+
 static VALUE
 ary_alloc(VALUE klass)
 {
diff --git a/file.c b/file.c
index c60de23..d82b6d5 100644
--- a/file.c
+++ b/file.c
@@ -153,23 +153,32 @@ file_path_convert(VALUE name)
     return name;
 }
 
-static VALUE
-rb_get_path_check(VALUE obj, int level)
+VALUE
+rb_get_path_check_to_string(VALUE obj, int level)
 {
     VALUE tmp;
     ID to_path;
-    rb_encoding *enc;
 
     if (insecure_obj_p(obj, level)) {
 	rb_insecure_operation();
     }
 
+    if (RB_TYPE_P(obj, T_STRING)) {
+	return obj;
+    }
     CONST_ID(to_path, "to_path");
     tmp = rb_check_funcall(obj, to_path, 0, 0);
     if (tmp == Qundef) {
 	tmp = obj;
     }
     StringValue(tmp);
+    return tmp;
+}
+
+VALUE
+rb_get_path_check_convert(VALUE obj, VALUE tmp, int level)
+{
+    rb_encoding *enc;
 
     tmp = file_path_convert(tmp);
     if (obj != tmp && insecure_obj_p(tmp, level)) {
@@ -187,6 +196,13 @@ rb_get_path_check(VALUE obj, int level)
     return rb_str_new4(tmp);
 }
 
+static VALUE
+rb_get_path_check(VALUE obj, int level)
+{
+    VALUE tmp = rb_get_path_check_to_string(obj, level);
+    return rb_get_path_check_convert(obj, tmp, level);
+}
+
 VALUE
 rb_get_path_no_checksafe(VALUE obj)
 {
@@ -3254,7 +3270,6 @@ rb_file_expand_path(VALUE fname, VALUE dname)
 VALUE
 rb_file_expand_path_fast(VALUE fname, VALUE dname)
 {
-    check_expand_path_args(fname, dname);
     return rb_file_expand_path_internal(fname, dname, 0, 0, EXPAND_PATH_BUFFER());
 }
 
@@ -5245,7 +5260,7 @@ rb_find_file_ext_safe(VALUE *filep, const char *const *ext, int safe_level)
 	rb_raise(rb_eSecurityError, "loading from non-absolute path %s", f);
     }
 
-    RB_GC_GUARD(load_path) = rb_get_load_path();
+    RB_GC_GUARD(load_path) = rb_get_expanded_load_path();
     if (!load_path) return 0;
 
     fname = rb_str_dup(*filep);
@@ -5310,7 +5325,7 @@ rb_find_file_safe(VALUE path, int safe_level)
 	rb_raise(rb_eSecurityError, "loading from non-absolute path %s", f);
     }
 
-    RB_GC_GUARD(load_path) = rb_get_load_path();
+    RB_GC_GUARD(load_path) = rb_get_expanded_load_path();
     if (load_path) {
 	long i;
 
diff --git a/hash.c b/hash.c
index 0a5f6a6..6642193 100644
--- a/hash.c
+++ b/hash.c
@@ -1079,7 +1079,7 @@ clear_i(VALUE key, VALUE value, VALUE dummy)
  *
  */
 
-static VALUE
+VALUE
 rb_hash_clear(VALUE hash)
 {
     rb_hash_modify_check(hash);
diff --git a/include/ruby/intern.h b/include/ruby/intern.h
index 82350d1..906661e 100644
--- a/include/ruby/intern.h
+++ b/include/ruby/intern.h
@@ -65,6 +65,7 @@ VALUE rb_ary_tmp_new(long);
 void rb_ary_free(VALUE);
 void rb_ary_modify(VALUE);
 VALUE rb_ary_freeze(VALUE);
+VALUE rb_ary_shared_with_p(VALUE, VALUE);
 VALUE rb_ary_aref(int, VALUE*, VALUE);
 VALUE rb_ary_subseq(VALUE, long, long);
 void rb_ary_store(VALUE, long, VALUE);
@@ -452,6 +453,7 @@ VALUE rb_hash_lookup(VALUE, VALUE);
 VALUE rb_hash_lookup2(VALUE, VALUE, VALUE);
 VALUE rb_hash_fetch(VALUE, VALUE);
 VALUE rb_hash_aset(VALUE, VALUE, VALUE);
+VALUE rb_hash_clear(VALUE);
 VALUE rb_hash_delete_if(VALUE);
 VALUE rb_hash_delete(VALUE,VALUE);
 typedef VALUE rb_hash_update_func(VALUE newkey, VALUE oldkey, VALUE value);
diff --git a/internal.h b/internal.h
index 59c9284..f5af903 100644
--- a/internal.h
+++ b/internal.h
@@ -94,6 +94,8 @@ VALUE rb_home_dir(const char *user, VALUE result);
 VALUE rb_realpath_internal(VALUE basedir, VALUE path, int strict);
 VALUE rb_file_expand_path_fast(VALUE, VALUE);
 VALUE rb_file_expand_path_internal(VALUE, VALUE, int, int, VALUE);
+VALUE rb_get_path_check_to_string(VALUE, int);
+VALUE rb_get_path_check_convert(VALUE, VALUE, int);
 void Init_File(void);
 
 #ifdef _WIN32
@@ -119,6 +121,7 @@ VALUE rb_iseq_clone(VALUE iseqval, VALUE newcbase);
 
 /* load.c */
 VALUE rb_get_load_path(void);
+VALUE rb_get_expanded_load_path(void);
 
 /* math.c */
 VALUE rb_math_atan2(VALUE, VALUE);
diff --git a/load.c b/load.c
index 163ec4c..e766880 100644
--- a/load.c
+++ b/load.c
@@ -18,7 +18,6 @@ VALUE ruby_dln_librefs;
 #define IS_DLEXT(e) (strcmp((e), DLEXT) == 0)
 #endif
 
-
 static const char *const loadable_ext[] = {
     ".rb", DLEXT,
 #ifdef DLEXT2
@@ -34,21 +33,120 @@ rb_get_load_path(void)
     return load_path;
 }
 
-VALUE
-rb_get_expanded_load_path(void)
+enum expand_type {
+    EXPAND_ALL,
+    EXPAND_RELATIVE,
+    EXPAND_HOME,
+    EXPAND_NON_CACHE
+};
+
+/* Construct expanded load path and store it to cache.
+   We rebuild load path partially if the cache is invalid.
+   We don't cache non string object and expand it every times. We ensure that
+   string objects in $LOAD_PATH are frozen.
+ */
+static void
+rb_construct_expanded_load_path(int type, int *has_relative, int *has_non_cache)
 {
-    VALUE load_path = rb_get_load_path();
+    rb_vm_t *vm = GET_VM();
+    VALUE load_path = vm->load_path;
+    VALUE expanded_load_path = vm->expanded_load_path;
     VALUE ary;
     long i;
+    int level = rb_safe_level();
 
     ary = rb_ary_new2(RARRAY_LEN(load_path));
     for (i = 0; i < RARRAY_LEN(load_path); ++i) {
-	VALUE path = rb_file_expand_path_fast(RARRAY_PTR(load_path)[i], Qnil);
-	rb_str_freeze(path);
-	rb_ary_push(ary, path);
+	VALUE path, as_str, expanded_path;
+	int is_string, non_cache;
+	char *as_cstr;
+	as_str = path = RARRAY_PTR(load_path)[i];
+	is_string = RB_TYPE_P(path, T_STRING) ? 1 : 0;
+	non_cache = !is_string ? 1 : 0;
+	as_str = rb_get_path_check_to_string(path, level);
+	as_cstr = RSTRING_PTR(as_str);
+
+	if (!non_cache) {
+	    if ((type == EXPAND_RELATIVE &&
+		    rb_is_absolute_path(as_cstr)) ||
+		(type == EXPAND_HOME &&
+		    (!as_cstr[0] || as_cstr[0] != '~')) ||
+		(type == EXPAND_NON_CACHE)) {
+		    /* Use cached expanded path. */
+		    rb_ary_push(ary, RARRAY_PTR(expanded_load_path)[i]);
+		    continue;
+	    }
+	}
+	if (!*has_relative && !rb_is_absolute_path(as_cstr))
+	    *has_relative = 1;
+	if (!*has_non_cache && non_cache)
+	    *has_non_cache = 1;
+	/* Freeze only string object. We expand other objects every times. */
+	if (is_string)
+	    rb_str_freeze(path);
+	as_str = rb_get_path_check_convert(path, as_str, level);
+	expanded_path = rb_file_expand_path_fast(as_str, Qnil);
+	rb_str_freeze(expanded_path);
+	rb_ary_push(ary, expanded_path);
     }
     rb_obj_freeze(ary);
-    return ary;
+    vm->expanded_load_path = ary;
+    rb_ary_replace(vm->load_path_snapshot, vm->load_path);
+}
+
+static VALUE
+load_path_getcwd(void)
+{
+    char *cwd = my_getcwd();
+    VALUE cwd_str = rb_filesystem_str_new_cstr(cwd);
+    xfree(cwd);
+    return cwd_str;
+}
+
+VALUE
+rb_get_expanded_load_path(void)
+{
+    rb_vm_t *vm = GET_VM();
+    const VALUE non_cache = Qtrue;
+
+    if (!rb_ary_shared_with_p(vm->load_path_snapshot, vm->load_path)) {
+	/* The load path was modified. Rebuild the expanded load path. */
+	int has_relative = 0, has_non_cache = 0;
+	rb_construct_expanded_load_path(EXPAND_ALL, &has_relative, &has_non_cache);
+	if (has_relative) {
+	    vm->load_path_check_cache = load_path_getcwd();
+	}
+	else if (has_non_cache) {
+	    /* Non string object. */
+	    vm->load_path_check_cache = non_cache;
+	}
+	else {
+	    vm->load_path_check_cache = 0;
+	}
+    }
+    else if (vm->load_path_check_cache == non_cache) {
+	int has_relative = 1, has_non_cache = 1;
+	/* Expand only non-cacheable objects. */
+	rb_construct_expanded_load_path(EXPAND_NON_CACHE,
+					&has_relative, &has_non_cache);
+    }
+    else if (vm->load_path_check_cache) {
+	int has_relative = 1, has_non_cache = 1;
+	VALUE cwd = load_path_getcwd();
+	if (!rb_str_equal(vm->load_path_check_cache, cwd)) {
+	    /* Current working directory or filesystem encoding was changed.
+	       Expand relative load path and non-cacheable objects again. */
+	    vm->load_path_check_cache = cwd;
+	    rb_construct_expanded_load_path(EXPAND_RELATIVE,
+					    &has_relative, &has_non_cache);
+	}
+	else {
+	    /* Expand only tilde (User HOME) and non-cacheable objects. */
+	    rb_construct_expanded_load_path(EXPAND_HOME,
+					    &has_relative, &has_non_cache);
+	}
+    }
+    return vm->expanded_load_path;
 }
 
 static VALUE
@@ -63,12 +161,121 @@ get_loaded_features(void)
     return GET_VM()->loaded_features;
 }
 
+static void
+reset_loaded_features_snapshot(void)
+{
+    rb_vm_t *vm = GET_VM();
+    rb_ary_replace(vm->loaded_features_snapshot, vm->loaded_features);
+}
+
+static VALUE
+get_loaded_features_index_raw(void)
+{
+    return GET_VM()->loaded_features_index;
+}
+
 static st_table *
 get_loading_table(void)
 {
     return GET_VM()->loading_table;
 }
 
+static void
+features_index_add_single(VALUE short_feature, VALUE offset)
+{
+    VALUE features_index, this_feature_index;
+    features_index = get_loaded_features_index_raw();
+    if ((this_feature_index = rb_hash_lookup(features_index, short_feature)) == Qnil) {
+	this_feature_index = rb_ary_new();
+	rb_hash_aset(features_index, short_feature, this_feature_index);
+    }
+    rb_ary_push(this_feature_index, offset);
+}
+
+/* Add to the loaded-features index all the required entries for
+   `feature`, located at `offset` in $LOADED_FEATURES.  We add an
+   index entry at each string `short_feature` for which
+     feature == "#{prefix}#{short_feature}#{e}"
+   where `e` is empty or matches %r{^\.[^./]*$}, and `prefix` is empty
+   or ends in '/'.  This maintains the invariant that `rb_feature_p()`
+   relies on for its fast lookup.
+*/
+static void
+features_index_add(VALUE feature, VALUE offset)
+{
+    VALUE short_feature;
+    const char *feature_str, *feature_end, *ext, *p;
+
+    feature_str = StringValuePtr(feature);
+    feature_end = feature_str + RSTRING_LEN(feature);
+
+    for (ext = feature_end; ext > feature_str; ext--)
+      if (*ext == '.' || *ext == '/')
+	break;
+    if (*ext != '.')
+      ext = NULL;
+    /* Now `ext` points to the only string matching %r{^\.[^./]*$} that is
+       at the end of `feature`, or is NULL if there is no such string. */
+
+    p = ext ? ext : feature_end;
+    while (1) {
+	p--;
+	while (p >= feature_str && *p != '/')
+	    p--;
+	if (p < feature_str)
+	    break;
+	/* Now *p == '/'.  We reach this point for every '/' in `feature`. */
+	short_feature = rb_str_substr(feature, p + 1 - feature_str, feature_end - p - 1);
+	features_index_add_single(short_feature, offset);
+	if (ext) {
+	    short_feature = rb_str_substr(feature, p + 1 - feature_str, ext - p - 1);
+	    features_index_add_single(short_feature, offset);
+	}
+    }
+    features_index_add_single(feature, offset);
+    if (ext) {
+	short_feature = rb_str_substr(feature, 0, ext - feature_str);
+	features_index_add_single(short_feature, offset);
+    }
+}
+
+static VALUE
+get_loaded_features_index(void)
+{
+    VALUE features;
+    int i;
+    rb_vm_t *vm = GET_VM();
+
+    if (!rb_ary_shared_with_p(vm->loaded_features_snapshot, vm->loaded_features)) {
+	/* The sharing was broken; something (other than us in rb_provide_feature())
+	   modified loaded_features.  Rebuild the index. */
+	rb_hash_clear(vm->loaded_features_index);
+	features = vm->loaded_features;
+	for (i = 0; i < RARRAY_LEN(features); i++) {
+	    VALUE entry, as_str;
+	    as_str = entry = rb_ary_entry(features, i);
+	    StringValue(as_str);
+	    if (as_str != entry)
+		rb_ary_store(features, i, as_str);
+	    rb_str_freeze(as_str);
+	    features_index_add(as_str, INT2FIX(i));
+	}
+	reset_loaded_features_snapshot();
+    }
+    return vm->loaded_features_index;
+}
+
+/* This searches `load_path` for a value such that
+     name == "#{load_path[i]}/#{feature}"
+   if `feature` is a suffix of `name`, or otherwise
+     name == "#{load_path[i]}/#{feature}#{ext}"
+   for an acceptable string `ext`.  It returns
+   `load_path[i].to_str` if found, else 0.
+
+   If type is 's', then `ext` is acceptable only if IS_DLEXT(ext);
+   if 'r', then only if IS_RBEXT(ext); otherwise `ext` may be absent
+   or have any value matching `%r{^\.[^./]*$}`.
+*/
 static VALUE
 loaded_feature_path(const char *name, long vlen, const char *feature, long len,
 		    int type, VALUE load_path)
@@ -77,7 +284,7 @@ loaded_feature_path(const char *name, long vlen, const char *feature, long len,
     long plen;
     const char *e;
 
-    if(vlen < len) return 0;
+    if (vlen < len+1) return 0;
     if (!strncmp(name+(vlen-len),feature,len)){
 	plen = vlen - len - 1;
     } else {
@@ -88,23 +295,22 @@ loaded_feature_path(const char *name, long vlen, const char *feature, long len,
 	    return 0;
 	plen = e - name - len - 1;
     }
+    if (type == 's' && !IS_DLEXT(&name[plen+len+1])
+     || type == 'r' && !IS_RBEXT(&name[plen+len+1])
+     || name[plen] != '/') {
+       return 0;
+    }
+    /* Now name == "#{prefix}/#{feature}#{ext}" where ext is acceptable
+       (possibly empty) and prefix is some string of length plen. */
+
     for (i = 0; i < RARRAY_LEN(load_path); ++i) {
 	VALUE p = RARRAY_PTR(load_path)[i];
 	const char *s = StringValuePtr(p);
 	long n = RSTRING_LEN(p);
 
 	if (n != plen ) continue;
-	if (n && (strncmp(name, s, n) || name[n] != '/')) continue;
-	switch (type) {
-	  case 's':
-	    if (IS_DLEXT(&name[n+len+1])) return p;
-	    break;
-	  case 'r':
-	    if (IS_RBEXT(&name[n+len+1])) return p;
-	    break;
-	  default:
-	    return p;
-	}
+        if (n && strncmp(name, s, n)) continue;
+        return p;
     }
     return 0;
 }
@@ -132,7 +338,7 @@ loaded_feature_path_i(st_data_t v, st_data_t b, st_data_t f)
 static int
 rb_feature_p(const char *feature, const char *ext, int rb, int expanded, const char **fn)
 {
-    VALUE v, features, p, load_path = 0;
+    VALUE features, features_index, feature_val, this_feature_index, v, p, load_path = 0;
     const char *f, *e;
     long i, len, elen, n;
     st_table *loading_tbl;
@@ -151,8 +357,39 @@ rb_feature_p(const char *feature, const char *ext, int rb, int expanded, const c
 	type = 0;
     }
     features = get_loaded_features();
-    for (i = 0; i < RARRAY_LEN(features); ++i) {
-	v = RARRAY_PTR(features)[i];
+    features_index = get_loaded_features_index();
+
+    feature_val = rb_str_new(feature, len);
+    this_feature_index = rb_hash_lookup(features_index, feature_val);
+    /* We search `features` for an entry such that either
+         "#{features[i]}" == "#{load_path[j]}/#{feature}#{e}"
+       for some j, or
+         "#{features[i]}" == "#{feature}#{e}"
+       Here `e` is an "allowed" extension -- either empty or one
+       of the extensions accepted by IS_RBEXT, IS_SOEXT, or
+       IS_DLEXT.  Further, if `ext && rb` then `IS_RBEXT(e)`,
+       and if `ext && !rb` then `IS_SOEXT(e) || IS_DLEXT(e)`.
+
+       If `expanded`, then only the latter form (without load_path[j])
+       is accepted.  Otherwise either form is accepted, *unless* `ext`
+       is false and an otherwise-matching entry of the first form is
+       preceded by an entry of the form
+         "#{features[i2]}" == "#{load_path[j2]}/#{feature}#{e2}"
+       where `e2` matches %r{^\.[^./]*$} but is not an allowed extension.
+       After a "distractor" entry of this form, only entries of the
+       form "#{feature}#{e}" are accepted.
+
+       In `rb_provide_feature()` and `get_loaded_features_index()` we
+       maintain an invariant that the array `this_feature_index` will
+       point to every entry in `features` which has the form
+         "#{prefix}#{feature}#{e}"
+       where `e` is empty or matches %r{^\.[^./]*$}, and `prefix` is empty
+       or ends in '/'.  This includes both match forms above, as well
+       as any distractors, so we may ignore all other entries in `features`.
+     */
+    for (i = 0; this_feature_index != Qnil && i < RARRAY_LEN(this_feature_index); i++) {
+	long index = FIX2LONG(rb_ary_entry(this_feature_index, i));
+	v = RARRAY_PTR(features)[index];
 	f = StringValuePtr(v);
 	if ((n = RSTRING_LEN(v)) < len) continue;
 	if (strncmp(f, feature, len) != 0) {
@@ -175,6 +412,7 @@ rb_feature_p(const char *feature, const char *ext, int rb, int expanded, const c
 	    return 'r';
 	}
     }
+
     loading_tbl = get_loading_table();
     if (loading_tbl) {
 	f = 0;
@@ -183,7 +421,7 @@ rb_feature_p(const char *feature, const char *ext, int rb, int expanded, const c
 	    fs.name = feature;
 	    fs.len = len;
 	    fs.type = type;
-	    fs.load_path = load_path ? load_path : rb_get_load_path();
+	    fs.load_path = load_path ? load_path : rb_get_expanded_load_path();
 	    fs.result = 0;
 	    st_foreach(loading_tbl, loaded_feature_path_i, (st_data_t)&fs);
 	    if ((f = fs.result) != 0) {
@@ -233,7 +471,7 @@ rb_feature_provided(const char *feature, const char **loading)
 
     if (*feature == '.' &&
 	(feature[1] == '/' || strncmp(feature+1, "./", 2) == 0)) {
-	fullpath = rb_file_expand_path_fast(rb_str_new2(feature), Qnil);
+	fullpath = rb_file_expand_path_fast(rb_get_path(rb_str_new2(feature)), Qnil);
 	feature = RSTRING_PTR(fullpath);
     }
     if (ext && !strchr(ext, '/')) {
@@ -254,11 +492,18 @@ rb_feature_provided(const char *feature, const char **loading)
 static void
 rb_provide_feature(VALUE feature)
 {
-    if (OBJ_FROZEN(get_loaded_features())) {
+    VALUE features;
+
+    features = get_loaded_features();
+    if (OBJ_FROZEN(features)) {
 	rb_raise(rb_eRuntimeError,
 		 "$LOADED_FEATURES is frozen; cannot append feature");
     }
-    rb_ary_push(get_loaded_features(), feature);
+    rb_str_freeze(feature);
+
+    rb_ary_push(features, feature);
+    features_index_add(feature, INT2FIX(RARRAY_LEN(features)-1));
+    reset_loaded_features_snapshot();
 }
 
 void
@@ -774,10 +1019,15 @@ Init_load()
     rb_alias_variable(rb_intern("$-I"), id_load_path);
     rb_alias_variable(rb_intern("$LOAD_PATH"), id_load_path);
     vm->load_path = rb_ary_new();
+    vm->expanded_load_path = rb_ary_new();
+    vm->load_path_snapshot = rb_ary_new();
+    vm->load_path_check_cache = 0;
 
     rb_define_virtual_variable("$\"", get_loaded_features, 0);
     rb_define_virtual_variable("$LOADED_FEATURES", get_loaded_features, 0);
     vm->loaded_features = rb_ary_new();
+    vm->loaded_features_snapshot = rb_ary_new();
+    vm->loaded_features_index = rb_hash_new();
 
     rb_define_global_function("load", rb_f_load, -1);
     rb_define_global_function("require", rb_f_require, 1);
diff --git a/ruby.c b/ruby.c
index 3ddd96c..7ffc78e 100644
--- a/ruby.c
+++ b/ruby.c
@@ -1366,7 +1366,8 @@ process_options(int argc, char **argv, struct cmdline_options *opt)
 	long i;
 	VALUE load_path = GET_VM()->load_path;
 	for (i = 0; i < RARRAY_LEN(load_path); ++i) {
-	    rb_enc_associate(RARRAY_PTR(load_path)[i], lenc);
+	    RARRAY_PTR(load_path)[i] =
+		rb_enc_associate(rb_str_dup(RARRAY_PTR(load_path)[i]), lenc);
 	}
     }
     if (!(opt->disable & DISABLE_BIT(gems))) {
diff --git a/test/ruby/test_require.rb b/test/ruby/test_require.rb
index 58a9ee2..ec75096 100644
--- a/test/ruby/test_require.rb
+++ b/test/ruby/test_require.rb
@@ -356,4 +356,114 @@ class TestRequire < Test::Unit::TestCase
     $:.replace(loadpath)
     $".replace(features)
   end
+
+  def test_require_changed_current_dir
+    bug7158 = '[ruby-core:47970]'
+    Dir.mktmpdir {|tmp|
+      Dir.chdir(tmp) {
+        Dir.mkdir("a")
+        Dir.mkdir("b")
+        open(File.join("a", "foo.rb"), "w") {}
+        open(File.join("b", "bar.rb"), "w") {|f|
+          f.puts "p :ok"
+        }
+        assert_in_out_err([], <<-INPUT, %w(:ok), [], bug7158)
+          $: << "."
+          Dir.chdir("a")
+          require "foo"
+          Dir.chdir("../b")
+          p :ng unless require "bar"
+          Dir.chdir("..")
+          p :ng if require "b/bar"
+        INPUT
+      }
+    }
+  end
+
+  def test_require_not_modified_load_path
+    bug7158 = '[ruby-core:47970]'
+    Dir.mktmpdir {|tmp|
+      Dir.chdir(tmp) {
+        open("foo.rb", "w") {}
+        assert_in_out_err([], <<-INPUT, %w(:ok), [], bug7158)
+          a = Object.new
+          def a.to_str
+            "#{tmp}"
+          end
+          $: << a
+          require "foo"
+          last_path = $:.pop
+          p :ok if last_path == a && last_path.class == Object
+        INPUT
+      }
+    }
+  end
+
+  def test_require_changed_home
+    bug7158 = '[ruby-core:47970]'
+    Dir.mktmpdir {|tmp|
+      Dir.chdir(tmp) {
+        open("foo.rb", "w") {}
+        Dir.mkdir("a")
+        open(File.join("a", "bar.rb"), "w") {}
+        assert_in_out_err([], <<-INPUT, %w(:ok), [], bug7158)
+          $: << '~'
+          ENV['HOME'] = "#{tmp}"
+          require "foo"
+          ENV['HOME'] = "#{tmp}/a"
+          p :ok if require "bar"
+        INPUT
+      }
+    }
+  end
+
+  def test_require_to_path_redefined_in_load_path
+    bug7158 = '[ruby-core:47970]'
+    Dir.mktmpdir {|tmp|
+      Dir.chdir(tmp) {
+        open("foo.rb", "w") {}
+        assert_in_out_err(["RUBYOPT"=>nil], <<-INPUT, %w(:ok), [], bug7158)
+          a = Object.new
+          def a.to_path
+            "bar"
+          end
+          $: << a
+          begin
+            require "foo"
+            p :ng
+          rescue LoadError
+          end
+          def a.to_path
+            "#{tmp}"
+          end
+          p :ok if require "foo"
+        INPUT
+      }
+    }
+  end
+
+  def test_require_to_str_redefined_in_load_path
+    bug7158 = '[ruby-core:47970]'
+    Dir.mktmpdir {|tmp|
+      Dir.chdir(tmp) {
+        open("foo.rb", "w") {}
+        assert_in_out_err(["RUBYOPT"=>nil], <<-INPUT, %w(:ok), [], bug7158)
+          a = Object.new
+          def a.to_str
+            "foo"
+          end
+          $: << a
+          begin
+            require "foo"
+            p :ng
+          rescue LoadError
+          end
+          def a.to_str
+            "#{tmp}"
+          end
+          p :ok if require "foo"
+        INPUT
+      }
+    }
+  end
 end
diff --git a/vm.c b/vm.c
index 80123c8..97dc21b 100644
--- a/vm.c
+++ b/vm.c
@@ -1592,7 +1592,12 @@ rb_vm_mark(void *ptr)
 	RUBY_MARK_UNLESS_NULL(vm->thgroup_default);
 	RUBY_MARK_UNLESS_NULL(vm->mark_object_ary);
 	RUBY_MARK_UNLESS_NULL(vm->load_path);
+	RUBY_MARK_UNLESS_NULL(vm->load_path_snapshot);
+	RUBY_MARK_UNLESS_NULL(vm->load_path_check_cache);
+	RUBY_MARK_UNLESS_NULL(vm->expanded_load_path);
 	RUBY_MARK_UNLESS_NULL(vm->loaded_features);
+	RUBY_MARK_UNLESS_NULL(vm->loaded_features_snapshot);
+	RUBY_MARK_UNLESS_NULL(vm->loaded_features_index);
 	RUBY_MARK_UNLESS_NULL(vm->top_self);
 	RUBY_MARK_UNLESS_NULL(vm->coverages);
 	rb_gc_mark_locations(vm->special_exceptions, vm->special_exceptions + ruby_special_error_count);
diff --git a/vm_core.h b/vm_core.h
index dfc0e3c..70c5f5c 100644
--- a/vm_core.h
+++ b/vm_core.h
@@ -299,7 +299,12 @@ typedef struct rb_vm_struct {
     /* load */
     VALUE top_self;
     VALUE load_path;
+    VALUE load_path_snapshot;
+    VALUE load_path_check_cache;
+    VALUE expanded_load_path;
     VALUE loaded_features;
+    VALUE loaded_features_snapshot;
+    VALUE loaded_features_index;
     struct st_table *loading_table;
 
     /* signal */
diff --git a/common.mk b/common.mk
index ccc9647..1d8e442 100644
--- a/common.mk
+++ b/common.mk
@@ -638,7 +638,8 @@ file.$(OBJEXT): {$(VPATH)}file.c $(RUBY_H_INCLUDES) {$(VPATH)}io.h \
 gc.$(OBJEXT): {$(VPATH)}gc.c $(RUBY_H_INCLUDES) {$(VPATH)}re.h \
   {$(VPATH)}regex.h $(ENCODING_H_INCLUDES) $(VM_CORE_H_INCLUDES) \
   {$(VPATH)}gc.h {$(VPATH)}io.h {$(VPATH)}eval_intern.h {$(VPATH)}util.h \
-  {$(VPATH)}debug.h {$(VPATH)}internal.h {$(VPATH)}constant.h
+  {$(VPATH)}debug.h {$(VPATH)}internal.h {$(VPATH)}constant.h \
+  {$(VPATH)}pool_alloc.inc.h {$(VPATH)}pool_alloc.h
 hash.$(OBJEXT): {$(VPATH)}hash.c $(RUBY_H_INCLUDES) {$(VPATH)}util.h \
   $(ENCODING_H_INCLUDES)
 inits.$(OBJEXT): {$(VPATH)}inits.c $(RUBY_H_INCLUDES) \
@@ -702,7 +703,7 @@ signal.$(OBJEXT): {$(VPATH)}signal.c $(RUBY_H_INCLUDES) \
   $(VM_CORE_H_INCLUDES) {$(VPATH)}debug.h
 sprintf.$(OBJEXT): {$(VPATH)}sprintf.c $(RUBY_H_INCLUDES) {$(VPATH)}re.h \
   {$(VPATH)}regex.h {$(VPATH)}vsnprintf.c $(ENCODING_H_INCLUDES)
-st.$(OBJEXT): {$(VPATH)}st.c $(RUBY_H_INCLUDES)
+st.$(OBJEXT): {$(VPATH)}st.c $(RUBY_H_INCLUDES) {$(VPATH)}pool_alloc.h
 strftime.$(OBJEXT): {$(VPATH)}strftime.c $(RUBY_H_INCLUDES) \
   {$(VPATH)}timev.h
 string.$(OBJEXT): {$(VPATH)}string.c $(RUBY_H_INCLUDES) {$(VPATH)}re.h \
diff --git a/configure.in b/configure.in
index 4aefd2d..547b6d9 100644
--- a/configure.in
+++ b/configure.in
@@ -1285,6 +1285,30 @@ if test $rb_cv_stack_end_address != no; then
   AC_DEFINE_UNQUOTED(STACK_END_ADDRESS, $rb_cv_stack_end_address)
 fi
 
+AS_CASE(["$target_os"],
+[openbsd*], [
+  AC_CACHE_CHECK(for heap align log on openbsd, rb_cv_page_size_log,
+    [rb_cv_page_size_log=no
+     for page_log in 12 13; do
+       AC_TRY_RUN([
+#include <math.h>
+#include <unistd.h>
+
+int
+main() {
+  if ((int)log2((double)sysconf(_SC_PAGESIZE)) != $page_log) return 1;
+  return 0;
+}
+       ],
+       rb_cv_page_size_log="$page_log"; break)
+     done])
+  if test $rb_cv_page_size_log != no; then
+    AC_DEFINE_UNQUOTED(HEAP_ALIGN_LOG, $rb_cv_page_size_log)
+  else
+    AC_DEFINE_UNQUOTED(HEAP_ALIGN_LOG, 12)
+  fi
+])
+
 dnl Checks for library functions.
 AC_TYPE_GETGROUPS
 AC_TYPE_SIGNAL
@@ -1384,7 +1408,8 @@ AC_CHECK_FUNCS(fmod killpg wait4 waitpid fork spawnv syscall __syscall chroot ge
 	      setsid telldir seekdir fchmod cosh sinh tanh log2 round\
 	      setuid setgid daemon select_large_fdset setenv unsetenv\
               mktime timegm gmtime_r clock_gettime gettimeofday poll ppoll\
-              pread sendfile shutdown sigaltstack dl_iterate_phdr)
+              pread sendfile shutdown sigaltstack dl_iterate_phdr\
+              posix_memalign memalign)
 
 AC_CACHE_CHECK(for unsetenv returns a value, rb_cv_unsetenv_return_value,
   [AC_TRY_COMPILE([
diff --git a/ext/-test-/st/numhash/numhash.c b/ext/-test-/st/numhash/numhash.c
index e186cd4..53d9e1b 100644
--- a/ext/-test-/st/numhash/numhash.c
+++ b/ext/-test-/st/numhash/numhash.c
@@ -54,7 +54,7 @@ numhash_i(st_data_t key, st_data_t value, st_data_t arg, int error)
 static VALUE
 numhash_each(VALUE self)
 {
-    return st_foreach((st_table *)DATA_PTR(self), numhash_i, self) ? Qtrue : Qfalse;
+    return st_foreach_check((st_table *)DATA_PTR(self), numhash_i, self, 0) ? Qtrue : Qfalse;
 }
 
 void
diff --git a/gc.c b/gc.c
index b5fc83c..3110059 100644
--- a/gc.c
+++ b/gc.c
@@ -20,6 +20,7 @@
 #include "vm_core.h"
 #include "internal.h"
 #include "gc.h"
+#include "pool_alloc.h"
 #include "constant.h"
 #include "ruby_atomic.h"
 #include <stdio.h>
@@ -37,7 +38,12 @@
 
 #if defined _WIN32 || defined __CYGWIN__
 #include <windows.h>
+#elif defined(HAVE_POSIX_MEMALIGN)
+#elif defined(HAVE_MEMALIGN)
+#include <malloc.h>
 #endif
+static void aligned_free(void *);
+static void *aligned_malloc(size_t alignment, size_t size);
 
 #ifdef HAVE_VALGRIND_MEMCHECK_H
 # include <valgrind/memcheck.h>
@@ -356,6 +362,24 @@ typedef struct mark_stack {
 
 #define CALC_EXACT_MALLOC_SIZE 0
 
+#ifdef POOL_ALLOC_API
+/* POOL ALLOC API */
+#define POOL_ALLOC_PART 1
+#include "pool_alloc.inc.h"
+#undef POOL_ALLOC_PART
+
+typedef struct pool_layout_t pool_layout_t;
+struct pool_layout_t {
+    pool_header
+      p6,  /* st_table && st_table_entry */
+      p11;  /* st_table.bins init size */
+} pool_layout = {
+    INIT_POOL(void*[6]),
+    INIT_POOL(void*[11])
+};
+static void pool_finalize_header(pool_header *header);
+#endif
+
 typedef struct rb_objspace {
     struct {
 	size_t limit;
@@ -365,6 +389,9 @@ typedef struct rb_objspace {
 	size_t allocations;
 #endif
     } malloc_params;
+#ifdef POOL_ALLOC_API
+    pool_layout_t *pool_headers;
+#endif
     struct {
 	size_t increment;
 	struct heaps_slot *ptr;
@@ -430,7 +457,11 @@ typedef struct rb_objspace {
 #define ruby_initial_gc_stress	initial_params.gc_stress
 int *ruby_initial_gc_stress_ptr = &ruby_initial_gc_stress;
 #else
+#  ifdef POOL_ALLOC_API
+static rb_objspace_t rb_objspace = {{GC_MALLOC_LIMIT}, &pool_layout, {HEAP_MIN_SLOTS}};
+#  else
 static rb_objspace_t rb_objspace = {{GC_MALLOC_LIMIT}, {HEAP_MIN_SLOTS}};
+#  endif
 int *ruby_initial_gc_stress_ptr = &rb_objspace.gc_stress;
 #endif
 #define malloc_limit		objspace->malloc_params.limit
@@ -481,6 +512,10 @@ rb_objspace_alloc(void)
     memset(objspace, 0, sizeof(*objspace));
     malloc_limit = initial_malloc_limit;
     ruby_gc_stress = ruby_initial_gc_stress;
+#ifdef POOL_ALLOC_API
+    objspace->pool_headers = (pool_layout_t*) malloc(sizeof(pool_layout));
+    memcpy(objspace->pool_headers, &pool_layout, sizeof(pool_layout));
+#endif
 
     return objspace;
 }
@@ -624,6 +659,13 @@ rb_objspace_free(rb_objspace_t *objspace)
 	heaps = 0;
     }
     free_stack_chunks(&objspace->mark_stack);
+#ifdef POOL_ALLOC_API
+    if (objspace->pool_headers) {
+        pool_finalize_header(&objspace->pool_headers->p6);
+        pool_finalize_header(&objspace->pool_headers->p11);
+        free(objspace->pool_headers);
+    }
+#endif
     free(objspace);
 }
 #endif
@@ -1148,6 +1190,27 @@ rb_gc_stats_enabled()
     return gc_statistics ? Qtrue : Qfalse;
 }
 
+#ifdef POOL_ALLOC_API
+/* POOL ALLOC API */
+#define POOL_ALLOC_PART 2
+#include "pool_alloc.inc.h"
+#undef POOL_ALLOC_PART
+
+void
+ruby_xpool_free(void *ptr)
+{
+    pool_free_entry((void**)ptr);
+}
+
+#define CONCRET_POOL_MALLOC(pnts) \
+void * ruby_xpool_malloc_##pnts##p () { \
+    return pool_alloc_entry(&rb_objspace.pool_headers->p##pnts ); \
+}
+CONCRET_POOL_MALLOC(6)
+CONCRET_POOL_MALLOC(11)
+#undef CONCRET_POOL_MALLOC
+
+#endif
 
 /*
  *  call-seq:
@@ -1663,6 +1726,55 @@ allocate_sorted_heaps(rb_objspace_t *objspace, size_t next_heaps_length)
     heaps_length = next_heaps_length;
 }
 
+static void *
+aligned_malloc(size_t alignment, size_t size)
+{
+    void *res;
+
+#if defined __MINGW32__
+    res = __mingw_aligned_malloc(size, alignment);
+#elif defined _WIN32 && !defined __CYGWIN__
+    res = _aligned_malloc(size, alignment);
+#elif defined(HAVE_POSIX_MEMALIGN)
+    if (posix_memalign(&res, alignment, size) == 0) {
+        return res;
+    }
+    else {
+        return NULL;
+    }
+#elif defined(HAVE_MEMALIGN)
+    res = memalign(alignment, size);
+#else
+    char* aligned;
+    res = malloc(alignment + size + sizeof(void*));
+    aligned = (char*)res + alignment + sizeof(void*);
+    aligned -= ((VALUE)aligned & (alignment - 1));
+    ((void**)aligned)[-1] = res;
+    res = (void*)aligned;
+#endif
+
+#if defined(_DEBUG) || defined(GC_DEBUG)
+    /* alignment must be a power of 2 */
+    assert((alignment - 1) & alignment == 0);
+    assert(alignment % sizeof(void*) == 0);
+#endif
+    return res;
+}
+
+static void
+aligned_free(void *ptr)
+{
+#if defined __MINGW32__
+    __mingw_aligned_free(ptr);
+#elif defined _WIN32 && !defined __CYGWIN__
+    _aligned_free(ptr);
+#elif defined(HAVE_MEMALIGN) || defined(HAVE_POSIX_MEMALIGN)
+    free(ptr);
+#else
+    free(((void**)ptr)[-1]);
+#endif
+}
+
 static void
 assign_heap_slot(rb_objspace_t *objspace)
 {
diff --git a/hash.c b/hash.c
index 6642193..2881cdb 100644
--- a/hash.c
+++ b/hash.c
@@ -44,7 +44,7 @@ rb_any_cmp(VALUE a, VALUE b)
     if (FIXNUM_P(a) && FIXNUM_P(b)) {
 	return a != b;
     }
-    if (TYPE(a) == T_STRING && RBASIC(a)->klass == rb_cString &&
+    if (RB_TYPE_P(a, T_STRING) && RBASIC(a)->klass == rb_cString &&
 	TYPE(b) == T_STRING && RBASIC(b)->klass == rb_cString) {
 	return rb_str_hash_cmp(a, b);
     }
@@ -80,20 +80,14 @@ rb_any_hash(VALUE a)
     VALUE hval;
     st_index_t hnum;
 
-    switch (TYPE(a)) {
-      case T_FIXNUM:
-      case T_SYMBOL:
-      case T_NIL:
-      case T_FALSE:
-      case T_TRUE:
-	hnum = rb_hash_end(rb_hash_start((unsigned int)a));
-	break;
-
-      case T_STRING:
+    if (SPECIAL_CONST_P(a)) {
+	if (a == Qundef) return 0;
+	hnum = rb_hash_end(rb_hash_start((st_index_t)a));
+    }
+    else if (BUILTIN_TYPE(a) == T_STRING) {
 	hnum = rb_str_hash(a);
-	break;
-
-      default:
+    }
+    else {
         hval = rb_hash(a);
 	hnum = FIX2LONG(hval);
     }
@@ -106,10 +100,8 @@ static const struct st_hash_type objhash = {
     rb_any_hash,
 };
 
-static const struct st_hash_type identhash = {
-    st_numcmp,
-    st_numhash,
-};
+extern const struct st_hash_type st_hashtype_num;
+#define identhash st_hashtype_num
 
 typedef int st_foreach_func(st_data_t, st_data_t, st_data_t);
 
@@ -124,7 +116,6 @@ foreach_safe_i(st_data_t key, st_data_t value, struct foreach_safe_arg *arg)
 {
     int status;
 
-    if (key == Qundef) return ST_CONTINUE;
     status = (*arg->func)(key, value, arg->arg);
     if (status == ST_CONTINUE) {
 	return ST_CHECK;
@@ -140,7 +131,7 @@ st_foreach_safe(st_table *table, int (*func)(ANYARGS), st_data_t a)
     arg.tbl = table;
     arg.func = (st_foreach_func *)func;
     arg.arg = a;
-    if (st_foreach(table, foreach_safe_i, (st_data_t)&arg)) {
+    if (st_foreach_check(table, foreach_safe_i, (st_data_t)&arg, 0)) {
 	rb_raise(rb_eRuntimeError, "hash modified during iteration");
     }
 }
@@ -154,21 +145,21 @@ struct hash_foreach_arg {
 };
 
 static int
-hash_foreach_iter(st_data_t key, st_data_t value, struct hash_foreach_arg *arg)
+hash_foreach_iter(st_data_t key, st_data_t value, st_data_t argp)
 {
+    struct hash_foreach_arg *arg = (struct hash_foreach_arg *)argp;
     int status;
     st_table *tbl;
 
     tbl = RHASH(arg->hash)->ntbl;
-    if ((VALUE)key == Qundef) return ST_CONTINUE;
     status = (*arg->func)((VALUE)key, (VALUE)value, arg->arg);
     if (RHASH(arg->hash)->ntbl != tbl) {
 	rb_raise(rb_eRuntimeError, "rehash occurred during iteration");
     }
     switch (status) {
       case ST_DELETE:
-	st_delete_safe(tbl, &key, 0, Qundef);
 	FL_SET(arg->hash, HASH_DELETED);
+	return ST_DELETE;
       case ST_CONTINUE:
 	break;
       case ST_STOP:
@@ -184,7 +175,7 @@ hash_foreach_ensure(VALUE hash)
 
     if (RHASH(hash)->iter_lev == 0) {
 	if (FL_TEST(hash, HASH_DELETED)) {
-	    st_cleanup_safe(RHASH(hash)->ntbl, Qundef);
+	    st_cleanup_safe(RHASH(hash)->ntbl, (st_data_t)Qundef);
 	    FL_UNSET(hash, HASH_DELETED);
 	}
     }
@@ -192,9 +183,10 @@ hash_foreach_ensure(VALUE hash)
 }
 
 static VALUE
-hash_foreach_call(struct hash_foreach_arg *arg)
+hash_foreach_call(VALUE arg)
 {
-    if (st_foreach(RHASH(arg->hash)->ntbl, hash_foreach_iter, (st_data_t)arg)) {
+    VALUE hash = ((struct hash_foreach_arg *)arg)->hash;
+    if (st_foreach_check(RHASH(hash)->ntbl, hash_foreach_iter, (st_data_t)arg, (st_data_t)Qundef)) {
 	rb_raise(rb_eRuntimeError, "hash modified during iteration");
     }
     return Qnil;
@@ -447,7 +439,7 @@ rb_hash_rehash_i(VALUE key, VALUE value, VALUE arg)
 {
     st_table *tbl = (st_table *)arg;
 
-    if (key != Qundef) st_insert(tbl, key, value);
+    st_insert(tbl, (st_data_t)key, (st_data_t)value);
     return ST_CONTINUE;
 }
 
@@ -490,6 +482,20 @@ rb_hash_rehash(VALUE hash)
     return hash;
 }
 
+static VALUE
+hash_default_value(VALUE hash, VALUE key)
+{
+    if (rb_method_basic_definition_p(CLASS_OF(hash), id_default)) {
+	VALUE ifnone = RHASH_IFNONE(hash);
+	if (!FL_TEST(hash, HASH_PROC_DEFAULT)) return ifnone;
+	if (key == Qundef) return Qnil;
+	return rb_funcall(ifnone, id_yield, 2, hash, key);
+    }
+    else {
+	return rb_funcall(hash, id_default, 1, key);
+    }
+}
+
 /*
  *  call-seq:
  *     hsh[key]    ->  value
@@ -510,13 +516,7 @@ rb_hash_aref(VALUE hash, VALUE key)
     st_data_t val;
 
     if (!RHASH(hash)->ntbl || !st_lookup(RHASH(hash)->ntbl, key, &val)) {
-	if (!FL_TEST(hash, HASH_PROC_DEFAULT) &&
-	    rb_method_basic_definition_p(CLASS_OF(hash), id_default)) {
-	    return RHASH_IFNONE(hash);
-	}
-	else {
-	    return rb_funcall(hash, id_default, 1, key);
-	}
+	return hash_default_value(hash, key);
     }
     return (VALUE)val;
 }
@@ -659,7 +659,7 @@ rb_hash_default(int argc, VALUE *argv, VALUE hash)
 static VALUE
 rb_hash_set_default(VALUE hash, VALUE ifnone)
 {
-    rb_hash_modify(hash);
+    rb_hash_modify_check(hash);
     RHASH_IFNONE(hash) = ifnone;
     FL_UNSET(hash, HASH_PROC_DEFAULT);
     return ifnone;
@@ -707,7 +707,7 @@ rb_hash_set_default_proc(VALUE hash, VALUE proc)
 {
     VALUE b;
 
-    rb_hash_modify(hash);
+    rb_hash_modify_check(hash);
     b = rb_check_convert_type(proc, T_DATA, "Proc", "to_proc");
     if (NIL_P(b) || !rb_obj_is_proc(b)) {
 	rb_raise(rb_eTypeError,
@@ -776,7 +776,7 @@ rb_hash_delete_key(VALUE hash, VALUE key)
     if (!RHASH(hash)->ntbl)
         return Qundef;
     if (RHASH(hash)->iter_lev > 0) {
-	if (st_delete_safe(RHASH(hash)->ntbl, &ktmp, &val, Qundef)) {
+	if (st_delete_safe(RHASH(hash)->ntbl, &ktmp, &val, (st_data_t)Qundef)) {
 	    FL_SET(hash, HASH_DELETED);
 	    return (VALUE)val;
 	}
@@ -809,7 +809,7 @@ rb_hash_delete(VALUE hash, VALUE key)
 {
     VALUE val;
 
-    rb_hash_modify(hash);
+    rb_hash_modify_check(hash);
     val = rb_hash_delete_key(hash, key);
     if (val != Qundef) return val;
     if (rb_block_given_p()) {
@@ -828,7 +828,6 @@ shift_i_safe(VALUE key, VALUE value, VALUE arg)
 {
     struct shift_var *var = (struct shift_var *)arg;
 
-    if (key == Qundef) return ST_CONTINUE;
     var->key = key;
     var->val = value;
     return ST_STOP;
@@ -852,33 +851,29 @@ rb_hash_shift(VALUE hash)
 {
     struct shift_var var;
 
-    rb_hash_modify(hash);
-    var.key = Qundef;
-    if (RHASH(hash)->iter_lev == 0) {
-	if (st_shift(RHASH(hash)->ntbl, &var.key, &var.val)) {
-	    return rb_assoc_new(var.key, var.val);
-	}
-    }
-    else {
-	rb_hash_foreach(hash, shift_i_safe, (VALUE)&var);
-
-	if (var.key != Qundef) {
-	    rb_hash_delete_key(hash, var.key);
-	    return rb_assoc_new(var.key, var.val);
+    rb_hash_modify_check(hash);
+    if (RHASH(hash)->ntbl) {
+	var.key = Qundef;
+        if (RHASH(hash)->iter_lev == 0) {
+            if (st_shift(RHASH(hash)->ntbl, &var.key, &var.val)) {
+                return rb_assoc_new(var.key, var.val);
+            }
+        }
+        else {
+            rb_hash_foreach(hash, shift_i_safe, (VALUE)&var);
+
+            if (var.key != Qundef) {
+                rb_hash_delete_key(hash, var.key);
+                return rb_assoc_new(var.key, var.val);
+            }
 	}
     }
-    if (FL_TEST(hash, HASH_PROC_DEFAULT)) {
-	return rb_funcall(RHASH_IFNONE(hash), id_yield, 2, hash, Qnil);
-    }
-    else {
-	return RHASH_IFNONE(hash);
-    }
+    return hash_default_value(hash, Qnil);
 }
 
 static int
 delete_if_i(VALUE key, VALUE value, VALUE hash)
 {
-    if (key == Qundef) return ST_CONTINUE;
     if (RTEST(rb_yield_values(2, key, value))) {
 	rb_hash_delete_key(hash, key);
     }
@@ -904,8 +899,9 @@ VALUE
 rb_hash_delete_if(VALUE hash)
 {
     RETURN_ENUMERATOR(hash, 0, 0);
-    rb_hash_modify(hash);
-    rb_hash_foreach(hash, delete_if_i, hash);
+    rb_hash_modify_check(hash);
+    if (RHASH(hash)->ntbl)
+	rb_hash_foreach(hash, delete_if_i, hash);
     return hash;
 }
 
@@ -976,7 +972,6 @@ rb_hash_values_at(int argc, VALUE *argv, VALUE hash)
 static int
 select_i(VALUE key, VALUE value, VALUE result)
 {
-    if (key == Qundef) return ST_CONTINUE;
     if (RTEST(rb_yield_values(2, key, value)))
 	rb_hash_aset(result, key, value);
     return ST_CONTINUE;
@@ -1010,7 +1005,6 @@ rb_hash_select(VALUE hash)
 static int
 keep_if_i(VALUE key, VALUE value, VALUE hash)
 {
-    if (key == Qundef) return ST_CONTINUE;
     if (!RTEST(rb_yield_values(2, key, value))) {
 	return ST_DELETE;
     }
@@ -1032,7 +1026,7 @@ rb_hash_select_bang(VALUE hash)
     st_index_t n;
 
     RETURN_ENUMERATOR(hash, 0, 0);
-    rb_hash_modify(hash);
+    rb_hash_modify_check(hash);
     if (!RHASH(hash)->ntbl)
         return Qnil;
     n = RHASH(hash)->ntbl->num_entries;
@@ -1057,8 +1051,9 @@ VALUE
 rb_hash_keep_if(VALUE hash)
 {
     RETURN_ENUMERATOR(hash, 0, 0);
-    rb_hash_modify(hash);
-    rb_hash_foreach(hash, keep_if_i, hash);
+    rb_hash_modify_check(hash);
+    if (RHASH(hash)->ntbl)
+	rb_hash_foreach(hash, keep_if_i, hash);
     return hash;
 }
 
@@ -1136,9 +1131,7 @@ rb_hash_aset(VALUE hash, VALUE key, VALUE val)
 static int
 replace_i(VALUE key, VALUE val, VALUE hash)
 {
-    if (key != Qundef) {
-	rb_hash_aset(hash, key, val);
-    }
+    rb_hash_aset(hash, key, val);
 
     return ST_CONTINUE;
 }
@@ -1219,7 +1212,6 @@ rb_hash_empty_p(VALUE hash)
 static int
 each_value_i(VALUE key, VALUE value)
 {
-    if (key == Qundef) return ST_CONTINUE;
     rb_yield(value);
     return ST_CONTINUE;
 }
@@ -1254,7 +1246,6 @@ rb_hash_each_value(VALUE hash)
 static int
 each_key_i(VALUE key, VALUE value)
 {
-    if (key == Qundef) return ST_CONTINUE;
     rb_yield(key);
     return ST_CONTINUE;
 }
@@ -1288,7 +1279,6 @@ rb_hash_each_key(VALUE hash)
 static int
 each_pair_i(VALUE key, VALUE value)
 {
-    if (key == Qundef) return ST_CONTINUE;
     rb_yield(rb_assoc_new(key, value));
     return ST_CONTINUE;
 }
@@ -1326,7 +1316,6 @@ rb_hash_each_pair(VALUE hash)
 static int
 to_a_i(VALUE key, VALUE value, VALUE ary)
 {
-    if (key == Qundef) return ST_CONTINUE;
     rb_ary_push(ary, rb_assoc_new(key, value));
     return ST_CONTINUE;
 }
@@ -1359,7 +1348,6 @@ inspect_i(VALUE key, VALUE value, VALUE str)
 {
     VALUE str2;
 
-    if (key == Qundef) return ST_CONTINUE;
     str2 = rb_inspect(key);
     if (RSTRING_LEN(str) > 1) {
 	rb_str_cat2(str, ", ");
@@ -1426,7 +1414,6 @@ rb_hash_to_hash(VALUE hash)
 static int
 keys_i(VALUE key, VALUE value, VALUE ary)
 {
-    if (key == Qundef) return ST_CONTINUE;
     rb_ary_push(ary, key);
     return ST_CONTINUE;
 }
@@ -1457,7 +1444,6 @@ rb_hash_keys(VALUE hash)
 static int
 values_i(VALUE key, VALUE value, VALUE ary)
 {
-    if (key == Qundef) return ST_CONTINUE;
     rb_ary_push(ary, value);
     return ST_CONTINUE;
 }
@@ -1516,7 +1502,6 @@ rb_hash_search_value(VALUE key, VALUE value, VALUE arg)
 {
     VALUE *data = (VALUE *)arg;
 
-    if (key == Qundef) return ST_CONTINUE;
     if (rb_equal(value, data[1])) {
 	data[0] = Qtrue;
 	return ST_STOP;
@@ -1560,7 +1545,6 @@ eql_i(VALUE key, VALUE val1, VALUE arg)
     struct equal_data *data = (struct equal_data *)arg;
     st_data_t val2;
 
-    if (key == Qundef) return ST_CONTINUE;
     if (!st_lookup(data->tbl, key, &val2)) {
 	data->result = Qfalse;
 	return ST_STOP;
@@ -1591,7 +1575,7 @@ hash_equal(VALUE hash1, VALUE hash2, int eql)
     struct equal_data data;
 
     if (hash1 == hash2) return Qtrue;
-    if (TYPE(hash2) != T_HASH) {
+    if (!RB_TYPE_P(hash2, T_HASH)) {
 	if (!rb_respond_to(hash2, rb_intern("to_hash"))) {
 	    return Qfalse;
 	}
@@ -1662,7 +1646,6 @@ hash_i(VALUE key, VALUE val, VALUE arg)
     st_index_t *hval = (st_index_t *)arg;
     st_index_t hdata[2];
 
-    if (key == Qundef) return ST_CONTINUE;
     hdata[0] = rb_hash(key);
     hdata[1] = rb_hash(val);
     *hval ^= st_hash(hdata, sizeof(hdata), 0);
@@ -1703,7 +1686,6 @@ rb_hash_hash(VALUE hash)
 static int
 rb_hash_invert_i(VALUE key, VALUE value, VALUE hash)
 {
-    if (key == Qundef) return ST_CONTINUE;
     rb_hash_aset(hash, value, key);
     return ST_CONTINUE;
 }
@@ -1732,7 +1714,6 @@ rb_hash_invert(VALUE hash)
 static int
 rb_hash_update_i(VALUE key, VALUE value, VALUE hash)
 {
-    if (key == Qundef) return ST_CONTINUE;
     hash_update(hash, key);
     st_insert(RHASH(hash)->ntbl, key, value);
     return ST_CONTINUE;
@@ -1741,7 +1722,6 @@ rb_hash_update_i(VALUE key, VALUE value, VALUE hash)
 static int
 rb_hash_update_block_i(VALUE key, VALUE value, VALUE hash)
 {
-    if (key == Qundef) return ST_CONTINUE;
     if (rb_hash_has_key(hash, key)) {
 	value = rb_yield_values(3, key, rb_hash_aref(hash, key), value);
     }
@@ -1798,7 +1778,6 @@ rb_hash_update_func_i(VALUE key, VALUE value, VALUE arg0)
     struct update_arg *arg = (struct update_arg *)arg0;
     VALUE hash = arg->hash;
 
-    if (key == Qundef) return ST_CONTINUE;
     if (rb_hash_has_key(hash, key)) {
 	value = (*arg->func)(key, rb_hash_aref(hash, key), value);
     }
@@ -1855,7 +1834,6 @@ assoc_i(VALUE key, VALUE val, VALUE arg)
 {
     VALUE *args = (VALUE *)arg;
 
-    if (key == Qundef) return ST_CONTINUE;
     if (RTEST(rb_equal(args[0], key))) {
 	args[1] = rb_assoc_new(key, val);
 	return ST_STOP;
@@ -1893,7 +1871,6 @@ rassoc_i(VALUE key, VALUE val, VALUE arg)
 {
     VALUE *args = (VALUE *)arg;
 
-    if (key == Qundef) return ST_CONTINUE;
     if (RTEST(rb_equal(args[0], val))) {
 	args[1] = rb_assoc_new(key, val);
 	return ST_STOP;
@@ -2190,7 +2167,7 @@ rb_env_path_tainted(void)
 }
 
 #if defined(_WIN32) || (defined(HAVE_SETENV) && defined(HAVE_UNSETENV))
-#elif defined __sun__
+#elif defined __sun
 static int
 in_origenv(const char *str)
 {
@@ -2278,7 +2255,7 @@ ruby_setenv(const char *name, const char *value)
 	    rb_sys_fail("unsetenv");
 #endif
     }
-#elif defined __sun__
+#elif defined __sun
     size_t len;
     char **env_ptr, *str;
     if (strchr(name, '=')) {
@@ -3076,11 +3053,9 @@ env_invert(void)
 static int
 env_replace_i(VALUE key, VALUE val, VALUE keys)
 {
-    if (key != Qundef) {
-	env_aset(Qnil, key, val);
-	if (rb_ary_includes(keys, key)) {
-	    rb_ary_delete(keys, key);
-	}
+    env_aset(Qnil, key, val);
+    if (rb_ary_includes(keys, key)) {
+	rb_ary_delete(keys, key);
     }
     return ST_CONTINUE;
 }
@@ -3112,12 +3087,10 @@ env_replace(VALUE env, VALUE hash)
 static int
 env_update_i(VALUE key, VALUE val)
 {
-    if (key != Qundef) {
-	if (rb_block_given_p()) {
-	    val = rb_yield_values(3, key, rb_f_getenv(Qnil, key), val);
-	}
-	env_aset(Qnil, key, val);
+    if (rb_block_given_p()) {
+	val = rb_yield_values(3, key, rb_f_getenv(Qnil, key), val);
     }
+    env_aset(Qnil, key, val);
     return ST_CONTINUE;
 }
 
@@ -3142,15 +3115,116 @@ env_update(VALUE env, VALUE hash)
 }
 
 /*
- *  A <code>Hash</code> is a collection of key-value pairs. It is
- *  similar to an <code>Array</code>, except that indexing is done via
- *  arbitrary keys of any object type, not an integer index. Hashes enumerate
- *  their values in the order that the corresponding keys were inserted.
+ *  A Hash is a dictionary-like collection of unique keys and their values.
+ *  Also called associative arrays, they are similar to Arrays, but where an
+ *  Array uses integers as its index, a Hash allows you to use any object
+ *  type.
+ *
+ *  Hashes enumerate their values in the order that the corresponding keys
+ *  were inserted.
+ *
+ *  A Hash can be easily created by using its implicit form:
+ *
+ *    grades = { "Jane Doe" => 10, "Jim Doe" => 6 }
+ *
+ *  Hashes allow an alternate syntax form when your keys are always symbols.
+ *  Instead of
+ *
+ *    options = { :font_size => 10, :font_family => "Arial" }
+ *
+ *  You could write it as:
+ *
+ *    options = { font_size: 10, font_family: "Arial" }
+ *
+ *  Each named key is a symbol you can access in hash:
+ *
+ *    options[:font_size]  # => 10
+ *
+ *  A Hash can also be created through its ::new method:
+ *
+ *    grades = Hash.new
+ *    grades["Dorothy Doe"] = 9
  *
  *  Hashes have a <em>default value</em> that is returned when accessing
- *  keys that do not exist in the hash. By default, that value is
- *  <code>nil</code>.
+ *  keys that do not exist in the hash. If no default is set +nil+ is used.
+ *  You can set the default value by sending it as an argument to Hash.new:
+ *
+ *    grades = Hash.new(0)
+ *
+ *  Or by using the #default= method:
+ *
+ *    grades = {"Timmy Doe" => 8}
+ *    grades.default = 0
+ *
+ *  Accessing a value in a Hash requires using its key:
+ *
+ *    puts grades["Jane Doe"] # => 10
+ *
+ *  === Common Uses
+ *
+ *  Hashes are an easy way to represent data structures, such as
+ *
+ *    books         = {}
+ *    books[:matz]  = "The Ruby Language"
+ *    books[:black] = "The Well-Grounded Rubyist"
+ *
+ *  Hashes are also commonly used as a way to have named parameters in
+ *  functions. Note that no brackets are used below. If a hash is the last
+ *  argument on a method call, no braces are needed, thus creating a really
+ *  clean interface:
+ *
+ *    Person.create(name: "John Doe", age: 27)
+ *
+ *    def self.create(params)
+ *      @name = params[:name]
+ *      @age  = params[:age]
+ *    end
+ *
+ *  === Hash Keys
+ *
+ *  Two objects refer to the same hash key when their <code>hash</code> value
+ *  is identical and the two objects are <code>eql?</code> to each other.
+ *
+ *  A user-defined class may be used as a hash key if the <code>hash</code>
+ *  and <code>eql?</code> methods are overridden to provide meaningful
+ *  behavior.  By default, separate instances refer to separate hash keys.
+ *
+ *  A typical implementation of <code>hash</code> is based on the
+ *  object's data while <code>eql?</code> is usually aliased to the overridden
+ *  <code>==</code> method:
+ *
+ *    class Book
+ *      attr_reader :author, :title
+ *
+ *      def initialize(author, title)
+ *        @author = author
+ *        @title = title
+ *      end
+ *
+ *      def ==(other)
+ *        self.class === other and
+ *          other.author == @author and
+ *          other.title == @title
+ *      end
+ *
+ *      alias eql? ==
+ *
+ *      def hash
+ *        @author.hash ^ @title.hash # XOR
+ *      end
+ *    end
+ *
+ *    book1 = Book.new 'matz', 'Ruby in a Nutshell'
+ *    book2 = Book.new 'matz', 'Ruby in a Nutshell'
+ *
+ *    reviews = {}
+ *
+ *    reviews[book1] = 'Great reference!'
+ *    reviews[book2] = 'Nice and compact!'
+ *
+ *    reviews.length #=> 1
  *
+ *  See also Object#hash and Object#eql?
  */
 
 void
diff --git a/include/ruby/st.h b/include/ruby/st.h
index 68cc511..94646b6 100644
--- a/include/ruby/st.h
+++ b/include/ruby/st.h
@@ -36,7 +36,7 @@ typedef unsigned long st_data_t;
 #elif SIZEOF_LONG_LONG == SIZEOF_VOIDP
 typedef unsigned LONG_LONG st_data_t;
 #else
-# error ---->> st.c requires sizeof(void*) == sizeof(long) to be compiled. <<----
+# error ---->> st.c requires sizeof(void*) == sizeof(long) or sizeof(LONG_LONG) to be compiled. <<----
 #endif
 #define ST_DATA_T_DEFINED
 
@@ -74,6 +74,11 @@ struct st_hash_type {
 
 #define ST_INDEX_BITS (sizeof(st_index_t) * CHAR_BIT)
 
+typedef struct st_packed_entry {
+    st_index_t hash;
+    st_data_t key, val;
+} st_packed_entry;
+
 struct st_table {
     const struct st_hash_type *type;
     st_index_t num_bins;
@@ -91,8 +96,17 @@ struct st_table {
     __extension__
 #endif
     st_index_t num_entries : ST_INDEX_BITS - 1;
-    struct st_table_entry **bins;
-    struct st_table_entry *head, *tail;
+    union {
+	struct {
+	    struct st_table_entry **bins;
+	    struct st_table_entry *head, *tail;
+	} big;
+	struct {
+	    struct st_packed_entry *entries;
+	    st_index_t real_entries;
+	} packed;
+	st_packed_entry upacked;
+    } as;
 };
 
 #define st_is_member(table,key) st_lookup((table),(key),(st_data_t *)0)
@@ -115,6 +129,7 @@ int st_insert2(st_table *, st_data_t, st_data_t, st_data_t (*)(st_data_t));
 int st_lookup(st_table *, st_data_t, st_data_t *);
 int st_get_key(st_table *, st_data_t, st_data_t *);
 int st_foreach(st_table *, int (*)(ANYARGS), st_data_t);
+int st_foreach_check(st_table *, int (*)(ANYARGS), st_data_t, st_data_t);
 int st_reverse_foreach(st_table *, int (*)(ANYARGS), st_data_t);
 void st_add_direct(st_table *, st_data_t, st_data_t);
 void st_free_table(st_table *);
diff --git a/pool_alloc.h b/pool_alloc.h
new file mode 100644
index 0000000..957708e
--- /dev/null
+++ b/pool_alloc.h
@@ -0,0 +1,11 @@
+#ifndef POOL_ALLOC_H
+#define POOL_ALLOC_H
+
+#define POOL_ALLOC_API
+#ifdef POOL_ALLOC_API
+void  ruby_xpool_free(void *ptr);
+void *ruby_xpool_malloc_6p();
+void *ruby_xpool_malloc_11p();
+#endif
+
+#endif
diff --git a/pool_alloc.inc.h b/pool_alloc.inc.h
new file mode 100644
index 0000000..a7879ab
--- /dev/null
+++ b/pool_alloc.inc.h
@@ -0,0 +1,156 @@
+/*
+ * this is generic pool allocator
+ * you should define following macroses:
+ * ITEM_NAME - unique identifier, which allows to hold functions in a namespace
+ * ITEM_TYPEDEF(name) - passed to typedef to localize item type
+ * free_entry - desired name of function for free entry
+ * alloc_entry - defired name of function for allocate entry
+ */
+
+#if POOL_ALLOC_PART == 1
+#ifdef HEAP_ALIGN_LOG
+#define DEFAULT_POOL_SIZE (1 << HEAP_ALIGN_LOG)
+#else
+#define DEFAULT_POOL_SIZE (sizeof(void*) * 2048)
+#endif
+typedef unsigned int pool_holder_counter;
+
+typedef struct pool_entry_list pool_entry_list;
+typedef struct pool_holder pool_holder;
+
+typedef struct pool_header {
+    pool_holder         *first;
+    pool_holder         *_black_magick;
+    pool_holder_counter  size; // size of entry in sizeof(void*) items
+    pool_holder_counter  total; // size of entry in sizeof(void*) items
+} pool_header;
+
+struct pool_holder {
+    pool_holder_counter free, total;
+    pool_header  *header;
+    void               *freep;
+    pool_holder        *fore, *back;
+    void *data[1];
+};
+#define POOL_DATA_SIZE(pool_size) (((pool_size) - sizeof(void*) * 6 - offsetof(pool_holder, data)) / sizeof(void*))
+#define POOL_ENTRY_SIZE(item_type) ((sizeof(item_type) - 1) / sizeof(void*) + 1)
+#define POOL_HOLDER_COUNT(pool_size, item_type) (POOL_DATA_SIZE(pool_size)/POOL_ENTRY_SIZE(item_type))
+#define INIT_POOL(item_type) {NULL, NULL, POOL_ENTRY_SIZE(item_type), POOL_HOLDER_COUNT(DEFAULT_POOL_SIZE, item_type)}
+
+#elif POOL_ALLOC_PART == 2
+static pool_holder *
+pool_holder_alloc(pool_header *header)
+{
+    pool_holder *holder;
+    pool_holder_counter i, size, count;
+    register void **ptr;
+
+    size_t sz = offsetof(pool_holder, data) +
+	    header->size * header->total * sizeof(void*);
+#define objspace (&rb_objspace)
+    vm_malloc_prepare(objspace, DEFAULT_POOL_SIZE);
+    if (header->first != NULL) return header->first;
+    TRY_WITH_GC(holder = (pool_holder*) aligned_malloc(DEFAULT_POOL_SIZE, sz));
+    malloc_increase += DEFAULT_POOL_SIZE;
+#if CALC_EXACT_MALLOC_SIZE
+    objspace->malloc_params.allocated_size += DEFAULT_POOL_SIZE;
+    objspace->malloc_params.allocations++;
+#endif
+#undef objspace
+
+    size = header->size;
+    count = header->total;
+    holder->free = count;
+    holder->total = count;
+    holder->header = header;
+    holder->fore = NULL;
+    holder->back = NULL;
+    holder->freep = &holder->data;
+    ptr = holder->data;
+    for(i = count - 1; i; i-- ) {
+	ptr = *ptr = ptr + size;
+    }
+    *ptr = NULL;
+    header->first = holder;
+    return holder;
+}
+
+static inline void
+pool_holder_unchaing(pool_header *header, pool_holder *holder)
+{
+    register pool_holder *fore = holder->fore, *back = holder->back;
+    holder->fore = NULL;
+    holder->back = NULL;
+    if (fore != NULL)  fore->back     = back;
+    else               header->_black_magick = back;
+    if (back != NULL)  back->fore     = fore;
+    else               header->first = fore;
+}
+
+static inline pool_holder *
+entry_holder(void **entry)
+{
+    return (pool_holder*)(((uintptr_t)entry) & ~(DEFAULT_POOL_SIZE - 1));
+}
+
+static inline void
+pool_free_entry(void **entry)
+{
+    pool_holder *holder = entry_holder(entry);
+    pool_header *header = holder->header;
+
+    if (holder->free++ == 0) {
+	register pool_holder *first = header->first;
+	if (first == NULL) {
+	    header->first = holder;
+	} else {
+	    holder->back = first;
+	    holder->fore = first->fore;
+	    first->fore = holder;
+	    if (holder->fore)
+		holder->fore->back = holder;
+	    else
+		header->_black_magick = holder;
+	}
+    } else if (holder->free == holder->total && header->first != holder ) {
+	pool_holder_unchaing(header, holder);
+	aligned_free(holder);
+#if CALC_EXACT_MALLOC_SIZE
+	rb_objspace.malloc_params.allocated_size -= DEFAULT_POOL_SIZE;
+	rb_objspace.malloc_params.allocations--;
+#endif
+	return;
+    }
+
+    *entry = holder->freep;
+    holder->freep = entry;
+}
+
+static inline void*
+pool_alloc_entry(pool_header *header)
+{
+    pool_holder *holder = header->first;
+    void **result;
+    if (holder == NULL) {
+	holder = pool_holder_alloc(header);
+    }
+
+    result = holder->freep;
+    holder->freep = *result;
+
+    if (--holder->free == 0) {
+	pool_holder_unchaing(header, holder);
+    }
+
+    return result;
+}
+
+static void
+pool_finalize_header(pool_header *header)
+{
+    if (header->first) {
+        aligned_free(header->first);
+        header->first = NULL;
+    }
+}
+#endif
diff --git a/st.c b/st.c
index 8c238c3..5a682cd 100644
--- a/st.c
+++ b/st.c
@@ -7,6 +7,7 @@
 #include "st.h"
 #else
 #include "ruby/ruby.h"
+#include "pool_alloc.h"
 #endif
 
 #include <stdio.h>
@@ -25,8 +26,17 @@ struct st_table_entry {
     st_table_entry *fore, *back;
 };
 
-#define ST_DEFAULT_MAX_DENSITY 5
+#define STATIC_ASSERT(name, expr) typedef int static_assert_##name##_check[(expr) ? 1 : -1];
+
+#define ST_DEFAULT_MAX_DENSITY 2
 #define ST_DEFAULT_INIT_TABLE_SIZE 11
+#define ST_DEFAULT_SECOND_TABLE_SIZE 19
+#define ST_DEFAULT_PACKED_TABLE_SIZE 19
+#define PACKED_UNIT (int)(sizeof(st_packed_entry) / sizeof(st_table_entry*))
+#define MAX_PACKED_HASH (int)(ST_DEFAULT_PACKED_TABLE_SIZE * sizeof(st_table_entry*) / sizeof(st_packed_entry))
+
+STATIC_ASSERT(st_packed_entry, sizeof(st_packed_entry) == sizeof(st_table_entry*[PACKED_UNIT]))
+STATIC_ASSERT(st_packed_bins, sizeof(st_packed_entry[MAX_PACKED_HASH]) <= sizeof(st_table_entry*[ST_DEFAULT_PACKED_TABLE_SIZE]))
 
     /*
      * DEFAULT_MAX_DENSITY is the default for the largest we allow the
@@ -38,7 +48,8 @@ struct st_table_entry {
      *
      */
 
-static const struct st_hash_type type_numhash = {
+#define type_numhash st_hashtype_num
+const struct st_hash_type st_hashtype_num = {
     st_numcmp,
     st_numhash,
 };
@@ -61,20 +72,128 @@ static void rehash(st_table *);
 #ifdef RUBY
 #define malloc xmalloc
 #define calloc xcalloc
+#define realloc xrealloc
 #define free(x) xfree(x)
 #endif
 
 #define numberof(array) (int)(sizeof(array) / sizeof((array)[0]))
 
-#define alloc(type) (type*)malloc((size_t)sizeof(type))
-#define Calloc(n,s) (char*)calloc((n),(s))
-
 #define EQUAL(table,x,y) ((x)==(y) || (*(table)->type->compare)((x),(y)) == 0)
 
-/* remove cast to unsigned int in the future */
-#define do_hash(key,table) (unsigned int)(st_index_t)(*(table)->type->hash)((key))
+#define do_hash(key,table) (st_index_t)(*(table)->type->hash)((key))
 #define do_hash_bin(key,table) (do_hash((key), (table))%(table)->num_bins)
 
+/* preparation for possible allocation improvements */
+#ifdef POOL_ALLOC_API
+#define st_alloc_entry() (st_table_entry *)ruby_xpool_malloc_6p()
+#define st_free_entry(entry) ruby_xpool_free(entry)
+#define st_alloc_table() (st_table *)ruby_xpool_malloc_6p()
+#define st_dealloc_table(table) ruby_xpool_free(table)
+static inline st_table_entry **
+st_alloc_bins(st_index_t size)
+{
+    st_table_entry **result;
+    if (size == 11) {
+        result = (st_table_entry **) ruby_xpool_malloc_11p();
+        memset(result, 0, 11 * sizeof(st_table_entry *));
+    }
+    else
+        result = (st_table_entry **) ruby_xcalloc(size, sizeof(st_table_entry*));
+    return result;
+}
+static inline void
+st_free_bins(st_table_entry **bins, st_index_t size)
+{
+    if (size == 11)
+	ruby_xpool_free(bins);
+    else
+	ruby_xfree(bins);
+}
+static inline st_table_entry**
+st_realloc_bins(st_table_entry **bins, st_index_t newsize, st_index_t oldsize)
+{
+    st_table_entry **new_bins = st_alloc_bins(newsize);
+    st_free_bins(bins, oldsize);
+    return new_bins;
+}
+#else
+#define st_alloc_entry() (st_table_entry *)malloc(sizeof(st_table_entry))
+#define st_free_entry(entry) free(entry)
+#define st_alloc_table() (st_table *)malloc(sizeof(st_table))
+#define st_dealloc_table(table) free(table)
+#define st_alloc_bins(size) (st_table_entry **)calloc(size, sizeof(st_table_entry *))
+#define st_free_bins(bins, size) free(bins)
+static inline st_table_entry**
+st_realloc_bins(st_table_entry **bins, st_index_t newsize, st_index_t oldsize)
+{
+    bins = (st_table_entry **)realloc(bins, newsize * sizeof(st_table_entry *));
+    MEMZERO(bins, st_table_entry*, newsize);
+    return bins;
+}
+#endif
+
+/* Shortage */
+#define bins as.big.bins
+#define head as.big.head
+#define tail as.big.tail
+#define real_entries as.packed.real_entries
+
+/* preparation for possible packing improvements */
+#define PACKED_BINS(table) ((table)->as.packed.entries)
+#define PACKED_ENT(table, i) PACKED_BINS(table)[i]
+#define PKEY(table, i) PACKED_ENT((table), (i)).key
+#define PVAL(table, i) PACKED_ENT((table), (i)).val
+#define PHASH(table, i) PACKED_ENT((table), (i)).hash
+#define PKEY_SET(table, i, v) (PKEY((table), (i)) = (v))
+#define PVAL_SET(table, i, v) (PVAL((table), (i)) = (v))
+#define PHASH_SET(table, i, v) (PHASH((table), (i)) = (v))
+
+/* this function depends much on packed layout, so that it placed here */
+static inline void
+remove_packed_entry(st_table *table, st_index_t i)
+{
+    table->real_entries--;
+    table->num_entries--;
+    if (i < table->real_entries) {
+	MEMMOVE(&PACKED_ENT(table, i), &PACKED_ENT(table, i+1),
+		st_packed_entry, table->real_entries - i);
+    }
+}
+
+static inline void
+remove_safe_packed_entry(st_table *table, st_index_t i, st_data_t never)
+{
+    table->num_entries--;
+    PKEY_SET(table, i, never);
+    PVAL_SET(table, i, never);
+    PHASH_SET(table, i, 0);
+}
+
+/* ultrapacking */
+#define real_upacked num_bins
+#define MAX_UPACKED_HASH 1
+#define ULTRAPACKED(table) ((table)->real_upacked <= 1)
+#define UPACKED_ENT(table) ((table)->as.upacked)
+#define UPKEY(table) UPACKED_ENT(table).key
+#define UPVAL(table) UPACKED_ENT(table).val
+#define UPHASH(table) UPACKED_ENT(table).hash
+#define UPKEY_SET(table, v) (UPACKED_ENT(table).key = (v))
+#define UPVAL_SET(table, v) (UPACKED_ENT(table).val = (v))
+#define UPHASH_SET(table, v) (UPACKED_ENT(table).hash = (v))
+static inline void
+remove_upacked_entry(st_table *table)
+{
+    table->real_upacked = table->num_entries = 0;
+}
+
+static inline void
+remove_safe_upacked_entry(st_table *table, st_data_t never)
+{
+    table->num_entries = 0;
+    UPKEY_SET(table, never);
+    UPVAL_SET(table, never);
+    UPHASH_SET(table, 0);
+}
 /*
  * MINSIZE is the minimum size of a dictionary.
  */
@@ -85,8 +204,8 @@ static void rehash(st_table *);
 Table of prime numbers 2^n+a, 2<=n<=30.
 */
 static const unsigned int primes[] = {
-	8 + 3,
-	16 + 3,
+	ST_DEFAULT_INIT_TABLE_SIZE,
+	ST_DEFAULT_SECOND_TABLE_SIZE,
 	32 + 5,
 	64 + 3,
 	128 + 3,
@@ -161,8 +280,6 @@ stat_col(void)
 }
 #endif
 
-#define MAX_PACKED_NUMHASH (ST_DEFAULT_INIT_TABLE_SIZE/2)
-
 st_table*
 st_init_table_with_size(const struct st_hash_type *type, st_index_t size)
 {
@@ -181,14 +298,19 @@ st_init_table_with_size(const struct st_hash_type *type, st_index_t size)
     }
 #endif
 
-    size = new_size(size);	/* round up to prime number */
 
-    tbl = alloc(st_table);
+    tbl = st_alloc_table();
     tbl->type = type;
     tbl->num_entries = 0;
-    tbl->entries_packed = type == &type_numhash && size/2 <= MAX_PACKED_NUMHASH;
+    tbl->entries_packed = size <= MAX_PACKED_HASH;
+    if (tbl->entries_packed) {
+	size = size <= MAX_UPACKED_HASH ? 0 : ST_DEFAULT_PACKED_TABLE_SIZE;
+    }
+    else {
+	size = new_size(size);	/* round up to prime number */
+    }
     tbl->num_bins = size;
-    tbl->bins = (st_table_entry **)Calloc(size, sizeof(st_table_entry*));
+    tbl->bins = size ? st_alloc_bins(size) : 0;
     tbl->head = 0;
     tbl->tail = 0;
 
@@ -243,17 +365,23 @@ st_clear(st_table *table)
     register st_table_entry *ptr, *next;
     st_index_t i;
 
+    if (ULTRAPACKED(table)) {
+	remove_upacked_entry(table);
+	return;
+    }
+
     if (table->entries_packed) {
         table->num_entries = 0;
+        table->real_entries = 0;
         return;
     }
 
-    for(i = 0; i < table->num_bins; i++) {
+    for (i = 0; i < table->num_bins; i++) {
 	ptr = table->bins[i];
 	table->bins[i] = 0;
 	while (ptr != 0) {
 	    next = ptr->next;
-	    free(ptr);
+	    st_free_entry(ptr);
 	    ptr = next;
 	}
     }
@@ -266,14 +394,19 @@ void
 st_free_table(st_table *table)
 {
     st_clear(table);
-    free(table->bins);
-    free(table);
+    if (!ULTRAPACKED(table)) {
+	st_free_bins(table->bins, table->num_bins);
+    }
+    st_dealloc_table(table);
 }
 
 size_t
 st_memsize(const st_table *table)
 {
-    if (table->entries_packed) {
+    if (ULTRAPACKED(table)) {
+	return sizeof(st_table);
+    }
+    else if (table->entries_packed) {
 	return table->num_bins * sizeof (void *) + sizeof(st_table);
     }
     else {
@@ -306,46 +439,77 @@ count_collision(const struct st_hash_type *type)
 #define FOUND_ENTRY
 #endif
 
-#define FIND_ENTRY(table, ptr, hash_val, bin_pos) do {\
-    (bin_pos) = (hash_val)%(table)->num_bins;\
-    (ptr) = (table)->bins[(bin_pos)];\
-    FOUND_ENTRY;\
-    if (PTR_NOT_EQUAL((table), (ptr), (hash_val), key)) {\
-	COLLISION;\
-	while (PTR_NOT_EQUAL((table), (ptr)->next, (hash_val), key)) {\
-	    (ptr) = (ptr)->next;\
-	}\
-	(ptr) = (ptr)->next;\
-    }\
-} while (0)
+#define FIND_ENTRY(table, ptr, hash_val, bin_pos) \
+    ((ptr) = find_entry((table), key, (hash_val), ((bin_pos) = (hash_val)%(table)->num_bins)))
+
+static st_table_entry *
+find_entry(st_table *table, st_data_t key, st_index_t hash_val, st_index_t bin_pos)
+{
+    register st_table_entry *ptr = table->bins[bin_pos];
+    FOUND_ENTRY;
+    if (PTR_NOT_EQUAL(table, ptr, hash_val, key)) {
+	COLLISION;
+	while (PTR_NOT_EQUAL(table, ptr->next, hash_val, key)) {
+	    ptr = ptr->next;
+	}
+	ptr = ptr->next;
+    }
+    return ptr;
+}
+
+static inline st_index_t
+find_packed_index(st_table *table, st_index_t hash_val, st_data_t key)
+{
+    st_index_t i = 0;
+    while (i < table->real_entries &&
+	   (PHASH(table, i) != hash_val || !EQUAL(table, key, PKEY(table, i)))) {
+	i++;
+    }
+    return i;
+}
+
+static inline int
+check_upacked(st_table *table, st_index_t hash_val, st_data_t key)
+{
+    return table->num_entries &&
+	UPHASH(table) == hash_val &&
+	EQUAL(table, key, UPKEY(table));
+}
 
 #define collision_check 0
 
 int
 st_lookup(st_table *table, register st_data_t key, st_data_t *value)
 {
-    st_index_t hash_val, bin_pos;
+    st_index_t hash_val;
     register st_table_entry *ptr;
 
+    hash_val = do_hash(key, table);
+
+    if (ULTRAPACKED(table)) {
+	if (check_upacked(table, hash_val, key)) {
+	    if (value != 0) *value = UPVAL(table);
+	    return 1;
+	}
+	return 0;
+    }
+
     if (table->entries_packed) {
-        st_index_t i;
-        for (i = 0; i < table->num_entries; i++) {
-            if ((st_data_t)table->bins[i*2] == key) {
-                if (value !=0) *value = (st_data_t)table->bins[i*2+1];
-                return 1;
-            }
-        }
+	st_index_t i = find_packed_index(table, hash_val, key);
+	if (i < table->real_entries) {
+	    if (value != 0) *value = PVAL(table, i);
+	    return 1;
+	}
         return 0;
     }
 
-    hash_val = do_hash(key, table);
-    FIND_ENTRY(table, ptr, hash_val, bin_pos);
+    ptr = find_entry(table, key, hash_val, hash_val % table->num_bins);
 
     if (ptr == 0) {
 	return 0;
     }
     else {
-	if (value != 0)  *value = ptr->record;
+	if (value != 0) *value = ptr->record;
 	return 1;
     }
 }
@@ -353,22 +517,29 @@ st_lookup(st_table *table, register st_data_t key, st_data_t *value)
 int
 st_get_key(st_table *table, register st_data_t key, st_data_t *result)
 {
-    st_index_t hash_val, bin_pos;
+    st_index_t hash_val;
     register st_table_entry *ptr;
 
+    hash_val = do_hash(key, table);
+
+    if (ULTRAPACKED(table)) {
+	if (check_upacked(table, hash_val, key)) {
+	    if (result != 0) *result = UPKEY(table);
+	    return 1;
+	}
+	return 0;
+    }
+
     if (table->entries_packed) {
-        st_index_t i;
-        for (i = 0; i < table->num_entries; i++) {
-            if ((st_data_t)table->bins[i*2] == key) {
-                if (result !=0) *result = (st_data_t)table->bins[i*2];
-                return 1;
-            }
-        }
+	st_index_t i = find_packed_index(table, hash_val, key);
+	if (i < table->real_entries) {
+	    if (result != 0) *result = PKEY(table, i);
+	    return 1;
+	}
         return 0;
     }
 
-    hash_val = do_hash(key, table);
-    FIND_ENTRY(table, ptr, hash_val, bin_pos);
+    ptr = find_entry(table, key, hash_val, hash_val % table->num_bins);
 
     if (ptr == 0) {
 	return 0;
@@ -382,85 +553,151 @@ st_get_key(st_table *table, register st_data_t key, st_data_t *result)
 #undef collision_check
 #define collision_check 1
 
-#define MORE_PACKABLE_P(table) \
-    ((st_index_t)((table)->num_entries+1) * 2 <= (table)->num_bins && \
-     (table)->num_entries+1 <= MAX_PACKED_NUMHASH)
-
-#define ADD_DIRECT(table, key, value, hash_val, bin_pos)\
-do {\
-    st_table_entry *entry;\
-    if ((table)->num_entries > ST_DEFAULT_MAX_DENSITY * (table)->num_bins) {\
-	rehash(table);\
-        (bin_pos) = (hash_val) % (table)->num_bins;\
-    }\
-    \
-    entry = alloc(st_table_entry);\
-    \
-    entry->hash = (hash_val);\
-    entry->key = (key);\
-    entry->record = (value);\
-    entry->next = (table)->bins[(bin_pos)];\
-    if ((table)->head != 0) {\
-	entry->fore = 0;\
-	(entry->back = (table)->tail)->fore = entry;\
-	(table)->tail = entry;\
-    }\
-    else {\
-	(table)->head = (table)->tail = entry;\
-	entry->fore = entry->back = 0;\
-    }\
-    (table)->bins[(bin_pos)] = entry;\
-    (table)->num_entries++;\
-} while (0)
+static inline st_table_entry *
+new_entry(st_table * table, st_data_t key, st_data_t value,
+	st_index_t hash_val, register st_index_t bin_pos)
+{
+    register st_table_entry *entry = st_alloc_entry();
+
+    entry->next = table->bins[bin_pos];
+    table->bins[bin_pos] = entry;
+    entry->hash = hash_val;
+    entry->key = key;
+    entry->record = value;
+
+    return entry;
+}
+
+static inline void
+add_direct(st_table *table, st_data_t key, st_data_t value,
+	   st_index_t hash_val, register st_index_t bin_pos)
+{
+    register st_table_entry *entry;
+    if (table->num_entries > ST_DEFAULT_MAX_DENSITY * table->num_bins) {
+	rehash(table);
+        bin_pos = hash_val % table->num_bins;
+    }
+
+    entry = new_entry(table, key, value, hash_val, bin_pos);
+
+    if (table->head != 0) {
+	entry->fore = 0;
+	(entry->back = table->tail)->fore = entry;
+	table->tail = entry;
+    }
+    else {
+	table->head = table->tail = entry;
+	entry->fore = entry->back = 0;
+    }
+    table->num_entries++;
+}
 
 static void
 unpack_entries(register st_table *table)
 {
     st_index_t i;
-    struct st_table_entry *packed_bins[MAX_PACKED_NUMHASH*2];
+    st_packed_entry packed_bins[MAX_PACKED_HASH];
+    register st_table_entry *entry, *preventry = 0, **chain;
     st_table tmp_table = *table;
 
-    memcpy(packed_bins, table->bins, sizeof(struct st_table_entry *) * table->num_entries*2);
-    table->bins = packed_bins;
+    MEMCPY(packed_bins, PACKED_BINS(table), st_packed_entry, MAX_PACKED_HASH);
+    table->as.packed.entries = packed_bins;
     tmp_table.entries_packed = 0;
-    tmp_table.num_entries = 0;
-    memset(tmp_table.bins, 0, sizeof(struct st_table_entry *) * tmp_table.num_bins);
-    for (i = 0; i < table->num_entries; i++) {
-        st_insert(&tmp_table, (st_data_t)packed_bins[i*2], (st_data_t)packed_bins[i*2+1]);
-    }
+#if ST_DEFAULT_INIT_TABLE_SIZE == ST_DEFAULT_PACKED_TABLE_SIZE
+    MEMZERO(tmp_table.bins, st_table_entry*, tmp_table.num_bins);
+#else
+    tmp_table.bins = st_realloc_bins(tmp_table.bins, ST_DEFAULT_INIT_TABLE_SIZE, tmp_table.num_bins);
+    tmp_table.num_bins = ST_DEFAULT_INIT_TABLE_SIZE;
+#endif
+    i = 0;
+    chain = &tmp_table.head;
+    do {
+	st_data_t key = packed_bins[i].key;
+	st_data_t val = packed_bins[i].val;
+	st_index_t hash = packed_bins[i].hash;
+	entry = new_entry(&tmp_table, key, val, hash,
+			  hash % ST_DEFAULT_INIT_TABLE_SIZE);
+	*chain = entry;
+	entry->back = preventry;
+	preventry = entry;
+	chain = &entry->fore;
+    } while (++i < MAX_PACKED_HASH);
+    *chain = NULL;
+    tmp_table.tail = entry;
     *table = tmp_table;
 }
 
+static void
+add_packed_direct(st_table *table, st_data_t key, st_data_t value, st_index_t hash_val)
+{
+    if (table->real_entries < MAX_PACKED_HASH) {
+	st_index_t i = table->real_entries++;
+	PKEY_SET(table, i, key);
+	PVAL_SET(table, i, value);
+	PHASH_SET(table, i, hash_val);
+	table->num_entries++;
+    }
+    else {
+	unpack_entries(table);
+	add_direct(table, key, value, hash_val, hash_val % table->num_bins);
+    }
+}
+
+static void
+add_upacked_direct(register st_table *table, register st_data_t key, st_data_t value, st_index_t hash_val)
+{
+    if (table->real_upacked) {
+	st_packed_entry *entries = (st_packed_entry *) st_alloc_bins(ST_DEFAULT_PACKED_TABLE_SIZE);
+	entries[0] = UPACKED_ENT(table);
+	entries[1].hash = hash_val;
+	entries[1].key = key;
+	entries[1].val = value;
+	table->num_bins = ST_DEFAULT_PACKED_TABLE_SIZE;
+	table->real_entries = 2;
+	table->num_entries++;
+	table->as.packed.entries = entries;
+    }
+    else {
+	table->real_upacked = 1;
+	table->num_entries = 1;
+	UPHASH_SET(table, hash_val);
+	UPKEY_SET(table, key);
+	UPVAL_SET(table, value);
+    }
+}
+
 int
 st_insert(register st_table *table, register st_data_t key, st_data_t value)
 {
-    st_index_t hash_val, bin_pos;
+    st_index_t hash_val;
+    register st_index_t bin_pos;
     register st_table_entry *ptr;
 
+    hash_val = do_hash(key, table);
+
+    if (ULTRAPACKED(table)) {
+	if (check_upacked(table, hash_val, key)) {
+	    UPVAL_SET(table, value);
+	    return 1;
+	}
+	add_upacked_direct(table, key, value, hash_val);
+	return 0;
+    }
+
     if (table->entries_packed) {
-        st_index_t i;
-        for (i = 0; i < table->num_entries; i++) {
-            if ((st_data_t)table->bins[i*2] == key) {
-                table->bins[i*2+1] = (struct st_table_entry*)value;
-                return 1;
-            }
-        }
-        if (MORE_PACKABLE_P(table)) {
-            i = table->num_entries++;
-            table->bins[i*2] = (struct st_table_entry*)key;
-            table->bins[i*2+1] = (struct st_table_entry*)value;
-            return 0;
-        }
-        else {
-            unpack_entries(table);
+	st_index_t i = find_packed_index(table, hash_val, key);
+	if (i < table->real_entries) {
+	    PVAL_SET(table, i, value);
+	    return 1;
         }
+	add_packed_direct(table, key, value, hash_val);
+	return 0;
     }
 
-    hash_val = do_hash(key, table);
     FIND_ENTRY(table, ptr, hash_val, bin_pos);
 
     if (ptr == 0) {
-	ADD_DIRECT(table, key, value, hash_val, bin_pos);
+	add_direct(table, key, value, hash_val, bin_pos);
 	return 0;
     }
     else {
@@ -473,34 +710,38 @@ int
 st_insert2(register st_table *table, register st_data_t key, st_data_t value,
 	   st_data_t (*func)(st_data_t))
 {
-    st_index_t hash_val, bin_pos;
+    st_index_t hash_val;
+    register st_index_t bin_pos;
     register st_table_entry *ptr;
 
+    hash_val = do_hash(key, table);
+
+    if (ULTRAPACKED(table)) {
+	if (check_upacked(table, hash_val, key)) {
+	    UPVAL_SET(table, value);
+	    return 1;
+	}
+	key = (*func)(key);
+	add_upacked_direct(table, key, value, hash_val);
+	return 0;
+    }
+
     if (table->entries_packed) {
-        st_index_t i;
-        for (i = 0; i < table->num_entries; i++) {
-            if ((st_data_t)table->bins[i*2] == key) {
-                table->bins[i*2+1] = (struct st_table_entry*)value;
-                return 1;
-            }
-        }
-        if (MORE_PACKABLE_P(table)) {
-            i = table->num_entries++;
-            table->bins[i*2] = (struct st_table_entry*)key;
-            table->bins[i*2+1] = (struct st_table_entry*)value;
-            return 0;
-        }
-        else {
-            unpack_entries(table);
-        }
+	st_index_t i = find_packed_index(table, hash_val, key);
+	if (i < table->real_entries) {
+	    PVAL_SET(table, i, value);
+	    return 1;
+	}
+	key = (*func)(key);
+	add_packed_direct(table, key, value, hash_val);
+	return 0;
     }
 
-    hash_val = do_hash(key, table);
     FIND_ENTRY(table, ptr, hash_val, bin_pos);
 
     if (ptr == 0) {
 	key = (*func)(key);
-	ADD_DIRECT(table, key, value, hash_val, bin_pos);
+	add_direct(table, key, value, hash_val, bin_pos);
 	return 0;
     }
     else {
@@ -512,36 +753,30 @@ st_insert2(register st_table *table, register st_data_t key, st_data_t value,
 void
 st_add_direct(st_table *table, st_data_t key, st_data_t value)
 {
-    st_index_t hash_val, bin_pos;
+    st_index_t hash_val;
+
+    hash_val = do_hash(key, table);
+    if (ULTRAPACKED(table)) {
+	add_upacked_direct(table, key, value, hash_val);
+	return;
+    }
 
     if (table->entries_packed) {
-        st_index_t i;
-        if (MORE_PACKABLE_P(table)) {
-            i = table->num_entries++;
-            table->bins[i*2] = (struct st_table_entry*)key;
-            table->bins[i*2+1] = (struct st_table_entry*)value;
-            return;
-        }
-        else {
-            unpack_entries(table);
-        }
+	add_packed_direct(table, key, value, hash_val);
+	return;
     }
 
-    hash_val = do_hash(key, table);
-    bin_pos = hash_val % table->num_bins;
-    ADD_DIRECT(table, key, value, hash_val, bin_pos);
+    add_direct(table, key, value, hash_val, hash_val % table->num_bins);
 }
 
 static void
 rehash(register st_table *table)
 {
     register st_table_entry *ptr, **new_bins;
-    st_index_t i, new_num_bins, hash_val;
+    st_index_t new_num_bins, hash_val;
 
     new_num_bins = new_size(table->num_bins+1);
-    new_bins = (st_table_entry**)
-	xrealloc(table->bins, new_num_bins * sizeof(st_table_entry*));
-    for (i = 0; i < new_num_bins; ++i) new_bins[i] = 0;
+    new_bins = st_realloc_bins(table->bins, new_num_bins, table->num_bins);
     table->num_bins = new_num_bins;
     table->bins = new_bins;
 
@@ -558,34 +793,37 @@ st_table*
 st_copy(st_table *old_table)
 {
     st_table *new_table;
-    st_table_entry *ptr, *entry, *prev, **tail;
+    st_table_entry *ptr, *entry, *prev, **tailp;
     st_index_t num_bins = old_table->num_bins;
     st_index_t hash_val;
 
-    new_table = alloc(st_table);
+    new_table = st_alloc_table();
     if (new_table == 0) {
 	return 0;
     }
 
     *new_table = *old_table;
-    new_table->bins = (st_table_entry**)
-	Calloc((unsigned)num_bins, sizeof(st_table_entry*));
+    if (ULTRAPACKED(old_table)) {
+	return new_table;
+    }
+
+    new_table->bins = st_alloc_bins(num_bins);
 
     if (new_table->bins == 0) {
-	free(new_table);
+	st_dealloc_table(new_table);
 	return 0;
     }
 
     if (old_table->entries_packed) {
-        memcpy(new_table->bins, old_table->bins, sizeof(struct st_table_entry *) * old_table->num_bins);
+        MEMCPY(new_table->bins, old_table->bins, st_table_entry*, old_table->num_bins);
         return new_table;
     }
 
     if ((ptr = old_table->head) != 0) {
 	prev = 0;
-	tail = &new_table->head;
+	tailp = &new_table->head;
 	do {
-	    entry = alloc(st_table_entry);
+	    entry = st_alloc_entry();
 	    if (entry == 0) {
 		st_free_table(new_table);
 		return 0;
@@ -595,8 +833,8 @@ st_copy(st_table *old_table)
 	    entry->next = new_table->bins[hash_val];
 	    new_table->bins[hash_val] = entry;
 	    entry->back = prev;
-	    *tail = prev = entry;
-	    tail = &entry->fore;
+	    *tailp = prev = entry;
+	    tailp = &entry->fore;
 	} while ((ptr = ptr->fore) != 0);
 	new_table->tail = prev;
     }
@@ -604,21 +842,22 @@ st_copy(st_table *old_table)
     return new_table;
 }
 
-#define REMOVE_ENTRY(table, ptr) do					\
-    {									\
-	if ((ptr)->fore == 0 && (ptr)->back == 0) {			\
-	    (table)->head = 0;						\
-	    (table)->tail = 0;						\
-	}								\
-	else {								\
-	    st_table_entry *fore = (ptr)->fore, *back = (ptr)->back;	\
-	    if (fore) fore->back = back;				\
-	    if (back) back->fore = fore;				\
-	    if ((ptr) == (table)->head) (table)->head = fore;		\
-	    if ((ptr) == (table)->tail) (table)->tail = back;		\
-	}								\
-	(table)->num_entries--;						\
-    } while (0)
+static inline void
+remove_entry(st_table *table, st_table_entry *ptr)
+{
+    if (ptr->fore == 0 && ptr->back == 0) {
+	table->head = 0;
+	table->tail = 0;
+    }
+    else {
+	st_table_entry *fore = ptr->fore, *back = ptr->back;
+	if (fore) fore->back = back;
+	if (back) back->fore = fore;
+	if (ptr == table->head) table->head = fore;
+	if (ptr == table->tail) table->tail = back;
+    }
+    table->num_entries--;
+}
 
 int
 st_delete(register st_table *table, register st_data_t *key, st_data_t *value)
@@ -627,30 +866,38 @@ st_delete(register st_table *table, register st_data_t *key, st_data_t *value)
     st_table_entry **prev;
     register st_table_entry *ptr;
 
+    hash_val = do_hash(*key, table);
+
+    if (ULTRAPACKED(table)) {
+	if (check_upacked(table, hash_val, *key)) {
+	    if (value != 0) *value = UPVAL(table);
+	    *key = UPKEY(table);
+	    remove_upacked_entry(table);
+	    return 1;
+	}
+	return 0;
+    }
+
     if (table->entries_packed) {
-        st_index_t i;
-        for (i = 0; i < table->num_entries; i++) {
-            if ((st_data_t)table->bins[i*2] == *key) {
-                if (value != 0) *value = (st_data_t)table->bins[i*2+1];
-                table->num_entries--;
-                memmove(&table->bins[i*2], &table->bins[(i+1)*2],
-                        sizeof(struct st_table_entry*) * 2*(table->num_entries-i));
-                return 1;
-            }
+	st_index_t i = find_packed_index(table, hash_val, *key);
+	if (i < table->real_entries) {
+	    if (value != 0) *value = PVAL(table, i);
+	    *key = PKEY(table, i);
+	    remove_packed_entry(table, i);
+	    return 1;
         }
         if (value != 0) *value = 0;
         return 0;
     }
 
-    hash_val = do_hash_bin(*key, table);
-
-    for (prev = &table->bins[hash_val]; (ptr = *prev) != 0; prev = &ptr->next) {
+    prev = &table->bins[hash_val % table->num_bins];
+    for (;(ptr = *prev) != 0; prev = &ptr->next) {
 	if (EQUAL(table, *key, ptr->key)) {
 	    *prev = ptr->next;
-	    REMOVE_ENTRY(table, ptr);
+	    remove_entry(table, ptr);
 	    if (value != 0) *value = ptr->record;
 	    *key = ptr->key;
-	    free(ptr);
+	    st_free_entry(ptr);
 	    return 1;
 	}
     }
@@ -665,25 +912,36 @@ st_delete_safe(register st_table *table, register st_data_t *key, st_data_t *val
     st_index_t hash_val;
     register st_table_entry *ptr;
 
+    hash_val = do_hash(*key, table);
+
+    if (ULTRAPACKED(table)) {
+	if (check_upacked(table, hash_val, *key)) {
+	    if (value != 0) *value = UPVAL(table);
+	    *key = UPKEY(table);
+	    remove_safe_upacked_entry(table, never);
+	    return 1;
+	}
+	if (value != 0) *value = 0;
+	return 0;
+    }
+
     if (table->entries_packed) {
-	st_index_t i;
-	for (i = 0; i < table->num_entries; i++) {
-	    if ((st_data_t)table->bins[i*2] == *key) {
-		if (value != 0) *value = (st_data_t)table->bins[i*2+1];
-		table->bins[i*2] = (void *)never;
-		return 1;
-	    }
+	st_index_t i = find_packed_index(table, hash_val, *key);
+	if (i < table->real_entries) {
+	    if (value != 0) *value = PVAL(table, i);
+	    *key = PKEY(table, i);
+	    remove_safe_packed_entry(table, i, never);
+	    return 1;
 	}
 	if (value != 0) *value = 0;
 	return 0;
     }
 
-    hash_val = do_hash_bin(*key, table);
-    ptr = table->bins[hash_val];
+    ptr = table->bins[hash_val % table->num_bins];
 
     for (; ptr != 0; ptr = ptr->next) {
 	if ((ptr->key != never) && EQUAL(table, ptr->key, *key)) {
-	    REMOVE_ENTRY(table, ptr);
+	    remove_entry(table, ptr);
 	    *key = ptr->key;
 	    if (value != 0) *value = ptr->record;
 	    ptr->key = ptr->record = never;
@@ -707,24 +965,29 @@ st_shift(register st_table *table, register st_data_t *key, st_data_t *value)
         return 0;
     }
 
-    if (table->entries_packed) {
-        if (value != 0) *value = (st_data_t)table->bins[1];
-        *key = (st_data_t)table->bins[0];
-        table->num_entries--;
-        memmove(&table->bins[0], &table->bins[2],
-                sizeof(struct st_table_entry*) * 2*table->num_entries);
+    if (ULTRAPACKED(table)) {
+        if (value != 0) *value = UPVAL(table);
+        *key = UPKEY(table);
+        remove_upacked_entry(table);
         return 1;
     }
 
+    if (table->entries_packed) {
+	if (value != 0) *value = PVAL(table, 0);
+	*key = PKEY(table, 0);
+	remove_packed_entry(table, 0);
+	return 1;
+    }
+
     hash_val = do_hash_bin(table->head->key, table);
     prev = &table->bins[hash_val];
     for (;(ptr = *prev) != 0; prev = &ptr->next) {
 	if (ptr == table->head) {
 	    *prev = ptr->next;
-            REMOVE_ENTRY(table, ptr);
+            remove_entry(table, ptr);
             if (value != 0) *value = ptr->record;
             *key = ptr->key;
-            free(ptr);
+            st_free_entry(ptr);
             return 1;
 	}
     }
@@ -740,17 +1003,23 @@ st_cleanup_safe(st_table *table, st_data_t never)
     st_table_entry *ptr, **last, *tmp;
     st_index_t i;
 
+    if (ULTRAPACKED(table)) {
+	table->real_upacked = table->num_entries;
+	return;
+    }
+
     if (table->entries_packed) {
 	st_index_t i = 0, j = 0;
-	while ((st_data_t)table->bins[i*2] != never) {
-	    if (i++ == table->num_entries) return;
+	while (PKEY(table, i) != never) {
+	    if (i++ == table->real_entries) return;
 	}
-	for (j = i; ++i < table->num_entries;) {
-	    if ((st_data_t)table->bins[i*2] == never) continue;
-	    table->bins[j*2] = table->bins[i*2];
-	    table->bins[j*2+1] = table->bins[i*2+1];
+	for (j = i; ++i < table->real_entries;) {
+	    if (PKEY(table, i) == never) continue;
+	    PACKED_ENT(table, j) = PACKED_ENT(table, i);
 	    j++;
 	}
+	table->real_entries = j;
+	/* table->num_entries really should be equal j at this moment, but let set it anyway */
 	table->num_entries = j;
 	return;
     }
@@ -761,7 +1030,7 @@ st_cleanup_safe(st_table *table, st_data_t never)
 	    if (ptr->key == never) {
 		tmp = ptr;
 		*last = ptr = ptr->next;
-		free(tmp);
+		st_free_entry(tmp);
 	    }
 	    else {
 		ptr = *(last = &ptr->next);
@@ -771,50 +1040,78 @@ st_cleanup_safe(st_table *table, st_data_t never)
 }
 
 int
-st_foreach(st_table *table, int (*func)(ANYARGS), st_data_t arg)
+st_foreach_check(st_table *table, int (*func)(ANYARGS), st_data_t arg, st_data_t never)
 {
     st_table_entry *ptr, **last, *tmp;
     enum st_retval retval;
-    st_index_t i;
+    st_data_t key, val;
+    st_index_t hash, i = 0;
+
+    if (table->num_entries == 0) {
+	return 0;
+    }
+
+    if (ULTRAPACKED(table)) {
+	key = UPKEY(table);
+	val = UPVAL(table);
+	hash = UPHASH(table);
+	if (key == never) return 0;
+	retval = (*func)(key, val, arg);
+	if (!ULTRAPACKED(table)) {
+	    goto packed;
+	}
+	switch(retval) {
+	  case ST_CHECK:
+	    if (UPHASH(table) == 0 && UPKEY(table) == never)
+		break;
+	    if (check_upacked(table, hash, key))
+		break;
+	    goto deleted;
+	  case ST_DELETE:
+	    remove_safe_upacked_entry(table, never);
+	  case ST_CONTINUE:
+	  case ST_STOP:
+	    break;
+	}
+	return 0;
+    }
 
     if (table->entries_packed) {
-        for (i = 0; i < table->num_entries; i++) {
-            st_index_t j;
-            st_data_t key, val;
-            key = (st_data_t)table->bins[i*2];
-            val = (st_data_t)table->bins[i*2+1];
-            retval = (*func)(key, val, arg);
+	for (i = 0; i < table->real_entries; i++) {
+	    key = PKEY(table, i);
+	    val = PVAL(table, i);
+	    hash = PHASH(table, i);
+	    if (key == never) continue;
+	    retval = (*func)(key, val, arg);
+	  packed:
 	    if (!table->entries_packed) {
-		FIND_ENTRY(table, ptr, key, i);
+		FIND_ENTRY(table, ptr, hash, i);
 		if (retval == ST_CHECK) {
 		    if (!ptr) goto deleted;
 		    goto unpacked_continue;
 		}
 		goto unpacked;
 	    }
-            switch (retval) {
+	    switch (retval) {
 	      case ST_CHECK:	/* check if hash is modified during iteration */
-                for (j = 0; j < table->num_entries; j++) {
-                    if ((st_data_t)table->bins[j*2] == key)
-                        break;
-                }
-                if (j == table->num_entries) {
+		if (PHASH(table, i) == 0 && PKEY(table, i) == never) {
+		    break;
+		}
+		i = find_packed_index(table, hash, key);
+		if (i == table->real_entries) {
 		    goto deleted;
-                }
+		}
 		/* fall through */
 	      case ST_CONTINUE:
 		break;
 	      case ST_STOP:
 		return 0;
 	      case ST_DELETE:
-                table->num_entries--;
-                memmove(&table->bins[i*2], &table->bins[(i+1)*2],
-                        sizeof(struct st_table_entry*) * 2*(table->num_entries-i));
-                i--;
-                break;
-            }
-        }
-        return 0;
+		remove_safe_packed_entry(table, i, never);
+		break;
+	    }
+	}
+	return 0;
     }
     else {
 	ptr = table->head;
@@ -822,6 +1119,8 @@ st_foreach(st_table *table, int (*func)(ANYARGS), st_data_t arg)
 
     if (ptr != 0) {
 	do {
+	    if (ptr->key == never)
+		goto unpacked_continue;
 	    i = ptr->hash % table->num_bins;
 	    retval = (*func)(ptr->key, ptr->record, arg);
 	  unpacked:
@@ -847,10 +1146,100 @@ st_foreach(st_table *table, int (*func)(ANYARGS), st_data_t arg)
 		for (; (tmp = *last) != 0; last = &tmp->next) {
 		    if (ptr == tmp) {
 			tmp = ptr->fore;
+			remove_entry(table, ptr);
+			ptr->key = ptr->record = never;
+			ptr->hash = 0;
+			ptr = tmp;
+			break;
+		    }
+		}
+	    }
+	} while (ptr && table->head);
+    }
+    return 0;
+}
+
+int
+st_foreach(st_table *table, int (*func)(ANYARGS), st_data_t arg)
+{
+    st_table_entry *ptr, **last, *tmp;
+    enum st_retval retval;
+    st_data_t key, val;
+    st_index_t hash, i = 0;
+
+    if (table->num_entries == 0) {
+	return 0;
+    }
+
+    if (ULTRAPACKED(table)) {
+	key = UPKEY(table);
+	val = UPVAL(table);
+	hash = UPHASH(table);
+	retval = (*func)(key, val, arg);
+	if (!ULTRAPACKED(table)) {
+	    goto packed;
+	}
+	switch (retval) {
+	  case ST_DELETE:
+	    remove_upacked_entry(table);
+	  case ST_CONTINUE:
+	  case ST_CHECK:
+	  case ST_STOP:
+	    break;
+	}
+	return 0;
+    }
+
+    if (table->entries_packed) {
+	for (i = 0; i < table->real_entries; i++) {
+	    key = PKEY(table, i);
+	    val = PVAL(table, i);
+	    hash = PHASH(table, i);
+	    retval = (*func)(key, val, arg);
+	  packed:
+	    if (!table->entries_packed) {
+		FIND_ENTRY(table, ptr, hash, i);
+		if (!ptr) return 0;
+		goto unpacked;
+	    }
+	    switch (retval) {
+	      case ST_CONTINUE:
+		break;
+	      case ST_CHECK:
+	      case ST_STOP:
+		return 0;
+	      case ST_DELETE:
+		remove_packed_entry(table, i);
+		i--;
+		break;
+	    }
+	}
+	return 0;
+    }
+    else {
+	ptr = table->head;
+    }
+
+    if (ptr != 0) {
+	do {
+	    i = ptr->hash % table->num_bins;
+	    retval = (*func)(ptr->key, ptr->record, arg);
+	  unpacked:
+	    switch (retval) {
+	      case ST_CONTINUE:
+		ptr = ptr->fore;
+		break;
+	      case ST_CHECK:
+	      case ST_STOP:
+		return 0;
+	      case ST_DELETE:
+		last = &table->bins[ptr->hash % table->num_bins];
+		for (; (tmp = *last) != 0; last = &tmp->next) {
+		    if (ptr == tmp) {
+			tmp = ptr->fore;
 			*last = ptr->next;
-			REMOVE_ENTRY(table, ptr);
-			free(ptr);
-			if (ptr == tmp) return 0;
+			remove_entry(table, ptr);
+			st_free_entry(ptr);
 			ptr = tmp;
 			break;
 		    }
@@ -873,13 +1262,13 @@ st_reverse_foreach(st_table *table, int (*func)(ANYARGS), st_data_t arg)
         for (i = table->num_entries-1; 0 <= i; i--) {
             int j;
             st_data_t key, val;
-            key = (st_data_t)table->bins[i*2];
-            val = (st_data_t)table->bins[i*2+1];
+            key = PKEY(table, i);
+            val = PVAL(table, i);
             retval = (*func)(key, val, arg);
             switch (retval) {
 	      case ST_CHECK:	/* check if hash is modified during iteration */
                 for (j = 0; j < table->num_entries; j++) {
-                    if ((st_data_t)table->bins[j*2] == key)
+                    if (PKEY(table, j) == key)
                         break;
                 }
                 if (j == table->num_entries) {
@@ -893,9 +1282,7 @@ st_reverse_foreach(st_table *table, int (*func)(ANYARGS), st_data_t arg)
 	      case ST_STOP:
 		return 0;
 	      case ST_DELETE:
-                table->num_entries--;
-                memmove(&table->bins[i*2], &table->bins[(i+1)*2],
-                        sizeof(struct st_table_entry*) * 2*(table->num_entries-i));
+		remove_packed_entry(table, i);
                 break;
             }
         }
@@ -928,8 +1315,8 @@ st_reverse_foreach(st_table *table, int (*func)(ANYARGS), st_data_t arg)
 		    if (ptr == tmp) {
 			tmp = ptr->back;
 			*last = ptr->next;
-			REMOVE_ENTRY(table, ptr);
-			free(ptr);
+			remove_entry(table, ptr);
+			st_free_entry(ptr);
 			ptr = tmp;
 			break;
 		    }
diff --git a/class.c b/class.c
index 02e6826..6bca707 100644
--- a/class.c
+++ b/class.c
@@ -31,7 +31,7 @@
 #include "internal.h"
 #include <ctype.h>
 
-extern st_table *rb_class_tbl;
+extern sa_table rb_class_tbl;
 static ID id_attached;
 
 /**
@@ -53,11 +53,15 @@ class_alloc(VALUE flags, VALUE klass)
     NEWOBJ(obj, struct RClass);
     OBJSETUP(obj, klass, flags);
     obj->ptr = ext;
-    RCLASS_IV_TBL(obj) = 0;
-    RCLASS_CONST_TBL(obj) = 0;
-    RCLASS_M_TBL(obj) = 0;
-    RCLASS_SUPER(obj) = 0;
-    RCLASS_IV_INDEX_TBL(obj) = 0;
+    MEMZERO(ext, struct rb_classext_struct, 1);
+    return (VALUE)obj;
+}
+
+static VALUE
+iclass_alloc()
+{
+    NEWOBJ(obj, struct RClass);
+    OBJSETUP(obj, rb_cClass, T_ICLASS);
     return (VALUE)obj;
 }
 
@@ -77,7 +81,6 @@ rb_class_boot(VALUE super)
     VALUE klass = class_alloc(T_CLASS, rb_cClass);
 
     RCLASS_SUPER(klass) = super;
-    RCLASS_M_TBL(klass) = st_init_numtable();
 
     OBJ_INFECT(klass, super);
     return (VALUE)klass;
@@ -120,43 +123,30 @@ rb_class_new(VALUE super)
     return rb_class_boot(super);
 }
 
-struct clone_method_data {
-    st_table *tbl;
-    VALUE klass;
-};
-
 VALUE rb_iseq_clone(VALUE iseqval, VALUE newcbase);
 
-static int
-clone_method(ID mid, const rb_method_entry_t *me, struct clone_method_data *data)
+static void
+clone_method(ID mid, const rb_method_entry_t *me, VALUE klass)
 {
     VALUE newiseqval;
     if (me->def && me->def->type == VM_METHOD_TYPE_ISEQ) {
 	rb_iseq_t *iseq;
-	newiseqval = rb_iseq_clone(me->def->body.iseq->self, data->klass);
+	newiseqval = rb_iseq_clone(me->def->body.iseq->self, klass);
 	GetISeqPtr(newiseqval, iseq);
-	rb_add_method(data->klass, mid, VM_METHOD_TYPE_ISEQ, iseq, me->flag);
+	rb_add_method(klass, mid, VM_METHOD_TYPE_ISEQ, iseq, me->flag);
 	RB_GC_GUARD(newiseqval);
     }
     else {
-	rb_method_entry_set(data->klass, mid, me, me->flag);
+	rb_method_entry_set(klass, mid, me, me->flag);
     }
-    return ST_CONTINUE;
 }
 
-static int
-clone_const(ID key, const rb_const_entry_t *ce, st_table *tbl)
+static void
+clone_const(sa_index_t key, st_data_t ce, sa_table *tbl)
 {
     rb_const_entry_t *nce = ALLOC(rb_const_entry_t);
-    *nce = *ce;
-    st_insert(tbl, key, (st_data_t)nce);
-    return ST_CONTINUE;
-}
-
-static int
-clone_const_i(st_data_t key, st_data_t value, st_data_t data)
-{
-    return clone_const((ID)key, (const rb_const_entry_t *)value, (st_table *)data);
+    *nce = *(const rb_const_entry_t*)ce;
+    sa_insert(tbl, (sa_index_t)key, (st_data_t)nce);
 }
 
 static void
@@ -177,6 +167,7 @@ class_init_copy_check(VALUE clone, VALUE orig)
 VALUE
 rb_mod_init_copy(VALUE clone, VALUE orig)
 {
+    ID id;
     if (RB_TYPE_P(clone, T_CLASS)) {
 	class_init_copy_check(clone, orig);
     }
@@ -186,36 +177,22 @@ rb_mod_init_copy(VALUE clone, VALUE orig)
 	rb_singleton_class_attached(RBASIC(clone)->klass, (VALUE)clone);
     }
     RCLASS_SUPER(clone) = RCLASS_SUPER(orig);
-    if (RCLASS_IV_TBL(orig)) {
-	st_data_t id;
 
-	if (RCLASS_IV_TBL(clone)) {
-	    st_free_table(RCLASS_IV_TBL(clone));
-	}
-	RCLASS_IV_TBL(clone) = st_copy(RCLASS_IV_TBL(orig));
-	CONST_ID(id, "__classpath__");
-	st_delete(RCLASS_IV_TBL(clone), &id, 0);
-	CONST_ID(id, "__classid__");
-	st_delete(RCLASS_IV_TBL(clone), &id, 0);
-    }
-    if (RCLASS_CONST_TBL(orig)) {
-	if (RCLASS_CONST_TBL(clone)) {
-	    rb_free_const_table(RCLASS_CONST_TBL(clone));
-	}
-	RCLASS_CONST_TBL(clone) = st_init_numtable();
-	st_foreach(RCLASS_CONST_TBL(orig), clone_const_i, (st_data_t)RCLASS_CONST_TBL(clone));
-    }
-    if (RCLASS_M_TBL(orig)) {
-	struct clone_method_data data;
+    sa_copy_to(RCLASS_IV_TBL(orig), RCLASS_IV_TBL(clone));
+    CONST_ID(id, "__classpath__");
+    sa_delete(RCLASS_IV_TBL(clone), (sa_index_t)id, 0);
+    CONST_ID(id, "__classid__");
+    sa_delete(RCLASS_IV_TBL(clone), (sa_index_t)id, 0);
 
-	if (RCLASS_M_TBL(clone)) {
-	    rb_free_m_table(RCLASS_M_TBL(clone));
-	}
-	data.tbl = RCLASS_M_TBL(clone) = st_init_numtable();
-	data.klass = clone;
-	st_foreach(RCLASS_M_TBL(orig), clone_method,
-		   (st_data_t)&data);
-    }
+    sa_clear(RCLASS_CONST_TBL(clone));
+    SA_FOREACH_START(RCLASS_CONST_TBL(orig));
+    clone_const(entry->key, value, RCLASS_CONST_TBL(clone));
+    SA_FOREACH_END();
+
+    rb_free_m_table(RCLASS_M_TBL(clone));
+    SA_FOREACH_START(RCLASS_M_TBL(orig));
+    clone_method(entry->key, (const rb_method_entry_t *)value, clone);
+    SA_FOREACH_END();
 
     return clone;
 }
@@ -228,7 +205,6 @@ rb_singleton_class_clone(VALUE obj)
     if (!FL_TEST(klass, FL_SINGLETON))
 	return klass;
     else {
-	struct clone_method_data data;
 	/* copy singleton(unnamed) class */
 	VALUE clone = class_alloc((RBASIC(klass)->flags & ~(FL_MARK)), 0);
 
@@ -240,18 +216,16 @@ rb_singleton_class_clone(VALUE obj)
 	}
 
 	RCLASS_SUPER(clone) = RCLASS_SUPER(klass);
-	if (RCLASS_IV_TBL(klass)) {
-	    RCLASS_IV_TBL(clone) = st_copy(RCLASS_IV_TBL(klass));
-	}
-	if (RCLASS_CONST_TBL(klass)) {
-	    RCLASS_CONST_TBL(clone) = st_init_numtable();
-	    st_foreach(RCLASS_CONST_TBL(klass), clone_const_i, (st_data_t)RCLASS_CONST_TBL(clone));
-	}
-	RCLASS_M_TBL(clone) = st_init_numtable();
-	data.tbl = RCLASS_M_TBL(clone);
-	data.klass = (VALUE)clone;
-	st_foreach(RCLASS_M_TBL(klass), clone_method,
-		   (st_data_t)&data);
+        sa_copy_to(RCLASS_IV_TBL(klass), RCLASS_IV_TBL(clone));
+
+        SA_FOREACH_START(RCLASS_CONST_TBL(klass));
+        clone_const(entry->key, value, RCLASS_CONST_TBL(clone));
+        SA_FOREACH_END();
+
+        SA_FOREACH_START(RCLASS_M_TBL(klass));
+        clone_method(entry->key, (const rb_method_entry_t*)value, clone);
+        SA_FOREACH_END();
+
 	rb_singleton_class_attached(RBASIC(clone)->klass, (VALUE)clone);
 	FL_SET(clone, FL_SINGLETON);
 	return (VALUE)clone;
@@ -266,10 +240,7 @@ void
 rb_singleton_class_attached(VALUE klass, VALUE obj)
 {
     if (FL_TEST(klass, FL_SINGLETON)) {
-	if (!RCLASS_IV_TBL(klass)) {
-	    RCLASS_IV_TBL(klass) = st_init_numtable();
-	}
-	st_insert(RCLASS_IV_TBL(klass), id_attached, obj);
+	sa_insert(RCLASS_IV_TBL(klass), (sa_index_t)id_attached, obj);
     }
 }
 
@@ -356,12 +327,12 @@ make_singleton_class(VALUE obj)
 static VALUE
 boot_defclass(const char *name, VALUE super)
 {
-    extern st_table *rb_class_tbl;
+    extern sa_table rb_class_tbl;
     VALUE obj = rb_class_boot(super);
     ID id = rb_intern(name);
 
     rb_name_class(obj, id);
-    st_add_direct(rb_class_tbl, id, obj);
+    sa_insert(&rb_class_tbl, (sa_index_t)id, obj);
     rb_const_set((rb_cObject ? rb_cObject : obj), id, obj);
     return obj;
 }
@@ -485,7 +456,7 @@ rb_define_class(const char *name, VALUE super)
 	rb_warn("no super class for `%s', Object assumed", name);
     }
     klass = rb_define_class_id(id, super);
-    st_add_direct(rb_class_tbl, id, klass);
+    sa_insert(&rb_class_tbl, (sa_index_t)id, klass);
     rb_name_class(klass, id);
     rb_const_set(rb_cObject, id, klass);
     rb_class_inherited(super, klass);
@@ -566,8 +537,6 @@ rb_module_new(void)
 {
     VALUE mdl = class_alloc(T_MODULE, rb_cModule);
 
-    RCLASS_M_TBL(mdl) = st_init_numtable();
-
     return (VALUE)mdl;
 }
 
@@ -596,7 +565,7 @@ rb_define_module(const char *name)
 	rb_raise(rb_eTypeError, "%s is not a module", rb_obj_classname(module));
     }
     module = rb_define_module_id(id);
-    st_add_direct(rb_class_tbl, id, module);
+    sa_insert(&rb_class_tbl, (sa_index_t)id, module);
     rb_const_set(rb_cObject, id, module);
 
     return module;
@@ -631,27 +600,15 @@ rb_define_module_id_under(VALUE outer, ID id)
 static VALUE
 include_class_new(VALUE module, VALUE super)
 {
-    VALUE klass = class_alloc(T_ICLASS, rb_cClass);
+    VALUE klass;
 
     if (BUILTIN_TYPE(module) == T_ICLASS) {
 	module = RBASIC(module)->klass;
     }
-    if (!RCLASS_IV_TBL(module)) {
-	RCLASS_IV_TBL(module) = st_init_numtable();
-    }
-    if (!RCLASS_CONST_TBL(module)) {
-	RCLASS_CONST_TBL(module) = st_init_numtable();
-    }
-    RCLASS_IV_TBL(klass) = RCLASS_IV_TBL(module);
-    RCLASS_CONST_TBL(klass) = RCLASS_CONST_TBL(module);
-    RCLASS_M_TBL(klass) = RCLASS_M_TBL(module);
+    klass = iclass_alloc();
+    RBASIC(klass)->klass = module;
+    RCLASS_EXT(klass) = RCLASS_EXT(module);
     RCLASS_SUPER(klass) = super;
-    if (TYPE(module) == T_ICLASS) {
-	RBASIC(klass)->klass = RBASIC(module)->klass;
-    }
-    else {
-	RBASIC(klass)->klass = module;
-    }
     OBJ_INFECT(klass, module);
     OBJ_INFECT(klass, super);
 
@@ -678,13 +635,13 @@ rb_include_module(VALUE klass, VALUE module)
     while (module) {
 	int superclass_seen = FALSE;
 
-	if (RCLASS_M_TBL(klass) == RCLASS_M_TBL(module))
+	if (RCLASS_EXT(klass) == RCLASS_EXT(module))
 	    rb_raise(rb_eArgError, "cyclic include detected");
 	/* ignore if the module included already in superclasses */
 	for (p = RCLASS_SUPER(klass); p; p = RCLASS_SUPER(p)) {
 	    switch (BUILTIN_TYPE(p)) {
 	      case T_ICLASS:
-		if (RCLASS_M_TBL(p) == RCLASS_M_TBL(module)) {
+		if (RCLASS_EXT(p) == RCLASS_EXT(module)) {
 		    if (!superclass_seen) {
 			c = p;  /* move insertion point */
 		    }
@@ -697,7 +654,7 @@ rb_include_module(VALUE klass, VALUE module)
 	    }
 	}
 	c = RCLASS_SUPER(c) = include_class_new(module, RCLASS_SUPER(c));
-	if (RMODULE_M_TBL(module) && RMODULE_M_TBL(module)->num_entries)
+	if (RMODULE_M_TBL(module)->num_entries)
 	    changed = 1;
 	if (RMODULE_CONST_TBL(module) && RMODULE_CONST_TBL(module)->num_entries)
 	    changed = 1;
@@ -830,58 +787,58 @@ ins_methods_push(ID name, long type, VALUE ary, long visi)
 }
 
 static int
-ins_methods_i(st_data_t name, st_data_t type, st_data_t ary)
+ins_methods_i(sa_index_t name, st_data_t type, st_data_t ary)
 {
     return ins_methods_push((ID)name, (long)type, (VALUE)ary, -1); /* everything but private */
 }
 
 static int
-ins_methods_prot_i(st_data_t name, st_data_t type, st_data_t ary)
+ins_methods_prot_i(sa_index_t name, st_data_t type, st_data_t ary)
 {
     return ins_methods_push((ID)name, (long)type, (VALUE)ary, NOEX_PROTECTED);
 }
 
 static int
-ins_methods_priv_i(st_data_t name, st_data_t type, st_data_t ary)
+ins_methods_priv_i(sa_index_t name, st_data_t type, st_data_t ary)
 {
     return ins_methods_push((ID)name, (long)type, (VALUE)ary, NOEX_PRIVATE);
 }
 
 static int
-ins_methods_pub_i(st_data_t name, st_data_t type, st_data_t ary)
+ins_methods_pub_i(sa_index_t name, st_data_t type, st_data_t ary)
 {
     return ins_methods_push((ID)name, (long)type, (VALUE)ary, NOEX_PUBLIC);
 }
 
 static int
-method_entry_i(st_data_t key, st_data_t value, st_data_t data)
+method_entry_i(sa_index_t key, st_data_t value, st_data_t data)
 {
     const rb_method_entry_t *me = (const rb_method_entry_t *)value;
-    st_table *list = (st_table *)data;
+    sa_table *list = (sa_table *)data;
     long type;
 
     if ((ID)key == ID_ALLOCATOR) {
 	return ST_CONTINUE;
     }
 
-    if (!st_lookup(list, key, 0)) {
+    if (!sa_lookup(list, key, 0)) {
 	if (UNDEFINED_METHOD_ENTRY_P(me)) {
 	    type = -1; /* none */
 	}
 	else {
 	    type = VISI(me->flag);
 	}
-	st_add_direct(list, key, type);
+	sa_insert(list, key, type);
     }
     return ST_CONTINUE;
 }
 
 static VALUE
-class_instance_method_list(int argc, VALUE *argv, VALUE mod, int obj, int (*func) (st_data_t, st_data_t, st_data_t))
+class_instance_method_list(int argc, VALUE *argv, VALUE mod, int obj, int (*func) (sa_index_t, st_data_t, st_data_t))
 {
     VALUE ary;
     int recur;
-    st_table *list;
+    sa_table list = SA_EMPTY_TABLE;
 
     if (argc == 0) {
 	recur = TRUE;
@@ -892,16 +849,15 @@ class_instance_method_list(int argc, VALUE *argv, VALUE mod, int obj, int (*func
 	recur = RTEST(r);
     }
 
-    list = st_init_numtable();
     for (; mod; mod = RCLASS_SUPER(mod)) {
-	st_foreach(RCLASS_M_TBL(mod), method_entry_i, (st_data_t)list);
+	sa_foreach(RCLASS_M_TBL(mod), method_entry_i, (st_data_t)&list);
 	if (BUILTIN_TYPE(mod) == T_ICLASS) continue;
 	if (obj && FL_TEST(mod, FL_SINGLETON)) continue;
 	if (!recur) break;
     }
     ary = rb_ary_new();
-    st_foreach(list, func, ary);
-    st_free_table(list);
+    sa_foreach(&list, func, ary);
+    sa_clear(&list);
 
     return ary;
 }
@@ -1115,7 +1071,7 @@ VALUE
 rb_obj_singleton_methods(int argc, VALUE *argv, VALUE obj)
 {
     VALUE recur, ary, klass;
-    st_table *list;
+    sa_table list = SA_EMPTY_TABLE;
 
     if (argc == 0) {
 	recur = Qtrue;
@@ -1124,20 +1080,19 @@ rb_obj_singleton_methods(int argc, VALUE *argv, VALUE obj)
 	rb_scan_args(argc, argv, "01", &recur);
     }
     klass = CLASS_OF(obj);
-    list = st_init_numtable();
     if (klass && FL_TEST(klass, FL_SINGLETON)) {
-	st_foreach(RCLASS_M_TBL(klass), method_entry_i, (st_data_t)list);
+	sa_foreach(RCLASS_M_TBL(klass), method_entry_i, (st_data_t)&list);
 	klass = RCLASS_SUPER(klass);
     }
     if (RTEST(recur)) {
 	while (klass && (FL_TEST(klass, FL_SINGLETON) || TYPE(klass) == T_ICLASS)) {
-	    st_foreach(RCLASS_M_TBL(klass), method_entry_i, (st_data_t)list);
+	    sa_foreach(RCLASS_M_TBL(klass), method_entry_i, (st_data_t)&list);
 	    klass = RCLASS_SUPER(klass);
 	}
     }
     ary = rb_ary_new();
-    st_foreach(list, ins_methods_i, ary);
-    st_free_table(list);
+    sa_foreach(&list, ins_methods_i, ary);
+    sa_clear(&list);
 
     return ary;
 }
diff --git a/common.mk b/common.mk
index 1d8e442..0d767ab 100644
--- a/common.mk
+++ b/common.mk
@@ -79,6 +79,7 @@ COMMONOBJS    = array.$(OBJEXT) \
 		safe.$(OBJEXT) \
 		signal.$(OBJEXT) \
 		sprintf.$(OBJEXT) \
+		sp_ar.$(OBJEXT) \
 		st.$(OBJEXT) \
 		strftime.$(OBJEXT) \
 		string.$(OBJEXT) \
@@ -703,6 +704,7 @@ signal.$(OBJEXT): {$(VPATH)}signal.c $(RUBY_H_INCLUDES) \
   $(VM_CORE_H_INCLUDES) {$(VPATH)}debug.h
 sprintf.$(OBJEXT): {$(VPATH)}sprintf.c $(RUBY_H_INCLUDES) {$(VPATH)}re.h \
   {$(VPATH)}regex.h {$(VPATH)}vsnprintf.c $(ENCODING_H_INCLUDES)
+sp_ar.$(OBJEXT): {$(VPATH)}sp_ar.c $(RUBY_H_INCLUDES)
 st.$(OBJEXT): {$(VPATH)}st.c $(RUBY_H_INCLUDES) {$(VPATH)}pool_alloc.h
 strftime.$(OBJEXT): {$(VPATH)}strftime.c $(RUBY_H_INCLUDES) \
   {$(VPATH)}timev.h
diff --git a/constant.h b/constant.h
index 8232910..9106847 100644
--- a/constant.h
+++ b/constant.h
@@ -23,7 +23,7 @@ typedef struct rb_const_entry_struct {
 
 VALUE rb_mod_private_constant(int argc, VALUE *argv, VALUE obj);
 VALUE rb_mod_public_constant(int argc, VALUE *argv, VALUE obj);
-void rb_free_const_table(st_table *tbl);
+void rb_free_const_table(sa_table *tbl);
 VALUE rb_public_const_get(VALUE klass, ID id);
 VALUE rb_public_const_get_at(VALUE klass, ID id);
 VALUE rb_public_const_get_from(VALUE klass, ID id);
diff --git a/ext/objspace/objspace.c b/ext/objspace/objspace.c
index fd08158..1e2ee5f 100644
--- a/ext/objspace/objspace.c
+++ b/ext/objspace/objspace.c
@@ -60,20 +60,10 @@ memsize_of(VALUE obj)
 	break;
       case T_MODULE:
       case T_CLASS:
-	size += st_memsize(RCLASS_M_TBL(obj));
-	if (RCLASS_IV_TBL(obj)) {
-	    size += st_memsize(RCLASS_IV_TBL(obj));
-	}
-	if (RCLASS_IV_INDEX_TBL(obj)) {
-	    size += st_memsize(RCLASS_IV_INDEX_TBL(obj));
-	}
-	if (RCLASS(obj)->ptr->iv_tbl) {
-	    size += st_memsize(RCLASS(obj)->ptr->iv_tbl);
-	}
-	if (RCLASS(obj)->ptr->const_tbl) {
-	    size += st_memsize(RCLASS(obj)->ptr->const_tbl);
-	}
-	size += sizeof(rb_classext_t);
+	size += sa_memsize(RCLASS_M_TBL(obj));
+        size += sa_memsize(RCLASS_IV_TBL(obj));
+        size += sa_memsize(RCLASS_IV_INDEX_TBL(obj));
+        size += sa_memsize(RCLASS_CONST_TBL(obj));
 	break;
       case T_STRING:
 	size += rb_str_memsize(obj);
diff --git a/gc.c b/gc.c
index 3110059..56f0217 100644
--- a/gc.c
+++ b/gc.c
@@ -688,7 +688,7 @@ rb_objspace_free(rb_objspace_t *objspace)
 
 #define HEAP_OBJ_LIMIT (unsigned int)(HEAP_SIZE / sizeof(struct RVALUE))
 
-extern st_table *rb_class_tbl;
+extern sa_table rb_class_tbl;
 
 int ruby_disable_gc_stress = 0;
 
@@ -2358,6 +2358,15 @@ mark_tbl(rb_objspace_t *objspace, st_table *tbl)
     st_foreach(tbl, mark_entry, (st_data_t)&arg);
 }
 
+static void
+mark_sa_tbl(rb_objspace_t *objspace, sa_table *tbl)
+{
+    if (!tbl) return;
+    SA_FOREACH_START(tbl);
+    gc_mark(objspace, (VALUE)value);
+    SA_FOREACH_END();
+}
+
 static int
 mark_key(VALUE key, VALUE value, st_data_t data)
 {
@@ -2434,74 +2443,52 @@ rb_mark_method_entry(const rb_method_entry_t *me)
     mark_method_entry(&rb_objspace, me);
 }
 
-static int
-mark_method_entry_i(ID key, const rb_method_entry_t *me, st_data_t data)
-{
-    struct mark_tbl_arg *arg = (void*)data;
-    mark_method_entry(arg->objspace, me);
-    return ST_CONTINUE;
-}
-
 static void
-mark_m_tbl(rb_objspace_t *objspace, st_table *tbl)
-{
-    struct mark_tbl_arg arg;
-    if (!tbl) return;
-    arg.objspace = objspace;
-    st_foreach(tbl, mark_method_entry_i, (st_data_t)&arg);
-}
-
-static int
-free_method_entry_i(ID key, rb_method_entry_t *me, st_data_t data)
+mark_m_tbl(rb_objspace_t *objspace, sa_table *tbl)
 {
-    if (!me->mark) {
-	rb_free_method_entry(me);
-    }
-    return ST_CONTINUE;
+    SA_FOREACH_START(tbl);
+    mark_method_entry(objspace, (const rb_method_entry_t*)value);
+    SA_FOREACH_END();
 }
 
 void
-rb_free_m_table(st_table *tbl)
+rb_free_m_table(sa_table *tbl)
 {
-    st_foreach(tbl, free_method_entry_i, 0);
-    st_free_table(tbl);
-}
-
-static int
-mark_const_entry_i(ID key, const rb_const_entry_t *ce, st_data_t data)
-{
-    struct mark_tbl_arg *arg = (void*)data;
-    gc_mark(arg->objspace, ce->value);
-    return ST_CONTINUE;
+    SA_FOREACH_START(tbl);
+    if (!((rb_method_entry_t*)value)->mark) {
+	rb_free_method_entry((rb_method_entry_t*)value);
+    }
+    SA_FOREACH_END();
+    sa_clear(tbl);
 }
 
 static void
-mark_const_tbl(rb_objspace_t *objspace, st_table *tbl)
+mark_const_tbl(rb_objspace_t *objspace, sa_table *tbl)
 {
-    struct mark_tbl_arg arg;
-    if (!tbl) return;
-    arg.objspace = objspace;
-    st_foreach(tbl, mark_const_entry_i, (st_data_t)&arg);
+    SA_FOREACH_START(tbl);
+    gc_mark(objspace, ((const rb_const_entry_t*)value)->value);
+    SA_FOREACH_END();
 }
 
-static int
-free_const_entry_i(ID key, rb_const_entry_t *ce, st_data_t data)
+void
+rb_free_const_table(sa_table *tbl)
 {
-    xfree(ce);
-    return ST_CONTINUE;
+    SA_FOREACH_START(tbl);
+    xfree((rb_const_entry_t*)value);
+    SA_FOREACH_END();
+    sa_clear(tbl);
 }
 
 void
-rb_free_const_table(st_table *tbl)
+rb_mark_tbl(st_table *tbl)
 {
-    st_foreach(tbl, free_const_entry_i, 0);
-    st_free_table(tbl);
+    mark_tbl(&rb_objspace, tbl);
 }
 
 void
-rb_mark_tbl(st_table *tbl)
+rb_mark_sa_tbl(sa_table *tbl)
 {
-    mark_tbl(&rb_objspace, tbl);
+    mark_sa_tbl(&rb_objspace, tbl);
 }
 
 void
@@ -2709,7 +2696,7 @@ gc_mark_children(rb_objspace_t *objspace, VALUE ptr)
       case T_CLASS:
       case T_MODULE:
 	mark_m_tbl(objspace, RCLASS_M_TBL(obj));
-	mark_tbl(objspace, RCLASS_IV_TBL(obj));
+	mark_sa_tbl(objspace, RCLASS_IV_TBL(obj));
 	mark_const_tbl(objspace, RCLASS_CONST_TBL(obj));
 	ptr = RCLASS_SUPER(obj);
 	goto again;
@@ -3284,15 +3271,9 @@ obj_free(rb_objspace_t *objspace, VALUE obj)
       case T_CLASS:
 	rb_clear_cache_by_class((VALUE)obj);
 	rb_free_m_table(RCLASS_M_TBL(obj));
-	if (RCLASS_IV_TBL(obj)) {
-	    st_free_table(RCLASS_IV_TBL(obj));
-	}
-	if (RCLASS_CONST_TBL(obj)) {
-	    rb_free_const_table(RCLASS_CONST_TBL(obj));
-	}
-	if (RCLASS_IV_INDEX_TBL(obj)) {
-	    st_free_table(RCLASS_IV_INDEX_TBL(obj));
-	}
+        sa_clear(RCLASS_IV_TBL(obj));
+        rb_free_const_table(RCLASS_CONST_TBL(obj));
+        sa_clear(RCLASS_IV_INDEX_TBL(obj));
         xfree(RANY(obj)->as.klass.ptr);
 	break;
       case T_STRING:
@@ -3345,7 +3326,6 @@ obj_free(rb_objspace_t *objspace, VALUE obj)
 	break;
       case T_ICLASS:
 	/* iClass shares table with the module */
-	xfree(RANY(obj)->as.klass.ptr);
 	break;
 
       case T_FLOAT:
@@ -3460,7 +3440,7 @@ gc_marks(rb_objspace_t *objspace)
     rb_mark_end_proc();
     rb_gc_mark_global_tbl();
 
-    mark_tbl(objspace, rb_class_tbl);
+    mark_sa_tbl(objspace, &rb_class_tbl);
 
     /* mark generic instance variables for special constants */
     rb_mark_generic_ivar_tbl();
diff --git a/include/ruby/intern.h b/include/ruby/intern.h
index 906661e..8bc790d 100644
--- a/include/ruby/intern.h
+++ b/include/ruby/intern.h
@@ -424,6 +424,7 @@ size_t ruby_stack_length(VALUE**);
 int rb_during_gc(void);
 void rb_gc_mark_locations(VALUE*, VALUE*);
 void rb_mark_tbl(struct st_table*);
+void rb_mark_sa_tbl(sa_table*);
 void rb_mark_set(struct st_table*);
 void rb_mark_hash(struct st_table*);
 void rb_gc_mark_maybe(VALUE);
@@ -857,7 +858,7 @@ VALUE rb_f_trace_var(int, VALUE*);
 VALUE rb_f_untrace_var(int, VALUE*);
 VALUE rb_f_global_variables(void);
 void rb_alias_variable(ID, ID);
-struct st_table* rb_generic_ivar_table(VALUE);
+sa_table* rb_generic_ivar_table(VALUE);
 void rb_copy_generic_ivar(VALUE,VALUE);
 void rb_mark_generic_ivar(VALUE);
 void rb_mark_generic_ivar_tbl(void);
diff --git a/include/ruby/ruby.h b/include/ruby/ruby.h
index 453a603..2625ea3 100644
--- a/include/ruby/ruby.h
+++ b/include/ruby/ruby.h
@@ -605,7 +605,7 @@ struct RObject {
 	struct {
 	    long numiv;
 	    VALUE *ivptr;
-            struct st_table *iv_index_tbl; /* shortcut for RCLASS_IV_INDEX_TBL(rb_obj_class(obj)) */
+            struct sa_table *iv_index_tbl; /* shortcut for RCLASS_IV_INDEX_TBL(rb_obj_class(obj)) */
 	} heap;
 	VALUE ary[ROBJECT_EMBED_LEN_MAX];
     } as;
@@ -629,9 +629,8 @@ typedef struct rb_classext_struct rb_classext_t;
 
 struct RClass {
     struct RBasic basic;
+    VALUE super;
     rb_classext_t *ptr;
-    struct st_table *m_tbl;
-    struct st_table *iv_index_tbl;
 };
 #define RCLASS_SUPER(c) rb_class_get_superclass(c)
 #define RMODULE_IV_TBL(m) RCLASS_IV_TBL(m)
diff --git a/include/ruby/st.h b/include/ruby/st.h
index 94646b6..0738cfd 100644
--- a/include/ruby/st.h
+++ b/include/ruby/st.h
@@ -152,6 +152,51 @@ st_index_t st_hash_start(st_index_t h);
 #pragma GCC visibility pop
 #endif
 
+typedef unsigned int sa_index_t;
+#define SA_STOP     ST_STOP
+#define SA_CONTINUE ST_CONTINUE
+
+#define SA_EMPTY   0
+
+typedef struct sa_entry {
+    sa_index_t next;
+    sa_index_t key;
+    st_data_t value;
+} sa_entry;
+
+typedef struct sa_table {
+    sa_index_t num_bins;
+    sa_index_t num_entries;
+    sa_index_t free_pos;
+    sa_entry *entries;
+} sa_table;
+
+#define SA_EMPTY_TABLE {0, 0, 0, 0};
+void sa_init_table(sa_table *, sa_index_t);
+sa_table *sa_new_table();
+int  sa_insert(sa_table *, sa_index_t, st_data_t);
+int  sa_lookup(sa_table *, sa_index_t, st_data_t *);
+int  sa_delete(sa_table *, sa_index_t, st_data_t *);
+void sa_clear(sa_table *);
+void sa_clear_no_free(sa_table *);
+void sa_free_table(sa_table *);
+int  sa_foreach(sa_table *, int (*)(ANYARGS), st_data_t);
+size_t sa_memsize(const sa_table *);
+sa_table *sa_copy(sa_table*);
+void sa_copy_to(sa_table*, sa_table*);
+typedef int (*sa_iter_func)(sa_index_t key, st_data_t val, st_data_t arg);
+
+#define SA_FOREACH_START_I(table, entry) do { \
+    sa_table *T##entry = (table); \
+    sa_index_t K##entry; \
+    for(K##entry = 0; K##entry < T##entry->num_bins; K##entry++) { \
+	sa_entry *entry = T##entry->entries + K##entry; \
+	if (entry->next != SA_EMPTY) { \
+	    st_data_t value = entry->value
+#define SA_FOREACH_END() } } } while(0)
+
+#define SA_FOREACH_START(table) SA_FOREACH_START_I(table, entry)
+
 #if defined(__cplusplus)
 #if 0
 { /* satisfy cc-mode */
diff --git a/internal.h b/internal.h
index f5af903..5927d93 100644
--- a/internal.h
+++ b/internal.h
@@ -24,18 +24,19 @@ struct rb_deprecated_classext_struct {
 };
 
 struct rb_classext_struct {
-    VALUE super;
-    struct st_table *iv_tbl;
-    struct st_table *const_tbl;
+    sa_table m_tbl;
+    sa_table iv_tbl;
+    sa_table const_tbl;
+    sa_table iv_index_tbl;
 };
 
 #undef RCLASS_SUPER
 #define RCLASS_EXT(c) (RCLASS(c)->ptr)
-#define RCLASS_SUPER(c) (RCLASS_EXT(c)->super)
-#define RCLASS_IV_TBL(c) (RCLASS_EXT(c)->iv_tbl)
-#define RCLASS_CONST_TBL(c) (RCLASS_EXT(c)->const_tbl)
-#define RCLASS_M_TBL(c) (RCLASS(c)->m_tbl)
-#define RCLASS_IV_INDEX_TBL(c) (RCLASS(c)->iv_index_tbl)
+#define RCLASS_SUPER(c) (RCLASS(c)->super)
+#define RCLASS_IV_TBL(c) (&RCLASS_EXT(c)->iv_tbl)
+#define RCLASS_CONST_TBL(c) (&RCLASS_EXT(c)->const_tbl)
+#define RCLASS_M_TBL(c) (&RCLASS_EXT(c)->m_tbl)
+#define RCLASS_IV_INDEX_TBL(c) (&RCLASS_EXT(c)->iv_index_tbl)
 
 struct vtm; /* defined by timev.h */
 
diff --git a/marshal.c b/marshal.c
index 6ae3a55..48d9d9b 100644
--- a/marshal.c
+++ b/marshal.c
@@ -506,7 +506,7 @@ w_uclass(VALUE obj, VALUE super, struct dump_arg *arg)
 }
 
 static int
-w_obj_each(ID id, VALUE value, struct dump_call_arg *arg)
+w_obj_each(sa_index_t id, VALUE value, struct dump_call_arg *arg)
 {
     if (id == rb_id_encoding()) return ST_CONTINUE;
     if (id == rb_intern("E")) return ST_CONTINUE;
@@ -553,13 +553,13 @@ w_encoding(VALUE obj, long num, struct dump_call_arg *arg)
 }
 
 static void
-w_ivar(VALUE obj, st_table *tbl, struct dump_call_arg *arg)
+w_ivar(VALUE obj, sa_table *tbl, struct dump_call_arg *arg)
 {
     long num = tbl ? tbl->num_entries : 0;
 
     w_encoding(obj, num, arg);
     if (tbl) {
-	st_foreach_safe(tbl, w_obj_each, (st_data_t)arg);
+	sa_foreach(tbl, w_obj_each, (st_data_t)arg);
     }
 }
 
@@ -586,7 +586,7 @@ static void
 w_object(VALUE obj, struct dump_arg *arg, int limit)
 {
     struct dump_call_arg c_arg;
-    st_table *ivtbl = 0;
+    sa_table *ivtbl = 0;
     st_data_t num;
     int hasiv = 0;
 #define has_ivars(obj, ivtbl) (((ivtbl) = rb_generic_ivar_table(obj)) != 0 || \
@@ -651,7 +651,7 @@ w_object(VALUE obj, struct dump_arg *arg, int limit)
 	}
 	if (rb_respond_to(obj, s_dump)) {
 	    VALUE v;
-            st_table *ivtbl2 = 0;
+            sa_table *ivtbl2 = 0;
             int hasiv2;
 
 	    v = rb_funcall(obj, s_dump, 1, INT2NUM(limit));
diff --git a/method.h b/method.h
index 9229896..2fecd57 100644
--- a/method.h
+++ b/method.h
@@ -100,6 +100,6 @@ int rb_method_entry_eq(const rb_method_entry_t *m1, const rb_method_entry_t *m2)
 void rb_mark_method_entry(const rb_method_entry_t *me);
 void rb_free_method_entry(rb_method_entry_t *me);
 void rb_sweep_method_entry(void *vm);
-void rb_free_m_table(st_table *tbl);
+void rb_free_m_table(sa_table *tbl);
 
 #endif /* METHOD_H */
diff --git a/object.c b/object.c
index 59611fd..4522d08 100644
--- a/object.c
+++ b/object.c
@@ -236,17 +236,8 @@ init_copy(VALUE dest, VALUE obj)
         break;
       case T_CLASS:
       case T_MODULE:
-	if (RCLASS_IV_TBL(dest)) {
-	    st_free_table(RCLASS_IV_TBL(dest));
-	    RCLASS_IV_TBL(dest) = 0;
-	}
-	if (RCLASS_CONST_TBL(dest)) {
-	    rb_free_const_table(RCLASS_CONST_TBL(dest));
-	    RCLASS_CONST_TBL(dest) = 0;
-	}
-	if (RCLASS_IV_TBL(obj)) {
-	    RCLASS_IV_TBL(dest) = st_copy(RCLASS_IV_TBL(obj));
-	}
+        rb_free_const_table(RCLASS_CONST_TBL(dest));
+        sa_copy_to(RCLASS_IV_TBL(obj), RCLASS_IV_TBL(dest));
         break;
     }
 }
@@ -537,7 +528,7 @@ rb_obj_is_kind_of(VALUE obj, VALUE c)
     }
 
     while (cl) {
-	if (cl == c || RCLASS_M_TBL(cl) == RCLASS_M_TBL(c))
+	if (cl == c || RCLASS_EXT(cl) == RCLASS_EXT(c))
 	    return Qtrue;
 	cl = RCLASS_SUPER(cl);
     }
@@ -1362,13 +1353,13 @@ rb_class_inherited_p(VALUE mod, VALUE arg)
 	rb_raise(rb_eTypeError, "compared with non class/module");
     }
     while (mod) {
-	if (RCLASS_M_TBL(mod) == RCLASS_M_TBL(arg))
+	if (RCLASS_EXT(mod) == RCLASS_EXT(arg))
 	    return Qtrue;
 	mod = RCLASS_SUPER(mod);
     }
     /* not mod < arg; check if mod > arg */
     while (arg) {
-	if (RCLASS_M_TBL(arg) == RCLASS_M_TBL(start))
+	if (RCLASS_EXT(arg) == RCLASS_EXT(start))
 	    return Qfalse;
 	arg = RCLASS_SUPER(arg);
     }
diff --git a/sp_ar.c b/sp_ar.c
new file mode 100644
index 0000000..2ed69bf
--- /dev/null
+++ b/sp_ar.c
@@ -0,0 +1,374 @@
+/*
+ * sparse array lib
+ * inspired by Lua table
+ * written by Sokolov Yura aka funny_falcon
+ */
+#ifdef NOT_RUBY
+#include "regint.h"
+#include "st.h"
+#else
+#include "ruby/ruby.h"
+#endif
+
+#include <stdio.h>
+#ifdef HAVE_STDLIB_H
+#include <stdlib.h>
+#endif
+#include <string.h>
+
+#ifdef RUBY
+#define malloc xmalloc
+#define calloc xcalloc
+#define realloc xrealloc
+#define free   xfree
+#endif
+
+#define sa_table_alloc()          (sa_table*)malloc(sizeof(sa_table))
+#define sa_table_xalloc()         (sa_table*)calloc(1, sizeof(sa_table))
+#define sa_table_dealloc(table)   free(table)
+#define sa_entry_alloc(n)         (sa_entry*)calloc((n), sizeof(sa_entry))
+#define sa_entry_dealloc(entries) free(entries)
+
+#define SA_LAST   1
+#define SA_OFFSET 2
+
+#define SA_MIN_SIZE 4
+
+void
+sa_init_table(register sa_table *table, sa_index_t num_bins)
+{
+    if (num_bins) {
+        table->num_entries = 0;
+        table->entries = sa_entry_alloc(num_bins);
+        table->num_bins = num_bins;
+        table->free_pos = num_bins;
+    }
+    else {
+        memset(table, 0, sizeof(sa_table));
+    }
+}
+
+sa_table*
+sa_new_table()
+{
+    sa_table* table = sa_table_alloc();
+    sa_init_table(table, 0);
+    return table;
+}
+
+static inline sa_index_t
+calc_pos(register sa_table* table, sa_index_t key)
+{
+    /* this formula is empirical */
+    /* it has no good avalance, but works well in our case */
+    key ^= key >> 16;
+    key *= 0x445229;
+    return (key + (key >> 16)) % table->num_bins;
+}
+
+static void
+fix_empty(register sa_table* table)
+{
+    while(--table->free_pos &&
+            table->entries[table->free_pos-1].next != SA_EMPTY);
+}
+
+#define FLOOR_TO_4 ((~((sa_index_t)0)) << 2)
+static sa_index_t
+find_empty(register sa_table* table, register sa_index_t pos)
+{
+    sa_index_t new_pos = table->free_pos-1;
+    sa_entry *entry;
+    pos &= FLOOR_TO_4;
+    entry = table->entries+pos;
+
+    if (entry->next == SA_EMPTY) { new_pos = pos; goto check; }
+    pos++; entry++;
+    if (entry->next == SA_EMPTY) { new_pos = pos; goto check; }
+    pos++; entry++;
+    if (entry->next == SA_EMPTY) { new_pos = pos; goto check; }
+    pos++; entry++;
+    if (entry->next == SA_EMPTY) { new_pos = pos; goto check; }
+
+check:
+    if (new_pos+1 == table->free_pos) fix_empty(table);
+    return new_pos;
+}
+
+static void resize(register sa_table* table);
+static int insert_into_chain(register sa_table*, register sa_index_t, st_data_t, sa_index_t pos);
+static int insert_into_main(register sa_table*, sa_index_t, st_data_t, sa_index_t pos, sa_index_t prev_pos);
+
+int
+sa_insert(register sa_table* table, register sa_index_t key, st_data_t value)
+{
+    sa_index_t pos, main_pos;
+    register sa_entry *entry;
+
+    if (table->num_bins == 0) {
+        sa_init_table(table, SA_MIN_SIZE);
+    }
+
+    pos = calc_pos(table, key);
+    entry = table->entries + pos;
+
+    if (entry->next == SA_EMPTY) {
+        entry->next = SA_LAST;
+        entry->key = key;
+        entry->value = value;
+        table->num_entries++;
+        if (pos+1 == table->free_pos) fix_empty(table);
+        return 0;
+    }
+
+    if (entry->key == key) {
+        entry->value = value;
+        return 1;
+    }
+
+    if (table->num_entries + (table->num_entries >> 2) > table->num_bins) {
+        resize(table);
+        return sa_insert(table, key, value);
+    }
+
+    main_pos = calc_pos(table, entry->key);
+    if (main_pos == pos) {
+        return insert_into_chain(table, key, value, pos);
+    }
+    else {
+        if (!table->free_pos) {
+            resize(table);
+            return sa_insert(table, key, value);
+        }
+        return insert_into_main(table, key, value, pos, main_pos);
+    }
+}
+
+static int
+insert_into_chain(register sa_table* table, register sa_index_t key, st_data_t value, sa_index_t pos)
+{
+    sa_entry *entry = table->entries + pos, *new_entry;
+    sa_index_t new_pos;
+
+    while (entry->next != SA_LAST) {
+        pos = entry->next - SA_OFFSET;
+        entry = table->entries + pos;
+        if (entry->key == key) {
+            entry->value = value;
+            return 1;
+        }
+    }
+
+    if (!table->free_pos) {
+        resize(table);
+        return sa_insert(table, key, value);
+    }
+
+    new_pos = find_empty(table, pos);
+    new_entry = table->entries + new_pos;
+    entry->next = new_pos + SA_OFFSET;
+
+    new_entry->next = SA_LAST;
+    new_entry->key = key;
+    new_entry->value = value;
+    table->num_entries++;
+    return 0;
+}
+
+static int
+insert_into_main(register sa_table* table, sa_index_t key, st_data_t value, sa_index_t pos, sa_index_t prev_pos)
+{
+    sa_entry *entry = table->entries + pos;
+    sa_index_t new_pos = find_empty(table, pos);
+    sa_entry *new_entry = table->entries + new_pos;
+    sa_index_t npos;
+
+    *new_entry = *entry;
+
+    while((npos = table->entries[prev_pos].next - SA_OFFSET) != pos) {
+        prev_pos = npos;
+    }
+    table->entries[prev_pos].next = new_pos + SA_OFFSET;
+
+    entry->next = SA_LAST;
+    entry->key = key;
+    entry->value = value;
+    table->num_entries++;
+    return 0;
+}
+
+static sa_index_t
+new_size(sa_index_t num_entries)
+{
+    sa_index_t msb = num_entries;
+    msb |= msb >> 1;
+    msb |= msb >> 2;
+    msb |= msb >> 4;
+    msb |= msb >> 8;
+    msb |= msb >> 16;
+    msb = ((msb >> 4) + 1) << 3;
+    return (num_entries & (msb | (msb >> 1))) + (msb >> 1);
+}
+
+static void
+resize(register sa_table *table)
+{
+    sa_table tmp_table;
+    sa_entry *entry;
+    sa_index_t i;
+
+    if (table->num_entries == 0) {
+        sa_entry_dealloc(table->entries);
+        memset(table, 0, sizeof(sa_table));
+        return;
+    }
+
+    sa_init_table(&tmp_table, new_size(table->num_entries + (table->num_entries >> 2)));
+    entry = table->entries;
+
+    for(i = 0; i < table->num_bins; i++, entry++) {
+        if (entry->next != SA_EMPTY) {
+            sa_insert(&tmp_table, entry->key, entry->value);
+        }
+    }
+    sa_entry_dealloc(table->entries);
+    *table = tmp_table;
+}
+
+int
+sa_lookup(register sa_table *table, register sa_index_t key, st_data_t *value)
+{
+    register sa_entry *entry;
+
+    if (table->num_entries == 0) return 0;
+
+    entry = table->entries + calc_pos(table, key);
+    if (entry->next == SA_EMPTY) return 0;
+
+    if (entry->key == key) goto found;
+    if (entry->next == SA_LAST) return 0;
+
+    entry = table->entries + (entry->next - SA_OFFSET);
+    if (entry->key == key) goto found;
+
+    while(entry->next != SA_LAST) {
+        entry = table->entries + (entry->next - SA_OFFSET);
+        if (entry->key == key) goto found;
+    }
+    return 0;
+found:
+    if (value) *value = entry->value;
+    return 1;
+}
+
+void
+sa_clear(sa_table *table)
+{
+    sa_entry_dealloc(table->entries);
+    memset(table, 0, sizeof(sa_table));
+}
+
+void
+sa_clear_no_free(sa_table *table)
+{
+    memset(table->entries, 0, sizeof(sa_entry) * table->num_bins);
+    table->num_entries = 0;
+    table->free_pos = table->num_bins;
+}
+
+void
+sa_free_table(sa_table *table)
+{
+    sa_entry_dealloc(table->entries);
+    sa_table_dealloc(table);
+}
+
+int
+sa_delete(sa_table *table, sa_index_t key, st_data_t *value)
+{
+    sa_index_t pos, prev_pos = ~0;
+    sa_entry *entry;
+
+    if (table->num_entries == 0) goto not_found;
+
+    pos = calc_pos(table, key);
+    entry = table->entries + pos;
+
+    if (entry->next == SA_EMPTY) goto not_found;
+
+    do {
+        if (entry->key == key) {
+            if (value) *value = entry->value;
+            if (entry->next != SA_LAST) {
+                sa_index_t npos = entry->next - SA_OFFSET;
+                *entry = table->entries[npos];
+                memset(table->entries + npos, 0, sizeof(sa_entry));
+            }
+            else {
+                memset(table->entries + pos, 0, sizeof(sa_entry));
+                if (~prev_pos) {
+                    table->entries[prev_pos].next = SA_LAST;
+                }
+            }
+            table->num_entries--;
+            if (table->num_entries < table->num_bins / 4) {
+                resize(table);
+            }
+            return 1;
+        }
+        if (entry->next == SA_LAST) break;
+        prev_pos = pos;
+        pos = entry->next - SA_OFFSET;
+        entry = table->entries + pos;
+    } while(1);
+
+not_found:
+    if (value) *value = 0;
+    return 0;
+}
+
+int
+sa_foreach(register sa_table *table, int (*func)(), st_data_t arg)
+{
+    sa_index_t i;
+    if (table->num_bins == 0) {
+        return 0;
+    }
+    for(i = 0; i < table->num_bins ; i++) {
+	if (table->entries[i].next != SA_EMPTY) {
+	    sa_index_t key = table->entries[i].key;
+	    st_data_t val = table->entries[i].value;
+	    if ((*func)(key, val, arg) == SA_STOP) break;
+	}
+    }
+    return 0;
+}
+
+size_t
+sa_memsize(const sa_table *table)
+{
+    return sizeof(sa_table) + table->num_bins * sizeof(sa_entry);
+}
+
+sa_table*
+sa_copy(sa_table *table)
+{
+    sa_table *new_table = sa_table_alloc();
+    *new_table = *table;
+    if (table->num_bins) {
+        new_table->entries = sa_entry_alloc(table->num_bins);
+        memcpy(new_table->entries, table->entries, table->num_bins*sizeof(sa_entry));
+    }
+    return new_table;
+}
+
+void
+sa_copy_to(sa_table *from, sa_table *to)
+{
+    sa_entry_dealloc(to->entries);
+    *to = *from;
+    if (to->num_bins) {
+	to->entries = sa_entry_alloc(to->num_bins);
+	memcpy(to->entries, from->entries, from->num_bins*sizeof(sa_entry));
+    }
+}
diff --git a/test/ruby/test_marshal.rb b/test/ruby/test_marshal.rb
index 0f3f794..5c2e502 100644
--- a/test/ruby/test_marshal.rb
+++ b/test/ruby/test_marshal.rb
@@ -312,7 +312,7 @@ class TestMarshal < Test::Unit::TestCase
         assert_equal(a.instance_variable_get(i), b.instance_variable_get(i), bug1932)
       end
     end
-    a.__send__(a.methods(true).grep(/=\z/)[0], a)
+    a.__send__(a.methods(true).grep(/r.*=\z/)[0], a)
     assert_nothing_raised(bug1932) do
       b = Marshal.load(Marshal.dump(a))
       assert_equal(ClassISO8859_1, b.class, bug1932)
diff --git a/test/ruby/test_method.rb b/test/ruby/test_method.rb
index ea6c5f2..6ccd37c 100644
--- a/test/ruby/test_method.rb
+++ b/test/ruby/test_method.rb
@@ -406,7 +406,7 @@ class TestMethod < Test::Unit::TestCase
     obj = a.new
     assert_equal([:a], obj.public_methods(false), bug)
     obj.extend(m)
-    assert_equal([:m1, :a], obj.public_methods(false), bug)
+    assert_equal([:a, :m1], obj.public_methods(false).sort, bug)
   end
 
   def test_visibility
diff --git a/test/ruby/test_module.rb b/test/ruby/test_module.rb
index 6673852..621b784 100644
--- a/test/ruby/test_module.rb
+++ b/test/ruby/test_module.rb
@@ -870,7 +870,7 @@ class TestModule < Test::Unit::TestCase
       (class << self ; self ; end).class_eval do
         define_method :method_added do |sym|
           memo << sym
-          memo << mod.instance_methods(false)
+          memo << mod.instance_methods(false).sort
           memo << (mod.instance_method(sym) rescue nil)
         end
       end
@@ -887,10 +887,10 @@ class TestModule < Test::Unit::TestCase
     assert_equal [:f, :g], memo.shift
     assert_equal mod.instance_method(:f), memo.shift
     assert_equal :a, memo.shift
-    assert_equal [:f, :g, :a], memo.shift
+    assert_equal [:a, :f, :g], memo.shift
     assert_equal mod.instance_method(:a), memo.shift
     assert_equal :a=, memo.shift
-    assert_equal [:f, :g, :a, :a=], memo.shift
+    assert_equal [:a, :a=, :f, :g], memo.shift
     assert_equal mod.instance_method(:a=), memo.shift
   end
 
diff --git a/time.c b/time.c
index 6fd36f0..9146414 100644
--- a/time.c
+++ b/time.c
@@ -4745,16 +4745,14 @@ time_mload(VALUE time, VALUE str)
     long nsec;
     VALUE submicro, nano_num, nano_den, offset;
     wideval_t timew;
-    st_data_t data;
 
     time_modify(time);
 
 #define get_attr(attr, iffound) \
     attr = rb_attr_get(str, id_##attr); \
     if (!NIL_P(attr)) { \
-	data = id_##attr; \
 	iffound; \
-        st_delete(rb_generic_ivar_table(str), &data, 0); \
+        sa_delete(rb_generic_ivar_table(str), (sa_index_t)id_##attr, 0); \
     }
 
     get_attr(nano_num, {});
diff --git a/variable.c b/variable.c
index 3da500e..4a61453 100644
--- a/variable.c
+++ b/variable.c
@@ -19,15 +19,15 @@
 #include "constant.h"
 #include "internal.h"
 
-st_table *rb_global_tbl;
-st_table *rb_class_tbl;
+sa_table rb_global_tbl;
+sa_table rb_class_tbl;
 static ID autoload, classpath, tmp_classpath, classid;
 
 void
 Init_var_tables(void)
 {
-    rb_global_tbl = st_init_numtable();
-    rb_class_tbl = st_init_numtable();
+    sa_init_table(&rb_global_tbl, 0);
+    sa_init_table(&rb_class_tbl, 0);
     CONST_ID(autoload, "__autoload__");
     CONST_ID(classpath, "__classpath__");
     CONST_ID(tmp_classpath, "__tmp_classpath__");
@@ -43,7 +43,7 @@ struct fc_result {
 };
 
 static VALUE
-fc_path(struct fc_result *fc, ID name)
+fc_path(struct fc_result *fc, sa_index_t name)
 {
     VALUE path, tmp;
 
@@ -51,8 +51,7 @@ fc_path(struct fc_result *fc, ID name)
     while (fc) {
 	st_data_t n;
 	if (fc->track == rb_cObject) break;
-	if (RCLASS_IV_TBL(fc->track) &&
-	    st_lookup(RCLASS_IV_TBL(fc->track), (st_data_t)classpath, &n)) {
+	if (sa_lookup(RCLASS_IV_TBL(fc->track), (sa_index_t)classpath, &n)) {
 	    tmp = rb_str_dup((VALUE)n);
 	    rb_str_cat2(tmp, "::");
 	    rb_str_append(tmp, path);
@@ -70,7 +69,7 @@ fc_path(struct fc_result *fc, ID name)
 }
 
 static int
-fc_i(ID key, rb_const_entry_t *ce, struct fc_result *res)
+fc_i(sa_index_t key, rb_const_entry_t *ce, struct fc_result *res)
 {
     VALUE value = ce->value;
     if (!rb_is_const_id(key)) return ST_CONTINUE;
@@ -98,7 +97,7 @@ fc_i(ID key, rb_const_entry_t *ce, struct fc_result *res)
 	    arg.klass = res->klass;
 	    arg.track = value;
 	    arg.prev = res;
-	    st_foreach(RCLASS_CONST_TBL(value), fc_i, (st_data_t)&arg);
+	    sa_foreach(RCLASS_CONST_TBL(value), fc_i, (st_data_t)&arg);
 	    if (arg.path) {
 		res->path = arg.path;
 		return ST_STOP;
@@ -123,18 +122,14 @@ find_class_path(VALUE klass)
     arg.track = rb_cObject;
     arg.prev = 0;
     if (RCLASS_CONST_TBL(rb_cObject)) {
-	st_foreach_safe(RCLASS_CONST_TBL(rb_cObject), fc_i, (st_data_t)&arg);
+	sa_foreach(RCLASS_CONST_TBL(rb_cObject), fc_i, (st_data_t)&arg);
     }
     if (arg.path == 0) {
-	st_foreach_safe(rb_class_tbl, fc_i, (st_data_t)&arg);
+	sa_foreach(&rb_class_tbl, fc_i, (st_data_t)&arg);
     }
     if (arg.path) {
-	st_data_t tmp = tmp_classpath;
-	if (!RCLASS_IV_TBL(klass)) {
-	    RCLASS_IV_TBL(klass) = st_init_numtable();
-	}
-	st_insert(RCLASS_IV_TBL(klass), (st_data_t)classpath, arg.path);
-	st_delete(RCLASS_IV_TBL(klass), &tmp, 0);
+	sa_insert(RCLASS_IV_TBL(klass), (sa_index_t)classpath, arg.path);
+	sa_delete(RCLASS_IV_TBL(klass), (sa_index_t)tmp_classpath, 0);
 	return arg.path;
     }
     return Qnil;
@@ -147,16 +142,15 @@ classname(VALUE klass)
     st_data_t n;
 
     if (!klass) klass = rb_cObject;
-    if (RCLASS_IV_TBL(klass)) {
-	if (!st_lookup(RCLASS_IV_TBL(klass), (st_data_t)classpath, &n)) {
-	    if (!st_lookup(RCLASS_IV_TBL(klass), (st_data_t)classid, &n)) {
+    if (RCLASS_IV_TBL(klass)->num_entries) {
+	if (!sa_lookup(RCLASS_IV_TBL(klass), (sa_index_t)classpath, &n)) {
+	    if (!sa_lookup(RCLASS_IV_TBL(klass), (sa_index_t)classid, &n)) {
 		return find_class_path(klass);
 	    }
 	    path = rb_str_dup(rb_id2str(SYM2ID((VALUE)n)));
 	    OBJ_FREEZE(path);
-	    st_insert(RCLASS_IV_TBL(klass), (st_data_t)classpath, (st_data_t)path);
-	    n = classid;
-	    st_delete(RCLASS_IV_TBL(klass), &n, 0);
+	    sa_insert(RCLASS_IV_TBL(klass), (sa_index_t)classpath, (st_data_t)path);
+	    sa_delete(RCLASS_IV_TBL(klass), (sa_index_t)classid, 0);
 	}
 	else {
 	    path = (VALUE)n;
@@ -192,8 +186,7 @@ rb_class_path(VALUE klass)
     st_data_t n = (st_data_t)path;
 
     if (!NIL_P(path)) return path;
-    if (RCLASS_IV_TBL(klass) && st_lookup(RCLASS_IV_TBL(klass),
-					  (st_data_t)tmp_classpath, &n)) {
+    if (sa_lookup(RCLASS_IV_TBL(klass), (sa_index_t)tmp_classpath, &n)) {
 	return (VALUE)n;
     }
     else {
@@ -364,7 +357,7 @@ rb_global_entry(ID id)
     struct global_entry *entry;
     st_data_t data;
 
-    if (!st_lookup(rb_global_tbl, (st_data_t)id, &data)) {
+    if (!sa_lookup(&rb_global_tbl, (sa_index_t)id, &data)) {
 	struct global_variable *var;
 	entry = ALLOC(struct global_entry);
 	var = ALLOC(struct global_variable);
@@ -378,7 +371,7 @@ rb_global_entry(ID id)
 
 	var->block_trace = 0;
 	var->trace = 0;
-	st_add_direct(rb_global_tbl, id, (st_data_t)entry);
+	sa_insert(&rb_global_tbl, (sa_index_t)id, (st_data_t)entry);
     }
     else {
 	entry = (struct global_entry *)data;
@@ -454,8 +447,8 @@ readonly_setter(VALUE val, ID id, void *data, struct global_variable *gvar)
     rb_name_error(id, "%s is a read-only variable", rb_id2name(id));
 }
 
-static int
-mark_global_entry(ID key, struct global_entry *entry)
+static void
+mark_global_entry(struct global_entry *entry)
 {
     struct trace_var *trace;
     struct global_variable *var = entry->var;
@@ -466,14 +459,14 @@ mark_global_entry(ID key, struct global_entry *entry)
 	if (trace->data) rb_gc_mark_maybe(trace->data);
 	trace = trace->next;
     }
-    return ST_CONTINUE;
 }
 
 void
 rb_gc_mark_global_tbl(void)
 {
-    if (rb_global_tbl)
-        st_foreach_safe(rb_global_tbl, mark_global_entry, 0);
+    SA_FOREACH_START(&rb_global_tbl);
+    mark_global_entry((struct global_entry*) value);
+    SA_FOREACH_END();
 }
 
 static ID
@@ -635,7 +628,7 @@ rb_f_untrace_var(int argc, VALUE *argv)
     rb_secure(4);
     rb_scan_args(argc, argv, "11", &var, &cmd);
     id = rb_to_id(var);
-    if (!st_lookup(rb_global_tbl, (st_data_t)id, &data)) {
+    if (!sa_lookup(&rb_global_tbl, (sa_index_t)id, &data)) {
 	rb_name_error(id, "undefined global variable %s", rb_id2name(id));
     }
 
@@ -742,13 +735,6 @@ rb_gvar_defined(struct global_entry *entry)
     return Qtrue;
 }
 
-static int
-gvar_i(ID key, struct global_entry *entry, VALUE ary)
-{
-    rb_ary_push(ary, ID2SYM(key));
-    return ST_CONTINUE;
-}
-
 /*
  *  call-seq:
  *     global_variables    -> array
@@ -765,7 +751,9 @@ rb_f_global_variables(void)
     char buf[2];
     int i;
 
-    st_foreach_safe(rb_global_tbl, gvar_i, ary);
+    SA_FOREACH_START(&rb_global_tbl);
+    rb_ary_push(ary, ID2SYM(entry->key));
+    SA_FOREACH_END();
     buf[0] = '$';
     for (i = 1; i <= 9; ++i) {
 	buf[1] = (char)(i + '0');
@@ -784,10 +772,10 @@ rb_alias_variable(ID name1, ID name2)
 	rb_raise(rb_eSecurityError, "Insecure: can't alias global variable");
 
     entry2 = rb_global_entry(name2);
-    if (!st_lookup(rb_global_tbl, (st_data_t)name1, &data1)) {
+    if (!sa_lookup(&rb_global_tbl, (sa_index_t)name1, &data1)) {
 	entry1 = ALLOC(struct global_entry);
 	entry1->id = name1;
-	st_add_direct(rb_global_tbl, name1, (st_data_t)entry1);
+	sa_insert(&rb_global_tbl, (sa_index_t)name1, (st_data_t)entry1);
     }
     else if ((entry1 = (struct global_entry *)data1)->var != entry2->var) {
 	struct global_variable *var = entry1->var;
@@ -815,7 +803,7 @@ rb_alias_variable(ID name1, ID name2)
 static int special_generic_ivar = 0;
 static st_table *generic_iv_tbl;
 
-st_table*
+sa_table*
 rb_generic_ivar_table(VALUE obj)
 {
     st_data_t tbl;
@@ -823,7 +811,7 @@ rb_generic_ivar_table(VALUE obj)
     if (!FL_TEST(obj, FL_EXIVAR)) return 0;
     if (!generic_iv_tbl) return 0;
     if (!st_lookup(generic_iv_tbl, (st_data_t)obj, &tbl)) return 0;
-    return (st_table *)tbl;
+    return (sa_table *)tbl;
 }
 
 static VALUE
@@ -833,7 +821,7 @@ generic_ivar_get(VALUE obj, ID id, int warn)
 
     if (generic_iv_tbl) {
 	if (st_lookup(generic_iv_tbl, (st_data_t)obj, &tbl)) {
-	    if (st_lookup((st_table *)tbl, (st_data_t)id, &val)) {
+	    if (sa_lookup((sa_table *)tbl, (sa_index_t)id, &val)) {
 		return (VALUE)val;
 	    }
 	}
@@ -847,7 +835,6 @@ generic_ivar_get(VALUE obj, ID id, int warn)
 static void
 generic_ivar_set(VALUE obj, ID id, VALUE val)
 {
-    st_table *tbl;
     st_data_t data;
 
     if (rb_special_const_p(obj)) {
@@ -859,24 +846,22 @@ generic_ivar_set(VALUE obj, ID id, VALUE val)
     }
     if (!st_lookup(generic_iv_tbl, (st_data_t)obj, &data)) {
 	FL_SET(obj, FL_EXIVAR);
-	tbl = st_init_numtable();
-	st_add_direct(generic_iv_tbl, (st_data_t)obj, (st_data_t)tbl);
-	st_add_direct(tbl, (st_data_t)id, (st_data_t)val);
-	return;
+	data = (st_data_t)sa_new_table();
+	st_add_direct(generic_iv_tbl, (st_data_t)obj, data);
     }
-    st_insert((st_table *)data, (st_data_t)id, (st_data_t)val);
+    sa_insert((sa_table *)data, (sa_index_t)id, (st_data_t)val);
 }
 
 static VALUE
 generic_ivar_defined(VALUE obj, ID id)
 {
-    st_table *tbl;
+    sa_table *tbl;
     st_data_t data;
 
     if (!generic_iv_tbl) return Qfalse;
     if (!st_lookup(generic_iv_tbl, (st_data_t)obj, &data)) return Qfalse;
-    tbl = (st_table *)data;
-    if (st_lookup(tbl, (st_data_t)id, &data)) {
+    tbl = (sa_table *)data;
+    if (sa_lookup(tbl, (sa_index_t)id, &data)) {
 	return Qtrue;
     }
     return Qfalse;
@@ -885,18 +870,18 @@ generic_ivar_defined(VALUE obj, ID id)
 static int
 generic_ivar_remove(VALUE obj, ID id, st_data_t *valp)
 {
-    st_table *tbl;
+    sa_table *tbl;
     st_data_t data, key = (st_data_t)id;
     int status;
 
     if (!generic_iv_tbl) return 0;
     if (!st_lookup(generic_iv_tbl, (st_data_t)obj, &data)) return 0;
-    tbl = (st_table *)data;
-    status = st_delete(tbl, &key, valp);
+    tbl = (sa_table *)data;
+    status = sa_delete(tbl, (sa_index_t)id, valp);
     if (tbl->num_entries == 0) {
 	key = (st_data_t)obj;
 	st_delete(generic_iv_tbl, &key, &data);
-	st_free_table((st_table *)data);
+	sa_free_table(tbl);
     }
     return status;
 }
@@ -908,22 +893,17 @@ rb_mark_generic_ivar(VALUE obj)
 
     if (!generic_iv_tbl) return;
     if (st_lookup(generic_iv_tbl, (st_data_t)obj, &tbl)) {
-	rb_mark_tbl((st_table *)tbl);
+	rb_mark_sa_tbl((sa_table *)tbl);
     }
 }
 
 static int
-givar_mark_i(ID key, VALUE value)
-{
-    rb_gc_mark(value);
-    return ST_CONTINUE;
-}
-
-static int
-givar_i(VALUE obj, st_table *tbl)
+givar_i(VALUE obj, sa_table *tbl)
 {
     if (rb_special_const_p(obj)) {
-	st_foreach_safe(tbl, givar_mark_i, 0);
+        SA_FOREACH_START(tbl);
+        rb_gc_mark((VALUE)value);
+        SA_FOREACH_END();
     }
     return ST_CONTINUE;
 }
@@ -943,7 +923,7 @@ rb_free_generic_ivar(VALUE obj)
 
     if (!generic_iv_tbl) return;
     if (st_delete(generic_iv_tbl, &key, &tbl))
-	st_free_table((st_table *)tbl);
+	sa_free_table((sa_table *)tbl);
 }
 
 RUBY_FUNC_EXPORTED size_t
@@ -951,7 +931,7 @@ rb_generic_ivar_memsize(VALUE obj)
 {
     st_data_t tbl;
     if (st_lookup(generic_iv_tbl, (st_data_t)obj, &tbl))
-	return st_memsize((st_table *)tbl);
+	return sa_memsize((sa_table *)tbl);
     return 0;
 }
 
@@ -970,17 +950,17 @@ rb_copy_generic_ivar(VALUE clone, VALUE obj)
         return;
     }
     if (st_lookup(generic_iv_tbl, (st_data_t)obj, &data)) {
-	st_table *tbl = (st_table *)data;
+	sa_table *tbl = (sa_table *)data;
 
         if (tbl->num_entries == 0)
             goto clear;
 
 	if (st_lookup(generic_iv_tbl, (st_data_t)clone, &data)) {
-	    st_free_table((st_table *)data);
-	    st_insert(generic_iv_tbl, (st_data_t)clone, (st_data_t)st_copy(tbl));
+	    sa_free_table((sa_table *)data);
+	    st_insert(generic_iv_tbl, (st_data_t)clone, (st_data_t)sa_copy(tbl));
 	}
 	else {
-	    st_add_direct(generic_iv_tbl, (st_data_t)clone, (st_data_t)st_copy(tbl));
+	    st_add_direct(generic_iv_tbl, (st_data_t)clone, (st_data_t)sa_copy(tbl));
 	    FL_SET(clone, FL_EXIVAR);
 	}
     }
@@ -990,7 +970,7 @@ static VALUE
 ivar_get(VALUE obj, ID id, int warn)
 {
     VALUE val, *ptr;
-    struct st_table *iv_index_tbl;
+    sa_table *iv_index_tbl;
     long len;
     st_data_t index;
 
@@ -1000,7 +980,7 @@ ivar_get(VALUE obj, ID id, int warn)
         ptr = ROBJECT_IVPTR(obj);
         iv_index_tbl = ROBJECT_IV_INDEX_TBL(obj);
         if (!iv_index_tbl) break;
-        if (!st_lookup(iv_index_tbl, (st_data_t)id, &index)) break;
+        if (!sa_lookup(iv_index_tbl, (sa_index_t)id, &index)) break;
         if (len <= (long)index) break;
         val = ptr[index];
         if (val != Qundef)
@@ -1008,7 +988,7 @@ ivar_get(VALUE obj, ID id, int warn)
 	break;
       case T_CLASS:
       case T_MODULE:
-	if (RCLASS_IV_TBL(obj) && st_lookup(RCLASS_IV_TBL(obj), (st_data_t)id, &index))
+	if (sa_lookup(RCLASS_IV_TBL(obj), (sa_index_t)id, &index))
 	    return (VALUE)index;
 	break;
       default:
@@ -1037,7 +1017,7 @@ rb_attr_get(VALUE obj, ID id)
 VALUE
 rb_ivar_set(VALUE obj, ID id, VALUE val)
 {
-    struct st_table *iv_index_tbl;
+    struct sa_table *iv_index_tbl;
     st_data_t index;
     long i, len;
     int ivar_extended;
@@ -1051,14 +1031,11 @@ rb_ivar_set(VALUE obj, ID id, VALUE val)
         if (!iv_index_tbl) {
             VALUE klass = rb_obj_class(obj);
             iv_index_tbl = RCLASS_IV_INDEX_TBL(klass);
-            if (!iv_index_tbl) {
-                iv_index_tbl = RCLASS_IV_INDEX_TBL(klass) = st_init_numtable();
-            }
         }
         ivar_extended = 0;
-        if (!st_lookup(iv_index_tbl, (st_data_t)id, &index)) {
+        if (!sa_lookup(iv_index_tbl, (sa_index_t)id, &index)) {
             index = iv_index_tbl->num_entries;
-            st_add_direct(iv_index_tbl, (st_data_t)id, index);
+            sa_insert(iv_index_tbl, (sa_index_t)id, index);
             ivar_extended = 1;
         }
         len = ROBJECT_NUMIV(obj);
@@ -1098,8 +1075,7 @@ rb_ivar_set(VALUE obj, ID id, VALUE val)
 	break;
       case T_CLASS:
       case T_MODULE:
-	if (!RCLASS_IV_TBL(obj)) RCLASS_IV_TBL(obj) = st_init_numtable();
-	st_insert(RCLASS_IV_TBL(obj), (st_data_t)id, val);
+	sa_insert(RCLASS_IV_TBL(obj), (sa_index_t)id, val);
         break;
       default:
 	generic_ivar_set(obj, id, val);
@@ -1112,13 +1088,13 @@ VALUE
 rb_ivar_defined(VALUE obj, ID id)
 {
     VALUE val;
-    struct st_table *iv_index_tbl;
+    struct sa_table *iv_index_tbl;
     st_data_t index;
     switch (TYPE(obj)) {
       case T_OBJECT:
         iv_index_tbl = ROBJECT_IV_INDEX_TBL(obj);
         if (!iv_index_tbl) break;
-        if (!st_lookup(iv_index_tbl, (st_data_t)id, &index)) break;
+        if (!sa_lookup(iv_index_tbl, (sa_index_t)id, &index)) break;
         if (ROBJECT_NUMIV(obj) <= (long)index) break;
         val = ROBJECT_IVPTR(obj)[index];
         if (val != Qundef)
@@ -1126,7 +1102,7 @@ rb_ivar_defined(VALUE obj, ID id)
 	break;
       case T_CLASS:
       case T_MODULE:
-	if (RCLASS_IV_TBL(obj) && st_lookup(RCLASS_IV_TBL(obj), (st_data_t)id, 0))
+	if (sa_lookup(RCLASS_IV_TBL(obj), (sa_index_t)id, 0))
 	    return Qtrue;
 	break;
       default:
@@ -1137,40 +1113,26 @@ rb_ivar_defined(VALUE obj, ID id)
     return Qfalse;
 }
 
-struct obj_ivar_tag {
-    VALUE obj;
-    int (*func)(ID key, VALUE val, st_data_t arg);
-    st_data_t arg;
-};
-
-static int
-obj_ivar_i(st_data_t key, st_data_t index, st_data_t arg)
-{
-    struct obj_ivar_tag *data = (struct obj_ivar_tag *)arg;
-    if ((long)index < ROBJECT_NUMIV(data->obj)) {
-        VALUE val = ROBJECT_IVPTR(data->obj)[(long)index];
-        if (val != Qundef) {
-            return (data->func)((ID)key, val, data->arg);
-        }
-    }
-    return ST_CONTINUE;
-}
-
 static void
 obj_ivar_each(VALUE obj, int (*func)(ANYARGS), st_data_t arg)
 {
-    st_table *tbl;
-    struct obj_ivar_tag data;
+    sa_table *tbl;
+    long numiv = ROBJECT_NUMIV(obj);
+    VALUE *vars = ROBJECT_IVPTR(obj);
 
     tbl = ROBJECT_IV_INDEX_TBL(obj);
-    if (!tbl)
+    if (!tbl || !numiv)
         return;
 
-    data.obj = obj;
-    data.func = (int (*)(ID key, VALUE val, st_data_t arg))func;
-    data.arg = arg;
-
-    st_foreach_safe(tbl, obj_ivar_i, (st_data_t)&data);
+    SA_FOREACH_START(tbl);
+    if ((long)value < numiv) {
+	VALUE val = vars[value];
+	if (val != Qundef) {
+	    if (((sa_iter_func)func)(entry->key, val, arg) == SA_STOP)
+		break;
+	}
+    }
+    SA_FOREACH_END();
 }
 
 void
@@ -1182,9 +1144,7 @@ rb_ivar_foreach(VALUE obj, int (*func)(ANYARGS), st_data_t arg)
 	break;
       case T_CLASS:
       case T_MODULE:
-	if (RCLASS_IV_TBL(obj)) {
-	    st_foreach_safe(RCLASS_IV_TBL(obj), func, arg);
-	}
+        sa_foreach(RCLASS_IV_TBL(obj), func, arg);
 	break;
       default:
 	if (!generic_iv_tbl) break;
@@ -1192,7 +1152,7 @@ rb_ivar_foreach(VALUE obj, int (*func)(ANYARGS), st_data_t arg)
 	    st_data_t tbl;
 
 	    if (st_lookup(generic_iv_tbl, (st_data_t)obj, &tbl)) {
-		st_foreach_safe((st_table *)tbl, func, arg);
+		sa_foreach((sa_table *)tbl, func, arg);
 	    }
 	}
 	break;
@@ -1202,11 +1162,11 @@ rb_ivar_foreach(VALUE obj, int (*func)(ANYARGS), st_data_t arg)
 st_index_t
 rb_ivar_count(VALUE obj)
 {
-    st_table *tbl;
+    sa_table *tbl;
     switch (TYPE(obj)) {
       case T_OBJECT:
 	if ((tbl = ROBJECT_IV_INDEX_TBL(obj)) != 0) {
-	    st_index_t i, count, num = tbl->num_entries;
+	    sa_index_t i, count, num = tbl->num_entries;
 	    const VALUE *const ivptr = ROBJECT_IVPTR(obj);
 	    for (i = count = 0; i < num; ++i) {
 		if (ivptr[i] != Qundef) {
@@ -1228,7 +1188,7 @@ rb_ivar_count(VALUE obj)
 	    st_data_t data;
 
 	    if (st_lookup(generic_iv_tbl, (st_data_t)obj, &data) &&
-		(tbl = (st_table *)data) != 0) {
+		(tbl = (sa_table *)data) != 0) {
 		return tbl->num_entries;
 	    }
 	}
@@ -1238,7 +1198,7 @@ rb_ivar_count(VALUE obj)
 }
 
 static int
-ivar_i(ID key, VALUE val, VALUE ary)
+ivar_i(sa_index_t key, VALUE val, VALUE ary)
 {
     if (rb_is_instance_id(key)) {
 	rb_ary_push(ary, ID2SYM(key));
@@ -1301,7 +1261,7 @@ rb_obj_remove_instance_variable(VALUE obj, VALUE name)
     VALUE val = Qnil;
     const ID id = rb_to_id(name);
     st_data_t n, v;
-    struct st_table *iv_index_tbl;
+    struct sa_table *iv_index_tbl;
     st_data_t index;
 
     if (!OBJ_UNTRUSTED(obj) && rb_safe_level() >= 4)
@@ -1315,7 +1275,7 @@ rb_obj_remove_instance_variable(VALUE obj, VALUE name)
       case T_OBJECT:
         iv_index_tbl = ROBJECT_IV_INDEX_TBL(obj);
         if (!iv_index_tbl) break;
-        if (!st_lookup(iv_index_tbl, (st_data_t)id, &index)) break;
+        if (!sa_lookup(iv_index_tbl, (sa_index_t)id, &index)) break;
         if (ROBJECT_NUMIV(obj) <= (long)index) break;
         val = ROBJECT_IVPTR(obj)[index];
         if (val != Qundef) {
@@ -1326,14 +1286,14 @@ rb_obj_remove_instance_variable(VALUE obj, VALUE name)
       case T_CLASS:
       case T_MODULE:
 	n = id;
-	if (RCLASS_IV_TBL(obj) && st_delete(RCLASS_IV_TBL(obj), &n, &v)) {
+	if (sa_delete(RCLASS_IV_TBL(obj), (sa_index_t)id, &v)) {
 	    return (VALUE)v;
 	}
 	break;
       default:
 	if (FL_TEST(obj, FL_EXIVAR) || rb_special_const_p(obj)) {
 	    v = val;
-	    if (generic_ivar_remove(obj, (st_data_t)id, &v)) {
+	    if (generic_ivar_remove(obj, id, &v)) {
 		return (VALUE)v;
 	    }
 	}
@@ -1410,20 +1370,20 @@ rb_mod_const_missing(VALUE klass, VALUE name)
 static void
 autoload_mark(void *ptr)
 {
-    rb_mark_tbl((st_table *)ptr);
+    rb_mark_sa_tbl((sa_table *)ptr);
 }
 
 static void
 autoload_free(void *ptr)
 {
-    st_free_table((st_table *)ptr);
+    sa_free_table((sa_table *)ptr);
 }
 
 static size_t
 autoload_memsize(const void *ptr)
 {
-    const st_table *tbl = ptr;
-    return st_memsize(tbl);
+    const sa_table *tbl = ptr;
+    return sa_memsize(tbl);
 }
 
 static const rb_data_type_t autoload_data_type = {
@@ -1432,14 +1392,14 @@ static const rb_data_type_t autoload_data_type = {
 };
 
 #define check_autoload_table(av) \
-    (struct st_table *)rb_check_typeddata((av), &autoload_data_type)
+    (struct sa_table *)rb_check_typeddata((av), &autoload_data_type)
 
 void
 rb_autoload(VALUE mod, ID id, const char *file)
 {
     st_data_t av;
     VALUE fn;
-    struct st_table *tbl;
+    struct sa_table *tbl;
 
     if (!rb_is_const_id(id)) {
 	rb_raise(rb_eNameError, "autoload must be constant name: %s", rb_id2name(id));
@@ -1448,43 +1408,41 @@ rb_autoload(VALUE mod, ID id, const char *file)
 	rb_raise(rb_eArgError, "empty file name");
     }
 
-    if ((tbl = RCLASS_CONST_TBL(mod)) && st_lookup(tbl, (st_data_t)id, &av) && ((rb_const_entry_t*)av)->value != Qundef)
+    if (sa_lookup(RCLASS_CONST_TBL(mod), (sa_index_t)id, &av) && ((rb_const_entry_t*)av)->value != Qundef)
 	return;
 
     rb_const_set(mod, id, Qundef);
     tbl = RCLASS_IV_TBL(mod);
-    if (tbl && st_lookup(tbl, (st_data_t)autoload, &av)) {
+    if (sa_lookup(tbl, (sa_index_t)autoload, &av)) {
 	tbl = check_autoload_table((VALUE)av);
     }
     else {
-	if (!tbl) tbl = RCLASS_IV_TBL(mod) = st_init_numtable();
 	av = (st_data_t)TypedData_Wrap_Struct(0, &autoload_data_type, 0);
-	st_add_direct(tbl, (st_data_t)autoload, av);
-	DATA_PTR(av) = tbl = st_init_numtable();
+	sa_insert(tbl, (sa_index_t)autoload, av);
+	DATA_PTR(av) = tbl = sa_new_table();
     }
     fn = rb_str_new2(file);
     FL_UNSET(fn, FL_TAINT);
     OBJ_FREEZE(fn);
-    st_insert(tbl, (st_data_t)id, (st_data_t)rb_node_newnode(NODE_MEMO, fn, rb_safe_level(), 0));
+    sa_insert(tbl, (sa_index_t)id, (st_data_t)rb_node_newnode(NODE_MEMO, fn, rb_safe_level(), 0));
 }
 
 static NODE*
 autoload_delete(VALUE mod, ID id)
 {
-    st_data_t val, load = 0, n = id;
+    st_data_t val, load = 0;
     rb_const_entry_t *ce;
 
-    st_delete(RCLASS_CONST_TBL(mod), &n, &val);
+    sa_delete(RCLASS_CONST_TBL(mod), (sa_index_t)id, &val);
     ce = (rb_const_entry_t*)val;
     if (ce) xfree(ce);
-    if (st_lookup(RCLASS_IV_TBL(mod), (st_data_t)autoload, &val)) {
-	struct st_table *tbl = check_autoload_table((VALUE)val);
+    if (sa_lookup(RCLASS_IV_TBL(mod), (sa_index_t)autoload, &val)) {
+	struct sa_table *tbl = check_autoload_table((VALUE)val);
 
-	st_delete(tbl, &n, &load);
+	sa_delete(tbl, (sa_index_t)id, &load);
 
 	if (tbl->num_entries == 0) {
-	    n = autoload;
-	    st_delete(RCLASS_IV_TBL(mod), &n, &val);
+	    sa_delete(RCLASS_IV_TBL(mod), (sa_index_t)autoload, &val);
 	}
     }
 
@@ -1509,14 +1467,14 @@ static NODE *
 autoload_node(VALUE mod, ID id, const char **loadingpath)
 {
     VALUE file;
-    struct st_table *tbl;
+    struct sa_table *tbl;
     st_data_t val;
     NODE *load;
     const char *loading;
     int safe;
 
-    if (!st_lookup(RCLASS_IV_TBL(mod), autoload, &val) ||
-	!(tbl = check_autoload_table((VALUE)val)) || !st_lookup(tbl, (st_data_t)id, &val)) {
+    if (!sa_lookup(RCLASS_IV_TBL(mod), (sa_index_t)autoload, &val) ||
+	!(tbl = check_autoload_table((VALUE)val)) || !sa_lookup(tbl, (sa_index_t)id, &val)) {
 	return 0;
     }
     load = (NODE *)val;
@@ -1541,10 +1499,10 @@ autoload_node(VALUE mod, ID id, const char **loadingpath)
 static int
 autoload_node_id(VALUE mod, ID id)
 {
-    struct st_table *tbl = RCLASS_CONST_TBL(mod);
+    struct sa_table *tbl = RCLASS_CONST_TBL(mod);
     st_data_t val;
 
-    if (!tbl || !st_lookup(tbl, (st_data_t)id, &val) || ((rb_const_entry_t*)val)->value != Qundef) {
+    if (!tbl || !sa_lookup(tbl, (sa_index_t)id, &val) || ((rb_const_entry_t*)val)->value != Qundef) {
 	return 0;
     }
     return 1;
@@ -1593,7 +1551,7 @@ rb_const_get_0(VALUE klass, ID id, int exclude, int recurse, int visibility)
     while (RTEST(tmp)) {
 	VALUE am = 0;
 	st_data_t data;
-	while (RCLASS_CONST_TBL(tmp) && st_lookup(RCLASS_CONST_TBL(tmp), (st_data_t)id, &data)) {
+	while (sa_lookup(RCLASS_CONST_TBL(tmp), (sa_index_t)id, &data)) {
 	    rb_const_entry_t *ce = (rb_const_entry_t *)data;
 	    if (visibility && ce->flag == CONST_PRIVATE) {
 		rb_name_error(id, "private constant %s::%s referenced", rb_class2name(klass), rb_id2name(id));
@@ -1686,12 +1644,12 @@ VALUE
 rb_const_remove(VALUE mod, ID id)
 {
     VALUE val;
-    st_data_t v, n = id;
+    st_data_t v;
 
     if (!OBJ_UNTRUSTED(mod) && rb_safe_level() >= 4)
 	rb_raise(rb_eSecurityError, "Insecure: can't remove constant");
     rb_check_frozen(mod);
-    if (!RCLASS_CONST_TBL(mod) || !st_delete(RCLASS_CONST_TBL(mod), &n, &v)) {
+    if (!sa_delete(RCLASS_CONST_TBL(mod), (sa_index_t)id, &v)) {
 	if (rb_const_defined_at(mod, id)) {
 	    rb_name_error(id, "cannot remove %s::%s",
 			  rb_class2name(mod), rb_id2name(id));
@@ -1711,27 +1669,20 @@ rb_const_remove(VALUE mod, ID id)
     return val;
 }
 
-static int
-sv_i(ID key, rb_const_entry_t *ce, st_table *tbl)
-{
-    if (rb_is_const_id(key)) {
-	if (!st_lookup(tbl, (st_data_t)key, 0)) {
-	    st_insert(tbl, (st_data_t)key, (st_data_t)ce);
-	}
-    }
-    return ST_CONTINUE;
-}
-
 void*
 rb_mod_const_at(VALUE mod, void *data)
 {
-    st_table *tbl = data;
+    sa_table *tbl = data;
     if (!tbl) {
-	tbl = st_init_numtable();
+	tbl = sa_new_table();
     }
-    if (RCLASS_CONST_TBL(mod)) {
-	st_foreach_safe(RCLASS_CONST_TBL(mod), sv_i, (st_data_t)tbl);
+    SA_FOREACH_START(RCLASS_CONST_TBL(mod));
+    if (rb_is_const_id(entry->key)) {
+	if (!sa_lookup(tbl, entry->key, 0)) {
+	    sa_insert(tbl, entry->key, value);
+	}
     }
+    SA_FOREACH_END();
     return tbl;
 }
 
@@ -1748,25 +1699,21 @@ rb_mod_const_of(VALUE mod, void *data)
     return data;
 }
 
-static int
-list_i(st_data_t key, st_data_t value, VALUE ary)
-{
-    ID sym = (ID)key;
-    rb_const_entry_t *ce = (rb_const_entry_t *)value;
-    if (ce->flag != CONST_PRIVATE) rb_ary_push(ary, ID2SYM(sym));
-    return ST_CONTINUE;
-}
-
 VALUE
 rb_const_list(void *data)
 {
-    st_table *tbl = data;
+    sa_table *tbl = data;
     VALUE ary;
 
     if (!tbl) return rb_ary_new2(0);
     ary = rb_ary_new2(tbl->num_entries);
-    st_foreach_safe(tbl, list_i, ary);
-    st_free_table(tbl);
+
+    SA_FOREACH_START(tbl);
+    rb_const_entry_t *ce = (rb_const_entry_t *)value;
+    if (ce->flag != CONST_PRIVATE) rb_ary_push(ary, ID2SYM(entry->key));
+    SA_FOREACH_END();
+
+    sa_free_table(tbl);
 
     return ary;
 }
@@ -1790,7 +1737,7 @@ VALUE
 rb_mod_constants(int argc, VALUE *argv, VALUE mod)
 {
     VALUE inherit;
-    st_table *tbl;
+    sa_table *tbl;
 
     if (argc == 0) {
 	inherit = Qtrue;
@@ -1817,7 +1764,7 @@ rb_const_defined_0(VALUE klass, ID id, int exclude, int recurse, int visibility)
     tmp = klass;
   retry:
     while (tmp) {
-	if (RCLASS_CONST_TBL(tmp) && st_lookup(RCLASS_CONST_TBL(tmp), (st_data_t)id, &value)) {
+	if (sa_lookup(RCLASS_CONST_TBL(tmp), (sa_index_t)id, &value)) {
 	    rb_const_entry_t *ce = (rb_const_entry_t *)value;
 	    if (visibility && ce->flag == CONST_PRIVATE) {
 		return (int)Qfalse;
@@ -1885,6 +1832,7 @@ void
 rb_const_set(VALUE klass, ID id, VALUE val)
 {
     rb_const_entry_t *ce;
+    st_data_t value;
     VALUE visibility = CONST_PUBLIC;
 
     if (NIL_P(klass)) {
@@ -1893,21 +1841,14 @@ rb_const_set(VALUE klass, ID id, VALUE val)
     }
 
     check_before_mod_set(klass, id, val, "constant");
-    if (!RCLASS_CONST_TBL(klass)) {
-	RCLASS_CONST_TBL(klass) = st_init_numtable();
-    }
-    else {
-	st_data_t value;
-
-	if (st_lookup(RCLASS_CONST_TBL(klass), (st_data_t)id, &value)) {
-	    rb_const_entry_t *ce = (rb_const_entry_t*)value;
-	    if (ce->value == Qundef)
-		autoload_delete(klass, id);
-	    else {
-		visibility = ce->flag;
-		rb_warn("already initialized constant %s", rb_id2name(id));
-	    }
-	}
+    if (sa_lookup(RCLASS_CONST_TBL(klass), (sa_index_t)id, &value)) {
+        rb_const_entry_t *ce = (rb_const_entry_t*)value;
+        if (ce->value == Qundef)
+            autoload_delete(klass, id);
+        else {
+            visibility = ce->flag;
+            rb_warn("already initialized constant %s", rb_id2name(id));
+        }
     }
 
     rb_vm_change_state();
@@ -1916,7 +1857,7 @@ rb_const_set(VALUE klass, ID id, VALUE val)
     ce->flag = (rb_const_flag_t)visibility;
     ce->value = val;
 
-    st_insert(RCLASS_CONST_TBL(klass), (st_data_t)id, (st_data_t)ce);
+    sa_insert(RCLASS_CONST_TBL(klass), (sa_index_t)id, (st_data_t)ce);
 }
 
 void
@@ -1958,8 +1899,7 @@ set_const_visibility(VALUE mod, int argc, VALUE *argv, rb_const_flag_t flag)
     for (i = 0; i < argc; i++) {
 	VALUE val = argv[i];
 	id = rb_to_id(val);
-	if (RCLASS_CONST_TBL(mod) &&
-	    st_lookup(RCLASS_CONST_TBL(mod), (st_data_t)id, &v)) {
+	if (sa_lookup(RCLASS_CONST_TBL(mod), (sa_index_t)id, &v)) {
 	    ((rb_const_entry_t*)v)->flag = flag;
 	}
 	else {
@@ -2008,7 +1948,7 @@ original_module(VALUE c)
 }
 
 #define CVAR_LOOKUP(v,r) do {\
-    if (RCLASS_IV_TBL(klass) && st_lookup(RCLASS_IV_TBL(klass),(st_data_t)id,(v))) {\
+    if (sa_lookup(RCLASS_IV_TBL(klass),(sa_index_t)id,(v))) {\
 	r;\
     }\
     if (FL_TEST(klass, FL_SINGLETON) ) {\
@@ -2027,7 +1967,7 @@ original_module(VALUE c)
 	klass = RCLASS_SUPER(klass);\
     }\
     while (klass) {\
-	if (RCLASS_IV_TBL(klass) && st_lookup(RCLASS_IV_TBL(klass),(st_data_t)id,(v))) {\
+	if (sa_lookup(RCLASS_IV_TBL(klass),(sa_index_t)id,(v))) {\
 	    r;\
 	}\
 	klass = RCLASS_SUPER(klass);\
@@ -2043,15 +1983,13 @@ rb_cvar_set(VALUE klass, ID id, VALUE val)
     CVAR_LOOKUP(0, {if (!front) front = klass; target = klass;});
     if (target) {
 	if (front && target != front) {
-	    st_data_t did = id;
-
 	    if (RTEST(ruby_verbose)) {
 		rb_warning("class variable %s of %s is overtaken by %s",
 			   rb_id2name(id), rb_class2name(original_module(front)),
 			   rb_class2name(original_module(target)));
 	    }
 	    if (BUILTIN_TYPE(front) == T_CLASS) {
-		st_delete(RCLASS_IV_TBL(front),&did,0);
+		sa_delete(RCLASS_IV_TBL(front),(sa_index_t)id,0);
 	    }
 	}
     }
@@ -2060,11 +1998,7 @@ rb_cvar_set(VALUE klass, ID id, VALUE val)
     }
 
     check_before_mod_set(target, id, val, "class variable");
-    if (!RCLASS_IV_TBL(target)) {
-	RCLASS_IV_TBL(target) = st_init_numtable();
-    }
-
-    st_insert(RCLASS_IV_TBL(target), (st_data_t)id, (st_data_t)val);
+    sa_insert(RCLASS_IV_TBL(target), (sa_index_t)id, (st_data_t)val);
 }
 
 VALUE
@@ -2080,15 +2014,13 @@ rb_cvar_get(VALUE klass, ID id)
 		      rb_id2name(id), rb_class2name(tmp));
     }
     if (front && target != front) {
-	st_data_t did = id;
-
 	if (RTEST(ruby_verbose)) {
 	    rb_warning("class variable %s of %s is overtaken by %s",
 		       rb_id2name(id), rb_class2name(original_module(front)),
 		       rb_class2name(original_module(target)));
 	}
 	if (BUILTIN_TYPE(front) == T_CLASS) {
-	    st_delete(RCLASS_IV_TBL(front),&did,0);
+	    sa_delete(RCLASS_IV_TBL(front),(sa_index_t)id,0);
 	}
     }
     return (VALUE)value;
@@ -2134,7 +2066,7 @@ rb_define_class_variable(VALUE klass, const char *name, VALUE val)
 }
 
 static int
-cv_i(ID key, VALUE value, VALUE ary)
+cv_i(sa_index_t key, VALUE value, VALUE ary)
 {
     if (rb_is_class_id(key)) {
 	VALUE kval = ID2SYM(key);
@@ -2167,7 +2099,7 @@ rb_mod_class_variables(VALUE obj)
     VALUE ary = rb_ary_new();
 
     if (RCLASS_IV_TBL(obj)) {
-	st_foreach_safe(RCLASS_IV_TBL(obj), cv_i, ary);
+	sa_foreach(RCLASS_IV_TBL(obj), cv_i, ary);
     }
     return ary;
 }
@@ -2196,7 +2128,7 @@ VALUE
 rb_mod_remove_cvar(VALUE mod, VALUE name)
 {
     const ID id = rb_to_id(name);
-    st_data_t val, n = id;
+    st_data_t val;
 
     if (!rb_is_class_id(id)) {
 	rb_name_error(id, "wrong class variable name %s", rb_id2name(id));
@@ -2204,7 +2136,7 @@ rb_mod_remove_cvar(VALUE mod, VALUE name)
     if (!OBJ_UNTRUSTED(mod) && rb_safe_level() >= 4)
 	rb_raise(rb_eSecurityError, "Insecure: can't remove class variable");
     rb_check_frozen(mod);
-    if (RCLASS_IV_TBL(mod) && st_delete(RCLASS_IV_TBL(mod), &n, &val)) {
+    if (RCLASS_IV_TBL(mod) && sa_delete(RCLASS_IV_TBL(mod), (sa_index_t)id, &val)) {
 	return (VALUE)val;
     }
     if (rb_cvar_defined(mod, id)) {
diff --git a/vm.c b/vm.c
index 97dc21b..905547e 100644
--- a/vm.c
+++ b/vm.c
@@ -1055,7 +1055,7 @@ static void
 add_opt_method(VALUE klass, ID mid, VALUE bop)
 {
     rb_method_entry_t *me;
-    if (st_lookup(RCLASS_M_TBL(klass), mid, (void *)&me) && me->def &&
+    if (sa_lookup(RCLASS_M_TBL(klass), (sa_index_t)mid, (void *)&me) && me->def &&
 	me->def->type == VM_METHOD_TYPE_CFUNC) {
 	st_insert(vm_opt_method_table, (st_data_t)me, (st_data_t)bop);
     }
diff --git a/vm_insnhelper.c b/vm_insnhelper.c
index 5f7d1ee..ade417e 100644
--- a/vm_insnhelper.c
+++ b/vm_insnhelper.c
@@ -1179,7 +1179,7 @@ vm_get_ev_const(rb_thread_t *th, const rb_iseq_t *iseq,
 		st_data_t data;
 	      search_continue:
 		if (RCLASS_CONST_TBL(klass) &&
-		    st_lookup(RCLASS_CONST_TBL(klass), id, &data)) {
+		    sa_lookup(RCLASS_CONST_TBL(klass), (sa_index_t)id, &data)) {
 		    val = ((rb_const_entry_t*)data)->value;
 		    if (val == Qundef) {
 			if (am == klass) break;
@@ -1289,10 +1289,10 @@ vm_getivar(VALUE obj, ID id, IC ic)
 	    st_data_t index;
 	    long len = ROBJECT_NUMIV(obj);
 	    VALUE *ptr = ROBJECT_IVPTR(obj);
-	    struct st_table *iv_index_tbl = ROBJECT_IV_INDEX_TBL(obj);
+	    struct sa_table *iv_index_tbl = ROBJECT_IV_INDEX_TBL(obj);
 
 	    if (iv_index_tbl) {
-		if (st_lookup(iv_index_tbl, id, &index)) {
+		if (sa_lookup(iv_index_tbl, (sa_index_t)id, &index)) {
 		    if ((long)index < len) {
 			val = ptr[index];
 		    }
@@ -1342,9 +1342,9 @@ vm_setivar(VALUE obj, ID id, VALUE val, IC ic)
 	    }
 	}
 	else {
-	    struct st_table *iv_index_tbl = ROBJECT_IV_INDEX_TBL(obj);
+	    struct sa_table *iv_index_tbl = ROBJECT_IV_INDEX_TBL(obj);
 
-	    if (iv_index_tbl && st_lookup(iv_index_tbl, (st_data_t)id, &index)) {
+	    if (iv_index_tbl && sa_lookup(iv_index_tbl, (sa_index_t)id, &index)) {
 		ic->ic_class = klass;
 		ic->ic_value.index = index;
 		ic->ic_vmstat = GET_VM_STATE_VERSION();
diff --git a/vm_method.c b/vm_method.c
index 85cb30c..d45df81 100644
--- a/vm_method.c
+++ b/vm_method.c
@@ -162,7 +162,7 @@ rb_method_entry_make(VALUE klass, ID mid, rb_method_type_t type,
 		     rb_method_definition_t *def, rb_method_flag_t noex)
 {
     rb_method_entry_t *me;
-    st_table *mtbl;
+    sa_table *mtbl;
     st_data_t data;
 
     if (NIL_P(klass)) {
@@ -190,7 +190,7 @@ rb_method_entry_make(VALUE klass, ID mid, rb_method_type_t type,
     mtbl = RCLASS_M_TBL(klass);
 
     /* check re-definition */
-    if (st_lookup(mtbl, mid, &data)) {
+    if (sa_lookup(mtbl, (sa_index_t)mid, &data)) {
 	rb_method_entry_t *old_me = (rb_method_entry_t *)data;
 	rb_method_definition_t *old_def = old_me->def;
 
@@ -248,7 +248,7 @@ rb_method_entry_make(VALUE klass, ID mid, rb_method_type_t type,
 	}
     }
 
-    st_insert(mtbl, mid, (st_data_t) me);
+    sa_insert(mtbl, (sa_index_t)mid, (st_data_t) me);
 
     return me;
 }
@@ -371,7 +371,7 @@ search_method(VALUE klass, ID id)
 	return 0;
     }
 
-    while (!st_lookup(RCLASS_M_TBL(klass), id, &body)) {
+    while (!sa_lookup(RCLASS_M_TBL(klass), (sa_index_t)id, &body)) {
 	klass = RCLASS_SUPER(klass);
 	if (!klass) {
 	    return 0;
@@ -429,7 +429,7 @@ rb_method_entry(VALUE klass, ID id)
 static void
 remove_method(VALUE klass, ID mid)
 {
-    st_data_t key, data;
+    st_data_t data;
     rb_method_entry_t *me = 0;
 
     if (klass == rb_cObject) {
@@ -443,14 +443,13 @@ remove_method(VALUE klass, ID mid)
 	rb_warn("removing `%s' may cause serious problems", rb_id2name(mid));
     }
 
-    if (!st_lookup(RCLASS_M_TBL(klass), mid, &data) ||
+    if (!sa_lookup(RCLASS_M_TBL(klass), (sa_index_t)mid, &data) ||
 	!(me = (rb_method_entry_t *)data) ||
 	(!me->def || me->def->type == VM_METHOD_TYPE_UNDEF)) {
 	rb_name_error(mid, "method `%s' not defined in %s",
 		      rb_id2name(mid), rb_class2name(klass));
     }
-    key = (st_data_t)mid;
-    st_delete(RCLASS_M_TBL(klass), &key, &data);
+    sa_delete(RCLASS_M_TBL(klass), (sa_index_t)mid, &data);
 
     rb_vm_check_redefinition_opt_method(me);
     rb_clear_cache_for_undef(klass, mid);
diff --git a/array.c b/array.c
index c33d106..747da0e 100644
--- a/array.c
+++ b/array.c
@@ -255,15 +255,24 @@ rb_ary_modify(VALUE ary)
     rb_ary_modify_check(ary);
     if (ARY_SHARED_P(ary)) {
         long len = RARRAY_LEN(ary);
+	VALUE shared = ARY_SHARED(ary);
         if (len <= RARRAY_EMBED_LEN_MAX) {
             VALUE *ptr = ARY_HEAP_PTR(ary);
-            VALUE shared = ARY_SHARED(ary);
             FL_UNSET_SHARED(ary);
             FL_SET_EMBED(ary);
             MEMCPY(ARY_EMBED_PTR(ary), ptr, VALUE, len);
             rb_ary_decrement_share(shared);
             ARY_SET_EMBED_LEN(ary, len);
         }
+	else if (ARY_SHARED_NUM(shared) == 1 && len > RARRAY_LEN(shared)>>1) {
+	    long shift = RARRAY_PTR(ary) - RARRAY_PTR(shared);
+	    ARY_SET_PTR(ary, RARRAY_PTR(shared));
+	    ARY_SET_CAPA(ary, RARRAY_LEN(shared));
+	    MEMMOVE(RARRAY_PTR(ary), RARRAY_PTR(ary)+shift, VALUE, len);
+	    FL_UNSET_SHARED(ary);
+            FL_SET_EMBED(shared);
+	    rb_ary_decrement_share(shared);
+	}
         else {
             VALUE *ptr = ALLOC_N(VALUE, len);
             MEMCPY(ptr, RARRAY_PTR(ary), VALUE, len);
@@ -274,6 +283,38 @@ rb_ary_modify(VALUE ary)
     }
 }
 
+static void
+ary_ensure_room_for_push(VALUE ary, long add_len)
+{
+    long new_len = RARRAY_LEN(ary) + add_len;
+    long capa;
+
+    if (ARY_SHARED_P(ary)) {
+        if (new_len > RARRAY_EMBED_LEN_MAX) {
+            VALUE shared = ARY_SHARED(ary);
+            if (ARY_SHARED_NUM(shared) == 1) {
+		if (RARRAY_PTR(ary) - RARRAY_PTR(shared) + new_len <= RARRAY_LEN(shared)) {
+		    rb_ary_modify_check(ary);
+		}
+		else {
+		    /* if array is shared, than it is likely it participate in push/shift pattern */
+		    rb_ary_modify(ary);
+		    capa = ARY_CAPA(ary);
+		    if (new_len > capa - (capa >> 6)) {
+			ary_double_capa(ary, new_len);
+		    }
+		}
+		return;
+            }
+        }
+    }
+    rb_ary_modify(ary);
+    capa = ARY_CAPA(ary);
+    if (new_len > capa) {
+	ary_double_capa(ary, new_len);
+    }
+}
+
 VALUE
 rb_ary_freeze(VALUE ary)
 {
@@ -446,8 +487,9 @@ ary_make_shared(VALUE ary)
 	OBJSETUP(shared, 0, T_ARRAY);
         FL_UNSET_EMBED(shared);
 
-        ARY_SET_LEN((VALUE)shared, RARRAY_LEN(ary));
+        ARY_SET_LEN((VALUE)shared, ARY_CAPA(ary));
         ARY_SET_PTR((VALUE)shared, RARRAY_PTR(ary));
+	rb_mem_clear(RARRAY_PTR(shared) + RARRAY_LEN(ary), ARY_CAPA(ary) - RARRAY_LEN(ary));
 	FL_SET_SHARED_ROOT(shared);
 	ARY_SET_SHARED_NUM((VALUE)shared, 1);
 	FL_SET_SHARED(ary);
@@ -737,8 +779,6 @@ ary_take_first_or_last(int argc, VALUE *argv, VALUE ary, enum ary_take_pos_flags
     return ary_make_partial(ary, rb_cArray, offset, n);
 }
 
-static VALUE rb_ary_push_1(VALUE ary, VALUE item);
-
 /*
  *  call-seq:
  *     ary << obj            -> ary
@@ -755,8 +795,12 @@ static VALUE rb_ary_push_1(VALUE ary, VALUE item);
 VALUE
 rb_ary_push(VALUE ary, VALUE item)
 {
-    rb_ary_modify(ary);
-    return rb_ary_push_1(ary, item);
+    long idx = RARRAY_LEN(ary);
+
+    ary_ensure_room_for_push(ary, 1);
+    RARRAY_PTR(ary)[idx] = item;
+    ARY_SET_LEN(ary, idx + 1);
+    return ary;
 }
 
 static VALUE
@@ -772,6 +816,18 @@ rb_ary_push_1(VALUE ary, VALUE item)
     return ary;
 }
 
+static VALUE
+rb_ary_cat(VALUE ary, const VALUE *ptr, long len)
+{
+    long oldlen = RARRAY_LEN(ary);
+
+    ary_ensure_room_for_push(ary, len);
+copy:
+    MEMCPY(RARRAY_PTR(ary) + oldlen, ptr, VALUE, len);
+    ARY_SET_LEN(ary, oldlen + len);
+    return ary;
+}
+
 /*
  *  call-seq:
  *     ary.push(obj, ... )   -> ary
@@ -788,11 +844,7 @@ rb_ary_push_1(VALUE ary, VALUE item)
 static VALUE
 rb_ary_push_m(int argc, VALUE *argv, VALUE ary)
 {
-    rb_ary_modify(ary);
-    while (argc--) {
-	rb_ary_push_1(ary, *argv++);
-    }
-    return ary;
+    return rb_ary_cat(ary, argv, argc);
 }
 
 VALUE
@@ -920,6 +972,55 @@ rb_ary_shift_m(int argc, VALUE *argv, VALUE ary)
     return result;
 }
 
+static void
+ary_ensure_room_for_unshift(VALUE ary, int argc)
+{
+    long len = RARRAY_LEN(ary);
+    long new_len = len + argc;
+    long capa;
+    VALUE *head, *sharedp;
+
+    if (ARY_SHARED_P(ary)) {
+	VALUE shared = ARY_SHARED(ary);
+	capa = RARRAY_LEN(shared);
+	if (ARY_SHARED_NUM(shared) == 1 && capa > new_len) {
+	    head = RARRAY_PTR(ary);
+	    sharedp = RARRAY_PTR(shared);
+	    goto makeroom_if_need;
+	}
+    }
+
+    rb_ary_modify(ary);
+    capa = ARY_CAPA(ary);
+    if (capa - (capa >> 6) <= new_len) {
+	ary_double_capa(ary, new_len);
+    }
+
+    /* use shared array for big "queues" */
+    if (new_len > ARY_DEFAULT_SIZE * 4) {
+	/* make a room for unshifted items */
+	capa = ARY_CAPA(ary);
+	ary_make_shared(ary);
+
+	head = sharedp = RARRAY_PTR(ary);
+	goto makeroom;
+makeroom_if_need:
+	if (head - sharedp < argc) {
+	    long room;
+makeroom:
+	    room = capa - new_len;
+	    room -= room >> 4;
+	    MEMMOVE(sharedp + argc + room, head, VALUE, len);
+	    head = sharedp + argc + room;
+	}
+	ARY_SET_PTR(ary, head - argc);
+    }
+    else {
+	/* sliding items */
+	MEMMOVE(RARRAY_PTR(ary) + argc, RARRAY_PTR(ary), VALUE, len);
+    }
+}
+
 /*
  *  call-seq:
  *     ary.unshift(obj, ...)  -> ary
@@ -935,19 +1036,16 @@ rb_ary_shift_m(int argc, VALUE *argv, VALUE ary)
 static VALUE
 rb_ary_unshift_m(int argc, VALUE *argv, VALUE ary)
 {
-    long len;
+    long len = RARRAY_LEN(ary);
 
-    rb_ary_modify(ary);
-    if (argc == 0) return ary;
-    if (ARY_CAPA(ary) <= (len = RARRAY_LEN(ary)) + argc) {
-	ary_double_capa(ary, len + argc);
+    if (argc == 0) {
+	rb_ary_modify_check(ary);
+	return ary;
     }
 
-    /* sliding items */
-    MEMMOVE(RARRAY_PTR(ary) + argc, RARRAY_PTR(ary), VALUE, len);
+    ary_ensure_room_for_unshift(ary, argc);
     MEMCPY(RARRAY_PTR(ary), argv, VALUE, argc);
-    ARY_INCREASE_LEN(ary, argc);
-
+    ARY_SET_LEN(ary, len + argc);
     return ary;
 }
 
@@ -1309,15 +1407,12 @@ rb_ary_splice(VALUE ary, long beg, long len, VALUE rpl)
 	rpl = rb_ary_to_ary(rpl);
 	rlen = RARRAY_LEN(rpl);
     }
-    rb_ary_modify(ary);
     if (beg >= RARRAY_LEN(ary)) {
 	if (beg > ARY_MAX_SIZE - rlen) {
 	    rb_raise(rb_eIndexError, "index %ld too big", beg);
 	}
+	ary_ensure_room_for_push(ary, rlen-len); /* len is 0 or negative */
 	len = beg + rlen;
-	if (len >= ARY_CAPA(ary)) {
-	    ary_double_capa(ary, len);
-	}
 	rb_mem_clear(RARRAY_PTR(ary) + RARRAY_LEN(ary), beg - RARRAY_LEN(ary));
 	if (rlen > 0) {
 	    MEMCPY(RARRAY_PTR(ary) + beg, RARRAY_PTR(rpl), VALUE, rlen);
@@ -1327,6 +1422,7 @@ rb_ary_splice(VALUE ary, long beg, long len, VALUE rpl)
     else {
 	long alen;
 
+	rb_ary_modify(ary);
 	alen = RARRAY_LEN(ary) + rlen - len;
 	if (alen >= ARY_CAPA(ary)) {
 	    ary_double_capa(ary, alen);
@@ -2116,12 +2212,13 @@ rb_ary_sort_bang(VALUE ary)
     if (RARRAY_LEN(ary) > 1) {
 	VALUE tmp = ary_make_substitution(ary); /* only ary refers tmp */
 	struct ary_sort_data data;
+	long len = RARRAY_LEN(ary);
 
 	RBASIC(tmp)->klass = 0;
 	data.ary = tmp;
 	data.opt_methods = 0;
 	data.opt_inited = 0;
-	ruby_qsort(RARRAY_PTR(tmp), RARRAY_LEN(tmp), sizeof(VALUE),
+	ruby_qsort(RARRAY_PTR(tmp), len, sizeof(VALUE),
 		   rb_block_given_p()?sort_1:sort_2, &data);
 
         if (ARY_EMBED_P(tmp)) {
@@ -2138,7 +2235,7 @@ rb_ary_sort_bang(VALUE ary)
             if (ARY_HEAP_PTR(ary) == ARY_HEAP_PTR(tmp)) {
                 assert(!ARY_EMBED_P(ary));
                 FL_UNSET_SHARED(ary);
-                ARY_SET_CAPA(ary, ARY_CAPA(tmp));
+                ARY_SET_CAPA(ary, RARRAY_LEN(tmp));
             }
             else {
                 assert(!ARY_SHARED_P(tmp));
@@ -2153,8 +2250,8 @@ rb_ary_sort_bang(VALUE ary)
                     xfree(ARY_HEAP_PTR(ary));
                 }
                 ARY_SET_PTR(ary, RARRAY_PTR(tmp));
-                ARY_SET_HEAP_LEN(ary, RARRAY_LEN(tmp));
-                ARY_SET_CAPA(ary, ARY_CAPA(tmp));
+                ARY_SET_HEAP_LEN(ary, len);
+                ARY_SET_CAPA(ary, RARRAY_LEN(tmp));
             }
             /* tmp was lost ownership for the ptr */
             FL_UNSET(tmp, FL_FREEZE);
diff --git a/test/ruby/test_array.rb b/test/ruby/test_array.rb
index 28d3e41..bcad2c0 100644
--- a/test/ruby/test_array.rb
+++ b/test/ruby/test_array.rb
@@ -414,6 +414,18 @@ class TestArray < Test::Unit::TestCase
     a = @cls[1, 2, 3]
     a[-1, 0] = a
     assert_equal([1, 2, 1, 2, 3, 3], a)
+
+    a = @cls[]
+    a[5,0] = [5]
+    assert_equal([nil, nil, nil, nil, nil, 5], a)
+
+    a = @cls[1]
+    a[1,0] = [2]
+    assert_equal([1, 2], a)
+
+    a = @cls[1]
+    a[1,1] = [2]
+    assert_equal([1, 2], a)
   end
 
   def test_assoc
diff --git a/gc.c b/gc.c
index 56f0217..6e3714d 100644
--- a/gc.c
+++ b/gc.c
@@ -108,11 +108,6 @@ ruby_gc_params_t initial_params = {
 #define LONG_LONG long
 #endif
 
-static int heap_min_slots = 10000;
-static int heap_slots_increment = 10000;
-static int initial_heap_slots_increment = 10000;
-static double heap_slots_growth_factor = 1.8;
-
 #define nomem_error GET_VM()->special_exceptions[ruby_error_nomemory]
 
 #if SIZEOF_LONG == SIZEOF_VOIDP
@@ -321,6 +316,28 @@ typedef struct RVALUE {
 #endif
 } RVALUE;
 
+
+/* tiny heap size */
+/* 32KB */
+/*#define HEAP_SIZE 0x8000 */
+/* 128KB */
+/*#define HEAP_SIZE 0x20000 */
+/* 64KB */
+/*#define HEAP_SIZE 0x10000 */
+/* 16KB */
+#define HEAP_SIZE 0x4000
+/* 8KB */
+/*#define HEAP_SIZE 0x2000 */
+/* 4KB */
+/*#define HEAP_SIZE 0x1000 */
+/* 2KB */
+/*#define HEAP_SIZE 0x800 */
+
+#define HEAP_OBJ_LIMIT (unsigned int)(HEAP_SIZE / sizeof(struct RVALUE))
+
+static int heap_slots_increment = 10000 / HEAP_OBJ_LIMIT;
+static double heap_slots_growth_factor = 1.8;
+
 #if defined(_MSC_VER) || defined(__BORLANDC__) || defined(__CYGWIN__)
 #pragma pack(pop)
 #endif
@@ -535,17 +552,6 @@ rb_gc_set_params(void)
 
     if (rb_safe_level() > 0) return;
 
-    envp = getenv("RUBY_GC_STATS");
-    if (envp != NULL) {
-        int i = atoi(envp);
-        if (i > 0) {
-            verbose_gc_stats = 1;
-            fprintf(stderr, "RUBY_GC_STATS=%d\n", verbose_gc_stats);
-        }
-        /* child processes should not inherit RUBY_GC_STATS */
-        ruby_unsetenv("RUBY_GC_STATS");
-    }
-
     envp = getenv("RUBY_GC_DATA_FILE");
     if (envp != NULL) {
         FILE* data_file = fopen(envp, "w");
@@ -559,6 +565,18 @@ rb_gc_set_params(void)
         ruby_unsetenv("RUBY_GC_DATA_FILE");
     }
 
+    envp = getenv("RUBY_GC_STATS");
+    if (envp != NULL) {
+        int i = atoi(envp);
+        if (i > 0) {
+            /* gc_statistics = 1; */
+            verbose_gc_stats = 1;
+            fprintf(gc_data_file, "RUBY_GC_STATS=%d\n", verbose_gc_stats);
+        }
+        /* child processes should not inherit RUBY_GC_STATS */
+        ruby_unsetenv("RUBY_GC_STATS");
+    }
+
     envp = getenv("RUBY_GC_MALLOC_LIMIT");
     if (envp != NULL) {
 	int malloc_limit_i = atoi(envp);
@@ -570,7 +588,7 @@ rb_gc_set_params(void)
 		    malloc_limit_i, initial_malloc_limit);
 	if (malloc_limit_i > 0) {
 	    initial_malloc_limit = malloc_limit_i;
-            // malloc_limit = initial_malloc_limit;
+            malloc_limit = initial_malloc_limit;
 	}
     }
 
@@ -609,8 +627,7 @@ rb_gc_set_params(void)
         if (verbose_gc_stats) {
             fprintf(gc_data_file, "RUBY_HEAP_SLOTS_INCREMENT=%s\n", envp);
         }
-        heap_slots_increment = i;
-        initial_heap_slots_increment = heap_slots_increment;
+        heap_slots_increment = i / HEAP_OBJ_LIMIT;
     }
 
     envp = getenv("RUBY_HEAP_SLOTS_GROWTH_FACTOR");
@@ -670,24 +687,6 @@ rb_objspace_free(rb_objspace_t *objspace)
 }
 #endif
 
-/* tiny heap size */
-/* 32KB */
-/*#define HEAP_SIZE 0x8000 */
-/* 128KB */
-/*#define HEAP_SIZE 0x20000 */
-/* 64KB */
-/*#define HEAP_SIZE 0x10000 */
-/* 16KB */
-#define HEAP_SIZE 0x4000
-/* 8KB */
-/*#define HEAP_SIZE 0x2000 */
-/* 4KB */
-/*#define HEAP_SIZE 0x1000 */
-/* 2KB */
-/*#define HEAP_SIZE 0x800 */
-
-#define HEAP_OBJ_LIMIT (unsigned int)(HEAP_SIZE / sizeof(struct RVALUE))
-
 extern sa_table rb_class_tbl;
 
 int ruby_disable_gc_stress = 0;
@@ -1520,7 +1519,6 @@ rb_gc_dump_file_and_line_info(int argc, VALUE *argv)
     rb_objspace_t *objspace = &rb_objspace;
     VALUE filename, str, include_classnames = Qnil;
     char *fname = NULL;
-    char *klass = NULL;
     FILE* f = NULL;
     size_t i = 0;
 
@@ -1778,17 +1776,15 @@ aligned_free(void *ptr)
 static void
 assign_heap_slot(rb_objspace_t *objspace)
 {
-    /*
-    if (gc_statistics & verbose_gc_stats) {
-	fprintf(gc_data_file, "assigning heap slot\n");
-    }
-    */
-
     RVALUE *p, *pend, *membase;
     struct heaps_slot *slot;
     size_t hi, lo, mid;
     size_t objs;
-
+    /*
+    if (gc_statistics & verbose_gc_stats) {
+	fprintf(gc_data_file, "assigning heap slot: %d\n", heaps_inc);
+    }
+    */
     objs = HEAP_OBJ_LIMIT;
     p = (RVALUE*)malloc(HEAP_SIZE);
     if (p == 0) {
@@ -1903,13 +1899,17 @@ static void
 set_heaps_increment(rb_objspace_t *objspace)
 {
     size_t next_heaps_length = (size_t)(heaps_used * heap_slots_growth_factor);
+    size_t next_heaps_length_alt = heaps_used + heap_slots_increment;
 
-    if (next_heaps_length == heaps_used) {
-        next_heaps_length++;
+    if (next_heaps_length < next_heaps_length_alt) {
+        next_heaps_length = next_heaps_length_alt;
     }
 
     heaps_inc = next_heaps_length - heaps_used;
-
+    /*
+    if (gc_statistics & verbose_gc_stats)
+	fprintf(gc_data_file, "heaps_inc:%lu, slots_inc: %lu\n", heaps_inc, heaps_inc * HEAP_OBJ_LIMIT);
+    */
     if (next_heaps_length > heaps_length) {
 	allocate_sorted_heaps(objspace, next_heaps_length);
     }
@@ -2527,7 +2527,7 @@ gc_mark_children(rb_objspace_t *objspace, VALUE ptr)
 
 #ifdef GC_DEBUG
     if (obj->file && obj->file != Qnil && is_pointer_to_heap(objspace, (void*)obj->file)) {
-	gc_mark(objspace, obj->file, lev);
+	gc_mark(objspace, obj->file);
     }
 #endif
 
@@ -2543,7 +2543,7 @@ gc_mark_children(rb_objspace_t *objspace, VALUE ptr)
 
 #ifdef GC_DEBUG
     if (obj->file && obj->file != Qnil && is_pointer_to_heap(objspace, (void*)obj->file)) {
-	gc_mark(objspace, obj->file, lev);
+	gc_mark(objspace, obj->file);
     }
 #endif
 
@@ -3028,14 +3028,14 @@ before_gc_sweep(rb_objspace_t *objspace)
         MEMZERO((void*)live_counts, unsigned long, T_MASK+1);
     }
 
-    objspace->heap.max_blocks_to_free = heaps_used - (heap_min_slots / HEAP_OBJ_LIMIT);
+    objspace->heap.max_blocks_to_free = heaps_used - (initial_heap_min_slots / HEAP_OBJ_LIMIT);
     objspace->heap.freed_blocks = 0;
 
     freelist = 0;
     objspace->heap.do_heap_free = (size_t)((heaps_used * HEAP_OBJ_LIMIT) * 0.65);
     objspace->heap.free_min = (size_t)((heaps_used * HEAP_OBJ_LIMIT)  * 0.2);
     if (objspace->heap.free_min < initial_free_min) {
-	objspace->heap.do_heap_free = heaps_used * HEAP_OBJ_LIMIT;
+        /* objspace->heap.do_heap_free = heaps_used * HEAP_OBJ_LIMIT; */
         objspace->heap.free_min = initial_free_min;
     }
     objspace->heap.sweep_slots = heaps;
@@ -3067,7 +3067,11 @@ after_gc_sweep(rb_objspace_t *objspace)
 	if (malloc_limit < initial_malloc_limit) malloc_limit = initial_malloc_limit;
     }
     malloc_increase = 0;
-
+    /*
+    if (verbose_gc_stats)
+        fprintf(gc_data_file, "heap size before freeing unused heaps: %7lu\n",
+                (unsigned long)heaps_used*HEAP_OBJ_LIMIT);
+    */
     free_unused_heaps(objspace);
 
     if (gc_statistics) {
@@ -3076,6 +3080,7 @@ after_gc_sweep(rb_objspace_t *objspace)
         if (verbose_gc_stats) {
             /* log gc stats if requested */
             fprintf(gc_data_file, "GC time: %lu musec\n", (unsigned long)(gc_time_accumulator-gc_time_accumulator_before_gc));
+            fprintf(gc_data_file, "heap size        : %7lu\n", (unsigned long)heaps_used*HEAP_OBJ_LIMIT);
             fprintf(gc_data_file, "objects processed: %7lu\n", (unsigned long)processed);
             fprintf(gc_data_file, "live objects     : %7lu\n", (unsigned long)live_after_last_mark_phase);
             fprintf(gc_data_file, "freelist objects : %7lu\n", (unsigned long)freelist_size);
@@ -3125,7 +3130,7 @@ rest_sweep(rb_objspace_t *objspace)
 
 static void gc_marks(rb_objspace_t *objspace);
 
-/* only called from rb_new_obj */
+/* only called from rb_newobj */
 static int
 gc_lazy_sweep(rb_objspace_t *objspace)
 {
@@ -3412,13 +3417,11 @@ gc_marks(rb_objspace_t *objspace)
     struct gc_list *list;
     rb_thread_t *th = GET_THREAD();
     GC_PROF_MARK_TIMER_START;
-
     /*
     if (gc_statistics & verbose_gc_stats) {
         fprintf(gc_data_file, "Marking objects\n");
     }
     */
-
     objspace->heap.live_num = 0;
     objspace->count++;
     live_objects = 0;
diff --git a/eval.c b/eval.c
index 9ceb833..fb5afc0 100644
--- a/eval.c
+++ b/eval.c
@@ -380,8 +380,7 @@ setup_exception(rb_thread_t *th, int tag, volatile VALUE mesg)
     if (file) line = rb_sourceline();
     if (file && !NIL_P(mesg)) {
 	if (mesg == sysstack_error) {
-	    at = rb_enc_sprintf(rb_usascii_encoding(), "%s:%d", file, line);
-	    at = rb_ary_new3(1, at);
+	    at = rb_make_backtrace();
 	    rb_iv_set(mesg, "bt", at);
 	}
 	else {
diff --git a/cont.c b/cont.c
index 1e42974..c656ac3 100644
--- a/cont.c
+++ b/cont.c
@@ -48,6 +48,7 @@
 #define RB_PAGE_MASK (~(RB_PAGE_SIZE - 1))
 static long pagesize;
 #define FIBER_MACHINE_STACK_ALLOCATION_SIZE  (0x10000)
+static int fiber_machine_stack_allocation_size = FIBER_MACHINE_STACK_ALLOCATION_SIZE;
 #endif
 
 #define CAPTURE_JUST_VALID_VM_STACK 1
@@ -620,7 +621,7 @@ fiber_setcontext(rb_fiber_t *newfib, rb_fiber_t *oldfib)
     rb_thread_t *th = GET_THREAD(), *sth = &newfib->cont.saved_thread;
 
     if (newfib->status != RUNNING) {
-	fiber_initialize_machine_stack_context(newfib, FIBER_MACHINE_STACK_ALLOCATION_SIZE);
+	fiber_initialize_machine_stack_context(newfib, fiber_machine_stack_allocation_size);
     }
 
     /* restore thread context */
@@ -1000,6 +1001,39 @@ rb_cont_call(int argc, VALUE *argv, VALUE contval)
  */
 
 #define FIBER_VM_STACK_SIZE (4 * 1024)
+static int fiber_vm_stack_size = FIBER_VM_STACK_SIZE;
+
+void
+rb_fiber_set_stack_params(void)
+{
+    char *envp;
+
+    if (rb_safe_level() > 0) return;
+
+#if FIBER_USE_NATIVE
+    envp = getenv("RUBY_FIBER_MACHINE_STACK_SIZE");
+    if (envp != NULL) {
+	int machine_stack_allocation_size = atoi(envp);
+	if (machine_stack_allocation_size > 0) {
+            fiber_machine_stack_allocation_size = 1024*machine_stack_allocation_size;
+	}
+	if (RTEST(ruby_verbose))
+	    fprintf(stderr, "fiber_machine_stack_size=%dKB (%dKB)\n",
+		    fiber_machine_stack_allocation_size/1024, FIBER_MACHINE_STACK_ALLOCATION_SIZE/1024);
+    }
+#endif
+
+    envp = getenv("RUBY_FIBER_VM_STACK_SIZE");
+    if (envp != NULL) {
+	int vm_stack_size = atoi(envp);
+	if (vm_stack_size > 0) {
+            fiber_vm_stack_size = 1024*vm_stack_size;
+	}
+	if (RTEST(ruby_verbose))
+	    fprintf(stderr, "fiber_vm_stack_size=%dKB (%dKB)\n",
+		    fiber_vm_stack_size/1024, FIBER_VM_STACK_SIZE/1024);
+    }
+}
 
 static const rb_data_type_t fiber_data_type = {
     "fiber",
diff --git a/include/ruby/intern.h b/include/ruby/intern.h
index 8bc790d..194be9e 100644
--- a/include/ruby/intern.h
+++ b/include/ruby/intern.h
@@ -205,6 +205,7 @@ VALUE rb_fiber_resume(VALUE fib, int argc, VALUE *args);
 VALUE rb_fiber_yield(int argc, VALUE *args);
 VALUE rb_fiber_current(void);
 VALUE rb_fiber_alive_p(VALUE);
+void  rb_fiber_set_stack_params(void);
 /* enum.c */
 /* enumerator.c */
 VALUE rb_enumeratorize(VALUE, VALUE, int, VALUE *);
diff --git a/ruby.c b/ruby.c
index 7ffc78e..83c1c2c 100644
--- a/ruby.c
+++ b/ruby.c
@@ -1501,6 +1501,7 @@ process_options(int argc, char **argv, struct cmdline_options *opt)
 
     rb_set_safe_level(opt->safe_level);
     rb_gc_set_params();
+    rb_fiber_set_stack_params();
 
     return iseq;
 }
